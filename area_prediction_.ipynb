{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-a00gUgPxTp"
      },
      "source": [
        "Necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EoyJ367P7rJ",
        "outputId": "47cafb25-4819-4eed-e93e-e29d52c2414d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwyUmkmqPxTt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset, Dataset, ClassLabel\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import DataCollatorWithPadding, pipeline\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZbne7IMPxTu"
      },
      "source": [
        "Load data and make preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlOLVSSgPxTv",
        "outputId": "e4a130ff-18f8-46c7-d015-045388705fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/arxivdataset\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"neelshah18/arxivdataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taPLJQxsPxTw"
      },
      "outputs": [],
      "source": [
        "data = pd.read_json(path + \"/arxivData.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "g35qYiFIPxTw",
        "outputId": "43303d02-5304-44c7-9c9a-124bdfd33885"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "0      [{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...    1  1802.00209v1   \n",
              "1      [{'name': 'Ji Young Lee'}, {'name': 'Franck De...   12  1603.03827v1   \n",
              "2      [{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...    2  1606.00776v2   \n",
              "3      [{'name': 'Sebastian Ruder'}, {'name': 'Joachi...   23  1705.08142v2   \n",
              "4      [{'name': 'Iulian V. Serban'}, {'name': 'Chinn...    7  1709.02349v2   \n",
              "...                                                  ...  ...           ...   \n",
              "40995  [{'name': 'Vitaly Feldman'}, {'name': 'Pravesh...   18   1404.4702v2   \n",
              "40996  [{'name': 'Orly Avner'}, {'name': 'Shie Mannor'}]   22   1404.5421v1   \n",
              "40997  [{'name': 'Ran Zhao'}, {'name': 'Deanna Needel...   22   1404.5899v1   \n",
              "40998  [{'name': 'Zongyan Huang'}, {'name': 'Matthew ...   25   1404.6369v1   \n",
              "40999  [{'name': 'Imen Trabelsi'}, {'name': 'Dorra Be...   27   1407.0380v1   \n",
              "\n",
              "                                                    link  month  \\\n",
              "0      [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "1      [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
              "2      [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
              "3      [{'rel': 'alternate', 'href': 'http://arxiv.or...      5   \n",
              "4      [{'rel': 'alternate', 'href': 'http://arxiv.or...      9   \n",
              "...                                                  ...    ...   \n",
              "40995  [{'rel': 'alternate', 'href': 'http://arxiv.or...      4   \n",
              "40996  [{'rel': 'alternate', 'href': 'http://arxiv.or...      4   \n",
              "40997  [{'rel': 'alternate', 'href': 'http://arxiv.or...      4   \n",
              "40998  [{'rel': 'related', 'href': 'http://dx.doi.org...      4   \n",
              "40999  [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
              "\n",
              "                                                 summary  \\\n",
              "0      We propose an architecture for VQA which utili...   \n",
              "1      Recent approaches based on artificial neural n...   \n",
              "2      We introduce the multiresolution recurrent neu...   \n",
              "3      Multi-task learning is motivated by the observ...   \n",
              "4      We present MILABOT: a deep reinforcement learn...   \n",
              "...                                                  ...   \n",
              "40995  We study the complexity of learning and approx...   \n",
              "40996  We consider the problem of multiple users targ...   \n",
              "40997  In this paper, we compare and analyze clusteri...   \n",
              "40998  Cylindrical algebraic decomposition(CAD) is a ...   \n",
              "40999  Several speaker identification systems are giv...   \n",
              "\n",
              "                                                     tag  \\\n",
              "0      [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n",
              "1      [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "2      [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "3      [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "4      [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "...                                                  ...   \n",
              "40995  [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "40996  [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
              "40997  [{'term': 'math.NA', 'scheme': 'http://arxiv.o...   \n",
              "40998  [{'term': 'cs.SC', 'scheme': 'http://arxiv.org...   \n",
              "40999  [{'term': 'cs.SD', 'scheme': 'http://arxiv.org...   \n",
              "\n",
              "                                                   title  year  \n",
              "0      Dual Recurrent Attention Units for Visual Ques...  2018  \n",
              "1      Sequential Short-Text Classification with Recu...  2016  \n",
              "2      Multiresolution Recurrent Neural Networks: An ...  2016  \n",
              "3      Learning what to share between loosely related...  2017  \n",
              "4                  A Deep Reinforcement Learning Chatbot  2017  \n",
              "...                                                  ...   ...  \n",
              "40995  Nearly Tight Bounds on $\\ell_1$ Approximation ...  2014  \n",
              "40996    Concurrent bandits and cognitive radio networks  2014  \n",
              "40997  A Comparison of Clustering and Missing Data Me...  2014  \n",
              "40998  Applying machine learning to the problem of ch...  2014  \n",
              "40999  A Multi Level Data Fusion Approach for Speaker...  2014  \n",
              "\n",
              "[41000 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a41ff230-8c62-49ae-9b1b-aa642eeb2d17\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...</td>\n",
              "      <td>1</td>\n",
              "      <td>1802.00209v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>We propose an architecture for VQA which utili...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Dual Recurrent Attention Units for Visual Ques...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[{'name': 'Ji Young Lee'}, {'name': 'Franck De...</td>\n",
              "      <td>12</td>\n",
              "      <td>1603.03827v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>3</td>\n",
              "      <td>Recent approaches based on artificial neural n...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Sequential Short-Text Classification with Recu...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...</td>\n",
              "      <td>2</td>\n",
              "      <td>1606.00776v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Multiresolution Recurrent Neural Networks: An ...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[{'name': 'Sebastian Ruder'}, {'name': 'Joachi...</td>\n",
              "      <td>23</td>\n",
              "      <td>1705.08142v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>5</td>\n",
              "      <td>Multi-task learning is motivated by the observ...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Learning what to share between loosely related...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[{'name': 'Iulian V. Serban'}, {'name': 'Chinn...</td>\n",
              "      <td>7</td>\n",
              "      <td>1709.02349v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>9</td>\n",
              "      <td>We present MILABOT: a deep reinforcement learn...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A Deep Reinforcement Learning Chatbot</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40995</th>\n",
              "      <td>[{'name': 'Vitaly Feldman'}, {'name': 'Pravesh...</td>\n",
              "      <td>18</td>\n",
              "      <td>1404.4702v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>4</td>\n",
              "      <td>We study the complexity of learning and approx...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Nearly Tight Bounds on $\\ell_1$ Approximation ...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40996</th>\n",
              "      <td>[{'name': 'Orly Avner'}, {'name': 'Shie Mannor'}]</td>\n",
              "      <td>22</td>\n",
              "      <td>1404.5421v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>4</td>\n",
              "      <td>We consider the problem of multiple users targ...</td>\n",
              "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Concurrent bandits and cognitive radio networks</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40997</th>\n",
              "      <td>[{'name': 'Ran Zhao'}, {'name': 'Deanna Needel...</td>\n",
              "      <td>22</td>\n",
              "      <td>1404.5899v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>4</td>\n",
              "      <td>In this paper, we compare and analyze clusteri...</td>\n",
              "      <td>[{'term': 'math.NA', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>A Comparison of Clustering and Missing Data Me...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40998</th>\n",
              "      <td>[{'name': 'Zongyan Huang'}, {'name': 'Matthew ...</td>\n",
              "      <td>25</td>\n",
              "      <td>1404.6369v1</td>\n",
              "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org...</td>\n",
              "      <td>4</td>\n",
              "      <td>Cylindrical algebraic decomposition(CAD) is a ...</td>\n",
              "      <td>[{'term': 'cs.SC', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Applying machine learning to the problem of ch...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40999</th>\n",
              "      <td>[{'name': 'Imen Trabelsi'}, {'name': 'Dorra Be...</td>\n",
              "      <td>27</td>\n",
              "      <td>1407.0380v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>Several speaker identification systems are giv...</td>\n",
              "      <td>[{'term': 'cs.SD', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A Multi Level Data Fusion Approach for Speaker...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41000 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a41ff230-8c62-49ae-9b1b-aa642eeb2d17')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a41ff230-8c62-49ae-9b1b-aa642eeb2d17 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a41ff230-8c62-49ae-9b1b-aa642eeb2d17');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8a2d30f0-86e5-49bf-b7ba-cc2f0796b793\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8a2d30f0-86e5-49bf-b7ba-cc2f0796b793')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8a2d30f0-86e5-49bf-b7ba-cc2f0796b793 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 41000,\n  \"fields\": [\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 36157,\n        \"samples\": [\n          \"[{'name': 'Cosmin Stamate'}, {'name': 'George D. Magoulas'}, {'name': 'Michael S. C. Thomas'}]\",\n          \"[{'name': 'E. R. Vimina'}, {'name': 'K. Poulose Jacob'}]\",\n          \"[{'name': 'Manu Goyal'}, {'name': 'Moi Hoon Yap'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          13,\n          31,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41000,\n        \"samples\": [\n          \"0809.0490v2\",\n          \"1503.00036v2\",\n          \"1711.09522v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41000,\n        \"samples\": [\n          \"[{'rel': 'related', 'href': 'http://dx.doi.org/10.4018/978-1-60566-766-9', 'type': 'text/html', 'title': 'doi'}, {'rel': 'alternate', 'href': 'http://arxiv.org/abs/0809.0490v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/0809.0490v2', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1503.00036v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1503.00036v2', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1711.09522v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1711.09522v2', 'type': 'application/pdf', 'title': 'pdf'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          7,\n          11,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40967,\n        \"samples\": [\n          \"Many state-of-the-art computer vision algorithms use large scale\\nconvolutional neural networks (CNNs) as basic building blocks. These CNNs are\\nknown for their huge number of parameters, high redundancy in weights, and\\ntremendous computing resource consumptions. This paper presents a learning\\nalgorithm to simplify and speed up these CNNs. Specifically, we introduce a\\n\\\"try-and-learn\\\" algorithm to train pruning agents that remove unnecessary CNN\\nfilters in a data-driven way. With the help of a novel reward function, our\\nagents removes a significant number of filters in CNNs while maintaining\\nperformance at a desired level. Moreover, this method provides an easy control\\nof the tradeoff between network performance and its scale. Per- formance of our\\nalgorithm is validated with comprehensive pruning experiments on several\\npopular CNNs for visual recognition and semantic segmentation tasks.\",\n          \"In this study, we present Swift Linked Data Miner, an interruptible algorithm\\nthat can directly mine an online Linked Data source (e.g., a SPARQL endpoint)\\nfor OWL 2 EL class expressions to extend an ontology with new SubClassOf:\\naxioms. The algorithm works by downloading only a small part of the Linked Data\\nsource at a time, building a smart index in the memory and swiftly iterating\\nover the index to mine axioms. We propose a transformation function from mined\\naxioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment,\\nthat most of the axioms mined by Swift Linked Data Miner are correct and can be\\nadded to an ontology. We provide a ready to use Prot\\\\'eg\\\\'e plugin implementing\\nthe algorithm, to support ontology engineers in their daily modeling work.\",\n          \"We investigate the capacity, convexity and characterization of a general\\nfamily of norm-constrained feed-forward networks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5874,\n        \"samples\": [\n          \"[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.DS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'F.2.2; I.5; J.3', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'physics.soc-ph', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'math.CO', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '05C85, 68R10, 90B15, 90C35', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'G.2.2', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'q-bio.NC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40955,\n        \"samples\": [\n          \"Hypothesis Testing for Automated Community Detection in Networks\",\n          \"Probabilistic Models for Computerized Adaptive Testing\",\n          \"Belief Revision and Rational Inference\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1993,\n        \"max\": 2018,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          2011,\n          2002,\n          2018\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-vUw2oZPxTx",
        "outputId": "ebc65d83-087c-475c-c0b7-d9af7ba8d441"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41000, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "orrn9U4HPxTx",
        "outputId": "6abdc88a-aeb7-44ed-d9ce-5f6ceb0a22a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author     0\n",
              "day        0\n",
              "id         0\n",
              "link       0\n",
              "month      0\n",
              "summary    0\n",
              "tag        0\n",
              "title      0\n",
              "year       0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>author</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>link</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>summary</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "0XA3VNofPxTy",
        "outputId": "f79b2365-b3c8-49e3-aca9-b98ee56a6c4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "data['tag'][0]\n",
        "# Понять что там с таргетами и их форматом\n",
        "# Вероятно labels_column = data['terms'].apply(literal_eval)\n",
        "# labels = labels_column.explode().inique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "urgMLyIAPxTy",
        "outputId": "33adcbea-67b9-4461-a285-bcd4c2e61ca5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          cs.AI\n",
              "1          cs.CL\n",
              "2          cs.CL\n",
              "3        stat.ML\n",
              "4          cs.CL\n",
              "          ...   \n",
              "40995      cs.LG\n",
              "40996      cs.LG\n",
              "40997    math.NA\n",
              "40998      cs.SC\n",
              "40999      cs.SD\n",
              "Name: category, Length: 41000, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cs.AI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cs.CL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cs.CL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stat.ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cs.CL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40995</th>\n",
              "      <td>cs.LG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40996</th>\n",
              "      <td>cs.LG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40997</th>\n",
              "      <td>math.NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40998</th>\n",
              "      <td>cs.SC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40999</th>\n",
              "      <td>cs.SD</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "data['category'] = [eval(i)[0]['term'].strip() for i in data['tag']]  # remain single label only (easy way)\n",
        "data['category']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t-WnS8LPxTz"
      },
      "source": [
        "Here we also need to collect all unique values for arxiv areas and save them to appropriate mapping \"category -- index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ux-PfvsPxTz",
        "outputId": "43962a5a-bd72-489c-8042-00e46e8fe456"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "126"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "unique_categories = np.unique(data['category'])\n",
        "num_categories = len(unique_categories)\n",
        "category_to_ind = {}\n",
        "for i, cat in enumerate(unique_categories):\n",
        "    category_to_ind[cat] = i\n",
        "num_categories"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\n",
        "    \"category\": unique_categories,\n",
        "    \"category_index\": np.arange(num_categories),\n",
        "}).set_index(\"category\").join(data.set_index(\"category\"), how=\"right\", sort=False).reset_index()"
      ],
      "metadata": {
        "id": "AaXNOhK9RtQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "D-UhCNMIPxT0",
        "outputId": "9f92d28a-2c28-4ced-fa86-c5598d08e38c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         15\n",
              "1         20\n",
              "2         20\n",
              "3        124\n",
              "4         20\n",
              "        ... \n",
              "40995     37\n",
              "40996     37\n",
              "40997     80\n",
              "40998     50\n",
              "40999     51\n",
              "Name: category_id, Length: 41000, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40995</th>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40996</th>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40997</th>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40998</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40999</th>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "data['category_id'] = [category_to_ind[cat] for cat in data['category']]\n",
        "data['category_id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmiiNWjkPxT0"
      },
      "source": [
        "To train model we will concatenate title and abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huTqKlasPxT0"
      },
      "outputs": [],
      "source": [
        "data['text_feature'] = data['title'] + \"\\n\" + data['summary']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-92iEhxhPxT1",
        "outputId": "e7858471-b3fe-4721-ea52-0866f85379e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dual Recurrent Attention Units for Visual Question Answering\\nWe propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset.',\n",
              " 'Sequential Short-Text Classification with Recurrent and Convolutional\\n  Neural Networks\\nRecent approaches based on artificial neural networks (ANNs) have shown\\npromising results for short-text classification. However, many short texts\\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\\nand most existing ANN-based systems do not leverage the preceding short texts\\nwhen classifying a subsequent one. In this work, we present a model based on\\nrecurrent neural networks and convolutional neural networks that incorporates\\nthe preceding short texts. Our model achieves state-of-the-art results on three\\ndifferent datasets for dialog act prediction.',\n",
              " 'Multiresolution Recurrent Neural Networks: An Application to Dialogue\\n  Response Generation\\nWe introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens. There are many ways to estimate or\\nlearn the high-level coarse tokens, but we argue that a simple extraction\\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\\nSuch procedure allows training the multiresolution recurrent neural network by\\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\\nthe standard log- likelihood objective w.r.t. natural language tokens (word\\nperplexity), optimizing the joint log-likelihood biases the model towards\\nmodeling high-level abstractions. We apply the proposed model to the task of\\ndialogue response generation in two challenging domains: the Ubuntu technical\\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\\ncompeting approaches by a substantial margin, achieving state-of-the-art\\nresults according to both automatic evaluation metrics and a human evaluation\\nstudy. On Twitter, the model appears to generate more relevant and on-topic\\nresponses according to automatic evaluation metrics. Finally, our experiments\\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\\nnatural language and is better able to capture long-term structure.',\n",
              " 'Learning what to share between loosely related tasks\\nMulti-task learning is motivated by the observation that humans bring to bear\\nwhat they know about related problems when solving new ones. Similarly, deep\\nneural networks can profit from related tasks by sharing parameters with other\\nnetworks. However, humans do not consciously decide to transfer knowledge\\nbetween tasks. In Natural Language Processing (NLP), it is hard to predict if\\nsharing will lead to improvements, particularly if tasks are only loosely\\nrelated. To overcome this, we introduce Sluice Networks, a general framework\\nfor multi-task learning where trainable parameters control the amount of\\nsharing. Our framework generalizes previous proposals in enabling sharing of\\nall combinations of subspaces, layers, and skip connections. We perform\\nexperiments on three task pairs, and across seven different domains, using data\\nfrom OntoNotes 5.0, and achieve up to 15% average error reductions over common\\napproaches to multi-task learning. We show that a) label entropy is predictive\\nof gains in sluice networks, confirming findings for hard parameter sharing and\\nb) while sluice networks easily fit noise, they are robust across domains in\\npractice.',\n",
              " 'A Deep Reinforcement Learning Chatbot\\nWe present MILABOT: a deep reinforcement learning chatbot developed by the\\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\\ncompetition. MILABOT is capable of conversing with humans on popular small talk\\ntopics through both speech and text. The system consists of an ensemble of\\nnatural language generation and retrieval models, including template-based\\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\\nvariable neural network models. By applying reinforcement learning to\\ncrowdsourced data and real-world user interactions, the system has been trained\\nto select an appropriate response from the models in its ensemble. The system\\nhas been evaluated through A/B testing with real-world users, where it\\nperformed significantly better than many competing systems. Due to its machine\\nlearning architecture, the system is likely to improve with additional data.',\n",
              " 'Generating Sentences by Editing Prototypes\\nWe propose a new generative model of sentences that first samples a prototype\\nsentence from the training corpus and then edits it into a new sentence.\\nCompared to traditional models that generate from scratch either left-to-right\\nor by first sampling a latent sentence vector, our prototype-then-edit model\\nimproves perplexity on language modeling and generates higher quality outputs\\naccording to human evaluation. Furthermore, the model gives rise to a latent\\nedit vector that captures interpretable semantics such as sentence similarity\\nand sentence-level analogies.',\n",
              " 'A Deep Reinforcement Learning Chatbot (Short Version)\\nWe present MILABOT: a deep reinforcement learning chatbot developed by the\\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\\ncompetition. MILABOT is capable of conversing with humans on popular small talk\\ntopics through both speech and text. The system consists of an ensemble of\\nnatural language generation and retrieval models, including neural network and\\ntemplate-based models. By applying reinforcement learning to crowdsourced data\\nand real-world user interactions, the system has been trained to select an\\nappropriate response from the models in its ensemble. The system has been\\nevaluated through A/B testing with real-world users, where it performed\\nsignificantly better than other systems. The results highlight the potential of\\ncoupling ensemble systems with deep reinforcement learning as a fruitful path\\nfor developing real-world, open-domain conversational agents.',\n",
              " 'Document Image Coding and Clustering for Script Discrimination\\nThe paper introduces a new method for discrimination of documents given in\\ndifferent scripts. The document is mapped into a uniformly coded text of\\nnumerical values. It is derived from the position of the letters in the text\\nline, based on their typographical characteristics. Each code is considered as\\na gray level. Accordingly, the coded text determines a 1-D image, on which\\ntexture analysis by run-length statistics and local binary pattern is\\nperformed. It defines feature vectors representing the script content of the\\ndocument. A modified clustering approach employed on document feature vector\\ngroups documents written in the same script. Experimentation performed on two\\ncustom oriented databases of historical documents in old Cyrillic, angular and\\nround Glagolitic as well as Antiqua and Fraktur scripts demonstrates the\\nsuperiority of the proposed method with respect to well-known methods in the\\nstate-of-the-art.',\n",
              " 'Tutorial on Answering Questions about Images with Deep Learning\\nTogether with the development of more accurate methods in Computer Vision and\\nNatural Language Understanding, holistic architectures that answer on questions\\nabout the content of real-world images have emerged. In this tutorial, we build\\na neural-based approach to answer questions about images. We base our tutorial\\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\\nmodels that we present here can achieve a competitive performance on both\\ndatasets, in fact, they are among the best methods that use a combination of\\nLSTM with a global, full frame CNN representation of an image. We hope that\\nafter reading this tutorial, the reader will be able to use Deep Learning\\nframeworks, such as Keras and introduced Kraino, to build various architectures\\nthat will lead to a further performance improvement on this challenging task.',\n",
              " 'pix2code: Generating Code from a Graphical User Interface Screenshot\\nTransforming a graphical user interface screenshot created by a designer into\\ncomputer code is a typical task conducted by a developer in order to build\\ncustomized software, websites, and mobile applications. In this paper, we show\\nthat deep learning methods can be leveraged to train a model end-to-end to\\nautomatically generate code from a single input image with over 77% of accuracy\\nfor three different platforms (i.e. iOS, Android and web-based technologies).',\n",
              " 'A Unified Deep Neural Network for Speaker and Language Recognition\\nLearned feature representations and sub-phoneme posteriors from Deep Neural\\nNetworks (DNNs) have been used separately to produce significant performance\\ngains for speaker and language recognition tasks. In this work we show how\\nthese gains are possible using a single DNN for both speaker and language\\nrecognition. The unified DNN approach is shown to yield substantial performance\\nimprovements on the the 2013 Domain Adaptation Challenge speaker recognition\\ntask (55% reduction in EER for the out-of-domain condition) and on the NIST\\n2011 Language Recognition Evaluation (48% reduction in EER for the 30s test\\ncondition).',\n",
              " 'Efficient Neural Architecture Search via Parameter Sharing\\nWe propose Efficient Neural Architecture Search (ENAS), a fast and\\ninexpensive approach for automatic model design. In ENAS, a controller learns\\nto discover neural network architectures by searching for an optimal subgraph\\nwithin a large computational graph. The controller is trained with policy\\ngradient to select a subgraph that maximizes the expected reward on the\\nvalidation set. Meanwhile the model corresponding to the selected subgraph is\\ntrained to minimize a canonical cross entropy loss. Thanks to parameter sharing\\nbetween child models, ENAS is fast: it delivers strong empirical performances\\nusing much fewer GPU-hours than all existing automatic model design approaches,\\nand notably, 1000x less expensive than standard Neural Architecture Search. On\\nthe Penn Treebank dataset, ENAS discovers a novel architecture that achieves a\\ntest perplexity of 55.8, establishing a new state-of-the-art among all methods\\nwithout post-training processing. On the CIFAR-10 dataset, ENAS designs novel\\narchitectures that achieve a test error of 2.89%, which is on par with NASNet\\n(Zoph et al., 2018), whose test error is 2.65%.',\n",
              " 'Building Machines That Learn and Think Like People\\nRecent progress in artificial intelligence (AI) has renewed interest in\\nbuilding systems that learn and think like people. Many advances have come from\\nusing deep neural networks trained end-to-end in tasks such as object\\nrecognition, video games, and board games, achieving performance that equals or\\neven beats humans in some respects. Despite their biological inspiration and\\nperformance achievements, these systems differ from human intelligence in\\ncrucial ways. We review progress in cognitive science suggesting that truly\\nhuman-like learning and thinking machines will have to reach beyond current\\nengineering trends in both what they learn, and how they learn it.\\nSpecifically, we argue that these machines should (a) build causal models of\\nthe world that support explanation and understanding, rather than merely\\nsolving pattern recognition problems; (b) ground learning in intuitive theories\\nof physics and psychology, to support and enrich the knowledge that is learned;\\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\\ngeneralize knowledge to new tasks and situations. We suggest concrete\\nchallenges and promising routes towards these goals that can combine the\\nstrengths of recent neural network advances with more structured cognitive\\nmodels.',\n",
              " 'Towards Bayesian Deep Learning: A Survey\\nWhile perception tasks such as visual object recognition and text\\nunderstanding play an important role in human intelligence, the subsequent\\ntasks that involve inference, reasoning and planning require an even higher\\nlevel of intelligence. The past few years have seen major advances in many\\nperception tasks using deep learning models. For higher-level inference,\\nhowever, probabilistic graphical models with their Bayesian nature are still\\nmore powerful and flexible. To achieve integrated intelligence that involves\\nboth perception and inference, it is naturally desirable to tightly integrate\\ndeep learning and Bayesian models within a principled probabilistic framework,\\nwhich we call Bayesian deep learning. In this unified framework, the perception\\nof text or images using deep learning can boost the performance of higher-level\\ninference and in return, the feedback from the inference process is able to\\nenhance the perception of text or images. This survey provides a general\\nintroduction to Bayesian deep learning and reviews its recent applications on\\nrecommender systems, topic models, and control. In this survey, we also discuss\\nthe relationship and differences between Bayesian deep learning and other\\nrelated topics like Bayesian treatment of neural networks.',\n",
              " \"Hierarchical Deep Reinforcement Learning: Integrating Temporal\\n  Abstraction and Intrinsic Motivation\\nLearning goal-directed behavior in environments with sparse feedback is a\\nmajor challenge for reinforcement learning algorithms. The primary difficulty\\narises due to insufficient exploration, resulting in an agent being unable to\\nlearn robust value functions. Intrinsically motivated agents can explore new\\nbehavior for its own sake rather than to directly solve problems. Such\\nintrinsic behaviors could eventually help the agent solve tasks posed by the\\nenvironment. We present hierarchical-DQN (h-DQN), a framework to integrate\\nhierarchical value functions, operating at different temporal scales, with\\nintrinsically motivated deep reinforcement learning. A top-level value function\\nlearns a policy over intrinsic goals, and a lower-level function learns a\\npolicy over atomic actions to satisfy the given goals. h-DQN allows for\\nflexible goal specifications, such as functions over entities and relations.\\nThis provides an efficient space for exploration in complicated environments.\\nWe demonstrate the strength of our approach on two problems with very sparse,\\ndelayed feedback: (1) a complex discrete stochastic decision process, and (2)\\nthe classic ATARI game `Montezuma's Revenge'.\",\n",
              " \"Learning Features by Watching Objects Move\\nThis paper presents a novel yet intuitive approach to unsupervised feature\\nlearning. Inspired by the human visual system, we explore whether low-level\\nmotion-based grouping cues can be used to learn an effective visual\\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\\nconvolutional network to segment objects from a single frame. Given the\\nextensive evidence that motion plays a key role in the development of the human\\nvisual system, we hope that this straightforward approach to unsupervised\\nlearning will be more effective than cleverly designed 'pretext' tasks studied\\nin the literature. Indeed, our extensive experiments show that this is the\\ncase. When used for transfer learning on object detection, our representation\\nsignificantly outperforms previous unsupervised approaches across multiple\\nsettings, especially when training data for the target task is scarce.\",\n",
              " 'Domain Adaptive Neural Networks for Object Recognition\\nWe propose a simple neural network model to deal with the domain adaptation\\nproblem in object recognition. Our model incorporates the Maximum Mean\\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\\nreduce the distribution mismatch between the source and target domains in the\\nlatent space. From experiments, we demonstrate that the MMD regularization is\\nan effective tool to provide good domain adaptation models on both SURF\\nfeatures and raw image pixels of a particular image data set. We also show that\\nour proposed model, preceded by the denoising auto-encoder pretraining,\\nachieves better performance than recent benchmark models on the same data sets.\\nThis work represents the first study of MMD measure in the context of neural\\nnetworks.',\n",
              " 'Beyond Temporal Pooling: Recurrence and Temporal Convolutions for\\n  Gesture Recognition in Video\\nRecent studies have demonstrated the power of recurrent neural networks for\\nmachine translation, image captioning and speech recognition. For the task of\\ncapturing temporal structure in video, however, there still remain numerous\\nopen research questions. Current research suggests using a simple temporal\\nfeature pooling strategy to take into account the temporal aspect of video. We\\ndemonstrate that this method is not sufficient for gesture recognition, where\\ntemporal information is more discriminative compared to general video\\nclassification tasks. We explore deep architectures for gesture recognition in\\nvideo and propose a new end-to-end trainable neural network architecture\\nincorporating temporal convolutions and bidirectional recurrence. Our main\\ncontributions are twofold; first, we show that recurrence is crucial for this\\ntask; second, we show that adding temporal convolutions leads to significant\\nimprovements. We evaluate the different approaches on the Montalbano gesture\\nrecognition dataset, where we achieve state-of-the-art results.',\n",
              " 'Telugu OCR Framework using Deep Learning\\nIn this paper, we address the task of Optical Character Recognition(OCR) for\\nthe Telugu script. We present an end-to-end framework that segments the text\\nimage, classifies the characters and extracts lines using a language model. The\\nsegmentation is based on mathematical morphology. The classification module,\\nwhich is the most challenging task of the three, is a deep convolutional neural\\nnetwork. The language is modelled as a third degree markov chain at the glyph\\nlevel. Telugu script is a complex alphasyllabary and the language is\\nagglutinative, making the problem hard. In this paper we apply the latest\\nadvances in neural networks to achieve state-of-the-art error rates. We also\\nreview convolutional neural networks in great detail and expound the\\nstatistical justification behind the many tricks needed to make Deep Learning\\nwork.',\n",
              " 'Adversarial Feature Learning\\nThe ability of the Generative Adversarial Networks (GANs) framework to learn\\ngenerative models mapping from simple latent distributions to arbitrarily\\ncomplex data distributions has been demonstrated empirically, with compelling\\nresults showing that the latent space of such generators captures semantic\\nvariation in the data distribution. Intuitively, models trained to predict\\nthese semantic latent representations given data may serve as useful feature\\nrepresentations for auxiliary problems where semantics are relevant. However,\\nin their existing form, GANs have no means of learning the inverse mapping --\\nprojecting data back into the latent space. We propose Bidirectional Generative\\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\\ndemonstrate that the resulting learned feature representation is useful for\\nauxiliary supervised discrimination tasks, competitive with contemporary\\napproaches to unsupervised and self-supervised feature learning.',\n",
              " 'The Mythos of Model Interpretability\\nSupervised machine learning models boast remarkable predictive capabilities.\\nBut can you trust your model? Will it work in deployment? What else can it tell\\nyou about the world? We want models to be not only good, but interpretable. And\\nyet the task of interpretation appears underspecified. Papers provide diverse\\nand sometimes non-overlapping motivations for interpretability, and offer\\nmyriad notions of what attributes render models interpretable. Despite this\\nambiguity, many papers proclaim interpretability axiomatically, absent further\\nexplanation. In this paper, we seek to refine the discourse on\\ninterpretability. First, we examine the motivations underlying interest in\\ninterpretability, finding them to be diverse and occasionally discordant. Then,\\nwe address model properties and techniques thought to confer interpretability,\\nidentifying transparency to humans and post-hoc explanations as competing\\nnotions. Throughout, we discuss the feasibility and desirability of different\\nnotions, and question the oft-made assertions that linear models are\\ninterpretable and that deep neural networks are not.',\n",
              " 'Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a\\n  Changing World\\nIn this paper, we focus on online representation learning in non-stationary\\nenvironments which may require continuous adaptation of model architecture. We\\npropose a novel online dictionary-learning (sparse-coding) framework which\\nincorporates the addition and deletion of hidden units (dictionary elements),\\nand is inspired by the adult neurogenesis phenomenon in the dentate gyrus of\\nthe hippocampus, known to be associated with improved cognitive function and\\nadaptation to new environments. In the online learning setting, where new input\\ninstances arrive sequentially in batches, the neuronal-birth is implemented by\\nadding new units with random initial weights (random dictionary elements); the\\nnumber of new units is determined by the current performance (representation\\nerror) of the dictionary, higher error causing an increase in the birth rate.\\nNeuronal-death is implemented by imposing l1/l2-regularization (group sparsity)\\non the dictionary within the block-coordinate descent optimization at each\\niteration of our online alternating minimization scheme, which iterates between\\nthe code and dictionary updates. Finally, hidden unit connectivity adaptation\\nis facilitated by introducing sparsity in dictionary elements. Our empirical\\nevaluation on several real-life datasets (images and language) as well as on\\nsynthetic data demonstrates that the proposed approach can considerably\\noutperform the state-of-art fixed-size (nonadaptive) online sparse coding of\\nMairal et al. (2009) in the presence of nonstationary data. Moreover, we\\nidentify certain properties of the data (e.g., sparse inputs with nearly\\nnon-overlapping supports) and of the model (e.g., dictionary sparsity)\\nassociated with such improvements.',\n",
              " 'Borrowing Treasures from the Wealthy: Deep Transfer Learning through\\n  Selective Joint Fine-tuning\\nDeep neural networks require a large amount of labeled training data during\\nsupervised learning. However, collecting and labeling so much data might be\\ninfeasible in many cases. In this paper, we introduce a source-target selective\\njoint fine-tuning scheme for improving the performance of deep learning tasks\\nwith insufficient training data. In this scheme, a target learning task with\\ninsufficient training data is carried out simultaneously with another source\\nlearning task with abundant training data. However, the source learning task\\ndoes not use all existing training data. Our core idea is to identify and use a\\nsubset of training images from the original source learning task whose\\nlow-level characteristics are similar to those from the target learning task,\\nand jointly fine-tune shared convolutional layers for both tasks. Specifically,\\nwe compute descriptors from linear or nonlinear filter bank responses on\\ntraining images from both tasks, and use such descriptors to search for a\\ndesired subset of training samples for the source learning task.\\n  Experiments demonstrate that our selective joint fine-tuning scheme achieves\\nstate-of-the-art performance on multiple visual classification tasks with\\ninsufficient training data for deep learning. Such tasks include Caltech 256,\\nMIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to\\nfine-tuning without a source domain, the proposed method can improve the\\nclassification accuracy by 2% - 10% using a single model.',\n",
              " 'Aligned Image-Word Representations Improve Inductive Transfer Across\\n  Vision-Language Tasks\\nAn important goal of computer vision is to build systems that learn visual\\nrepresentations over time that can be applied to many tasks. In this paper, we\\ninvestigate a vision-language embedding as a core representation and show that\\nit leads to better cross-task transfer than standard multi-task learning. In\\nparticular, the task of visual recognition is aligned to the task of visual\\nquestion answering by forcing each to use the same word-region embeddings. We\\nshow this leads to greater inductive transfer from recognition to VQA than\\nstandard multitask learning. Visual recognition also improves, especially for\\ncategories that have relatively few recognition training labels but appear\\noften in the VQA setting. Thus, our paper takes a small step towards creating\\nmore general vision systems by showing the benefit of interpretable, flexible,\\nand trainable core representations.',\n",
              " 'Universal Adversarial Perturbations Against Semantic Image Segmentation\\nWhile deep learning is remarkably successful on perceptual tasks, it was also\\nshown to be vulnerable to adversarial perturbations of the input. These\\nperturbations denote noise added to the input that was generated specifically\\nto fool the system while being quasi-imperceptible for humans. More severely,\\nthere even exist universal perturbations that are input-agnostic but fool the\\nnetwork on the majority of inputs. While recent work has focused on image\\nclassification, this work proposes attacks against semantic image segmentation:\\nwe present an approach for generating (universal) adversarial perturbations\\nthat make the network yield a desired target segmentation as output. We show\\nempirically that there exist barely perceptible universal noise patterns which\\nresult in nearly the same predicted segmentation for arbitrary inputs.\\nFurthermore, we also show the existence of universal noise which removes a\\ntarget class (e.g., all pedestrians) from the segmentation while leaving the\\nsegmentation mostly unchanged otherwise.',\n",
              " 'The loss surface of deep and wide neural networks\\nWhile the optimization problem behind deep neural networks is highly\\nnon-convex, it is frequently observed in practice that training deep networks\\nseems possible without getting stuck in suboptimal points. It has been argued\\nthat this is the case as all local minima are close to being globally optimal.\\nWe show that this is (almost) true, in fact almost all local minima are\\nglobally optimal, for a fully connected network with squared loss and analytic\\nactivation function given that the number of hidden units of one layer of the\\nnetwork is larger than the number of training points and the network structure\\nfrom this layer on is pyramidal.',\n",
              " \"Semantically Decomposing the Latent Spaces of Generative Adversarial\\n  Networks\\nWe propose a new algorithm for training generative adversarial networks that\\njointly learns latent codes for both identities (e.g. individual humans) and\\nobservations (e.g. specific photographs). By fixing the identity portion of the\\nlatent codes, we can generate diverse images of the same subject, and by fixing\\nthe observation portion, we can traverse the manifold of subjects while\\nmaintaining contingent aspects such as lighting and pose. Our algorithm\\nfeatures a pairwise training scheme in which each sample from the generator\\nconsists of two images with a common identity code. Corresponding samples from\\nthe real dataset consist of two distinct photographs of the same subject. In\\norder to fool the discriminator, the generator must produce pairs that are\\nphotorealistic, distinct, and appear to depict the same individual. We augment\\nboth the DCGAN and BEGAN approaches with Siamese discriminators to facilitate\\npairwise training. Experiments with human judges and an off-the-shelf face\\nverification system demonstrate our algorithm's ability to generate convincing,\\nidentity-matched photographs.\",\n",
              " 'Variants of RMSProp and Adagrad with Logarithmic Regret Bounds\\nAdaptive gradient methods have become recently very popular, in particular as\\nthey have been shown to be useful in the training of deep neural networks. In\\nthis paper we have analyzed RMSProp, originally proposed for the training of\\ndeep neural networks, in the context of online convex optimization and show\\n$\\\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and\\nSC-RMSProp for which we show logarithmic regret bounds for strongly convex\\nfunctions. Finally, we demonstrate in the experiments that these new variants\\noutperform other adaptive gradient techniques or stochastic gradient descent in\\nthe optimization of strongly convex functions as well as in training of deep\\nneural networks.',\n",
              " 'ALICE: Towards Understanding Adversarial Learning for Joint Distribution\\n  Matching\\nWe investigate the non-identifiability issues associated with bidirectional\\nadversarial training for joint distribution matching. Within a framework of\\nconditional entropy, we propose both adversarial and non-adversarial approaches\\nto learn desirable matched joint distributions for unsupervised and supervised\\ntasks. We unify a broad family of adversarial models as joint distribution\\nmatching problems. Our approach stabilizes learning of unsupervised\\nbidirectional adversarial learning methods. Further, we introduce an extension\\nfor semi-supervised learning tasks. Theoretical results are validated in\\nsynthetic data and real-world applications.',\n",
              " 'A systematic study of the class imbalance problem in convolutional\\n  neural networks\\nIn this study, we systematically investigate the impact of class imbalance on\\nclassification performance of convolutional neural networks (CNNs) and compare\\nfrequently used methods to address the issue. Class imbalance is a common\\nproblem that has been comprehensively studied in classical machine learning,\\nyet very limited systematic research is available in the context of deep\\nlearning. In our study, we use three benchmark datasets of increasing\\ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of\\nimbalance on classification and perform an extensive comparison of several\\nmethods to address the issue: oversampling, undersampling, two-phase training,\\nand thresholding that compensates for prior class probabilities. Our main\\nevaluation metric is area under the receiver operating characteristic curve\\n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is\\nassociated with notable difficulties in the context of imbalanced data. Based\\non results from our experiments we conclude that (i) the effect of class\\nimbalance on classification performance is detrimental; (ii) the method of\\naddressing class imbalance that emerged as dominant in almost all analyzed\\nscenarios was oversampling; (iii) oversampling should be applied to the level\\nthat totally eliminates the imbalance, whereas undersampling can perform better\\nwhen the imbalance is only removed to some extent; (iv) as opposed to some\\nclassical machine learning models, oversampling does not necessarily cause\\noverfitting of CNNs; (v) thresholding should be applied to compensate for prior\\nclass probabilities when overall number of properly classified cases is of\\ninterest.',\n",
              " 'Regularization for Deep Learning: A Taxonomy\\nRegularization is one of the crucial ingredients of deep learning, yet the\\nterm regularization has various definitions, and regularization methods are\\noften studied separately from each other. In our work we present a systematic,\\nunifying taxonomy to categorize existing methods. We distinguish methods that\\naffect data, network architectures, error terms, regularization terms, and\\noptimization procedures. We do not provide all details about the listed\\nmethods; instead, we present an overview of how the methods can be sorted into\\nmeaningful categories and sub-categories. This helps revealing links and\\nfundamental similarities between them. Finally, we include practical\\nrecommendations both for users and for developers of new regularization\\nmethods.',\n",
              " 'Clustering with Deep Learning: Taxonomy and New Methods\\nClustering is a fundamental machine learning method. The quality of its\\nresults is dependent on the data distribution. For this reason, deep neural\\nnetworks can be used for learning better representations of the data. In this\\npaper, we propose a systematic taxonomy for clustering with deep learning, in\\naddition to a review of methods from the field. Based on our taxonomy, creating\\nnew methods is more straightforward. We also propose a new approach which is\\nbuilt on the taxonomy and surpasses some of the limitations of some previous\\nwork. Our experimental evaluation on image datasets shows that the method\\napproaches state-of-the-art clustering quality, and performs better in some\\ncases.',\n",
              " 'Coarse to fine non-rigid registration: a chain of scale-specific neural\\n  networks for multimodal image alignment with application to remote sensing\\nWe tackle here the problem of multimodal image non-rigid registration, which\\nis of prime importance in remote sensing and medical imaging. The difficulties\\nencountered by classical registration approaches include feature design and\\nslow optimization by gradient descent. By analyzing these methods, we note the\\nsignificance of the notion of scale. We design easy-to-train,\\nfully-convolutional neural networks able to learn scale-specific features. Once\\nchained appropriately, they perform global registration in linear time, getting\\nrid of gradient descent schemes by predicting directly the deformation.We show\\ntheir performance in terms of quality and speed through various tasks of remote\\nsensing multimodal image alignment. In particular, we are able to register\\ncorrectly cadastral maps of buildings as well as road polylines onto RGB\\nimages, and outperform current keypoint matching methods.',\n",
              " 'Describing Videos by Exploiting Temporal Structure\\nRecent progress in using recurrent neural networks (RNNs) for image\\ndescription has motivated the exploration of their application for video\\ndescription. However, while images are static, working with videos requires\\nmodeling their dynamic temporal structure and then properly integrating that\\ninformation into a natural language description. In this context, we propose an\\napproach that successfully takes into account both the local and global\\ntemporal structure of videos to produce descriptions. First, our approach\\nincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)\\nrepresentation of the short temporal dynamics. The 3-D CNN representation is\\ntrained on video action recognition tasks, so as to produce a representation\\nthat is tuned to human motion and behavior. Second we propose a temporal\\nattention mechanism that allows to go beyond local temporal modeling and learns\\nto automatically select the most relevant temporal segments given the\\ntext-generating RNN. Our approach exceeds the current state-of-art for both\\nBLEU and METEOR metrics on the Youtube2Text dataset. We also present results on\\na new, larger and more challenging dataset of paired video and natural language\\ndescriptions.',\n",
              " 'Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in\\n  the Blanks\\nHybrid methods that utilize both content and rating information are commonly\\nused in many recommender systems. However, most of them use either handcrafted\\nfeatures or the bag-of-words representation as a surrogate for the content\\ninformation but they are neither effective nor natural enough. To address this\\nproblem, we develop a collaborative recurrent autoencoder (CRAE) which is a\\ndenoising recurrent autoencoder (DRAE) that models the generation of content\\nsequences in the collaborative filtering (CF) setting. The model generalizes\\nrecent advances in recurrent deep learning from i.i.d. input to non-i.i.d.\\n(CF-based) input and provides a new denoising scheme along with a novel\\nlearnable pooling scheme for the recurrent autoencoder. To do this, we first\\ndevelop a hierarchical Bayesian model for the DRAE and then generalize it to\\nthe CF setting. The synergy between denoising and CF enables CRAE to make\\naccurate recommendations while learning to fill in the blanks in sequences.\\nExperiments on real-world datasets from different domains (CiteULike and\\nNetflix) show that, by jointly modeling the order-aware generation of sequences\\nfor the content information and performing CF for the ratings, CRAE is able to\\nsignificantly outperform the state of the art on both the recommendation task\\nbased on ratings and the sequence generation task based on content information.',\n",
              " 'Sentiment Classification using Images and Label Embeddings\\nIn this project we analysed how much semantic information images carry, and\\nhow much value image data can add to sentiment analysis of the text associated\\nwith the images. To better understand the contribution from images, we compared\\nmodels which only made use of image data, models which only made use of text\\ndata, and models which combined both data types. We also analysed if this\\napproach could help sentiment classifiers generalize to unknown sentiments.',\n",
              " 'Natural-Parameter Networks: A Class of Probabilistic Neural Networks\\nNeural networks (NN) have achieved state-of-the-art performance in various\\napplications. Unfortunately in applications where training data is\\ninsufficient, they are often prone to overfitting. One effective way to\\nalleviate this problem is to exploit the Bayesian approach by using Bayesian\\nneural networks (BNN). Another shortcoming of NN is the lack of flexibility to\\ncustomize different distributions for the weights and neurons according to the\\ndata, as is often done in probabilistic graphical models. To address these\\nproblems, we propose a class of probabilistic neural networks, dubbed\\nnatural-parameter networks (NPN), as a novel and lightweight Bayesian treatment\\nof NN. NPN allows the usage of arbitrary exponential-family distributions to\\nmodel the weights and neurons. Different from traditional NN and BNN, NPN takes\\ndistributions as input and goes through layers of transformation before\\nproducing distributions to match the target output distributions. As a Bayesian\\ntreatment, efficient backpropagation (BP) is performed to learn the natural\\nparameters for the distributions over both the weights and neurons. The output\\ndistributions of each layer, as byproducts, may be used as second-order\\nrepresentations for the associated tasks such as link prediction. Experiments\\non real-world datasets show that NPN can achieve state-of-the-art performance.',\n",
              " 'Learning to Perform Physics Experiments via Deep Reinforcement Learning\\nWhen encountering novel objects, humans are able to infer a wide range of\\nphysical properties such as mass, friction and deformability by interacting\\nwith them in a goal driven way. This process of active interaction is in the\\nsame spirit as a scientist performing experiments to discover hidden facts.\\nRecent advances in artificial intelligence have yielded machines that can\\nachieve superhuman performance in Go, Atari, natural language processing, and\\ncomplex control problems; however, it is not clear that these systems can rival\\nthe scientific intuition of even a young child. In this work we introduce a\\nbasic set of tasks that require agents to estimate properties such as mass and\\ncohesion of objects in an interactive simulated environment where they can\\nmanipulate the objects and observe the consequences. We found that state of art\\ndeep reinforcement learning methods can learn to perform the experiments\\nnecessary to discover such hidden properties. By systematically manipulating\\nthe problem difficulty and the cost incurred by the agent for performing\\nexperiments, we found that agents learn different strategies that balance the\\ncost of gathering information against the cost of making mistakes in different\\nsituations.',\n",
              " 'A Network-based End-to-End Trainable Task-oriented Dialogue System\\nTeaching machines to accomplish tasks by conversing naturally with humans is\\nchallenging. Currently, developing task-oriented dialogue systems requires\\ncreating multiple components and typically this involves either a large amount\\nof handcrafting, or acquiring costly labelled datasets to solve a statistical\\nlearning problem for each component. In this work we introduce a neural\\nnetwork-based text-in, text-out end-to-end trainable goal-oriented dialogue\\nsystem along with a new way of collecting dialogue data based on a novel\\npipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue\\nsystems easily and without making too many assumptions about the task at hand.\\nThe results show that the model can converse with human subjects naturally\\nwhilst helping them to accomplish tasks in a restaurant search domain.',\n",
              " 'A Factorization Machine Framework for Testing Bigram Embeddings in\\n  Knowledgebase Completion\\nEmbedding-based Knowledge Base Completion models have so far mostly combined\\ndistributed representations of individual entities or relations to compute\\ntruth scores of missing links. Facts can however also be represented using\\npairwise embeddings, i.e. embeddings for pairs of entities and relations. In\\nthis paper we explore such bigram embeddings with a flexible Factorization\\nMachine model and several ablations from it. We investigate the relevance of\\nvarious bigram types on the fb15k237 dataset and find relative improvements\\ncompared to a compositional model.',\n",
              " 'Neural Networks for Joint Sentence Classification in Medical Paper\\n  Abstracts\\nExisting models based on artificial neural networks (ANNs) for sentence\\nclassification often do not incorporate the context in which sentences appear,\\nand classify sentences individually. However, traditional sentence\\nclassification approaches have been shown to greatly benefit from jointly\\nclassifying subsequent sentences, such as with conditional random fields. In\\nthis work, we present an ANN architecture that combines the effectiveness of\\ntypical ANN models to classify sentences in isolation, with the strength of\\nstructured prediction. Our model achieves state-of-the-art results on two\\ndifferent datasets for sequential sentence classification in medical abstracts.',\n",
              " 'De-identification of Patient Notes with Recurrent Neural Networks\\nObjective: Patient notes in electronic health records (EHRs) may contain\\ncritical information for medical investigations. However, the vast majority of\\nmedical investigators can only access de-identified notes, in order to protect\\nthe confidentiality of patients. In the United States, the Health Insurance\\nPortability and Accountability Act (HIPAA) defines 18 types of protected health\\ninformation (PHI) that needs to be removed to de-identify patient notes. Manual\\nde-identification is impractical given the size of EHR databases, the limited\\nnumber of researchers with access to the non-de-identified notes, and the\\nfrequent mistakes of human annotators. A reliable automated de-identification\\nsystem would consequently be of high value.\\n  Materials and Methods: We introduce the first de-identification system based\\non artificial neural networks (ANNs), which requires no handcrafted features or\\nrules, unlike existing systems. We compare the performance of the system with\\nstate-of-the-art systems on two datasets: the i2b2 2014 de-identification\\nchallenge dataset, which is the largest publicly available de-identification\\ndataset, and the MIMIC de-identification dataset, which we assembled and is\\ntwice as large as the i2b2 2014 dataset.\\n  Results: Our ANN model outperforms the state-of-the-art systems. It yields an\\nF1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision\\nof 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with\\na recall 99.25 and a precision of 99.06.\\n  Conclusion: Our findings support the use of ANNs for de-identification of\\npatient notes, as they show better performance than previously published\\nsystems while requiring no feature engineering.',\n",
              " \"Reasoning with Memory Augmented Neural Networks for Language\\n  Comprehension\\nHypothesis testing is an important cognitive process that supports human\\nreasoning. In this paper, we introduce a computational hypothesis testing\\napproach based on memory augmented neural networks. Our approach involves a\\nhypothesis testing loop that reconsiders and progressively refines a previously\\nformed hypothesis in order to generate new hypotheses to test. We apply the\\nproposed approach to language comprehension task by using Neural Semantic\\nEncoders (NSE). Our NSE models achieve the state-of-the-art results showing an\\nabsolute improvement of 1.2% to 2.6% accuracy over previous results obtained by\\nsingle and ensemble systems on standard machine comprehension benchmarks such\\nas the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.\",\n",
              " 'Automatic Rule Extraction from Long Short Term Memory Networks\\nAlthough deep learning models have proven effective at solving problems in\\nnatural language processing, the mechanism by which they come to their\\nconclusions is often unclear. As a result, these models are generally treated\\nas black boxes, yielding no insight of the underlying learned patterns. In this\\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\\napproach for tracking the importance of a given input to the LSTM for a given\\noutput. By identifying consistently important patterns of words, we are able to\\ndistill state of the art LSTMs on sentiment analysis and question answering\\ninto a set of representative phrases. This representation is then\\nquantitatively validated by using the extracted phrases to construct a simple,\\nrule-based classifier which approximates the output of the LSTM.',\n",
              " \"Comparing Rule-Based and Deep Learning Models for Patient Phenotyping\\nObjective: We investigate whether deep learning techniques for natural\\nlanguage processing (NLP) can be used efficiently for patient phenotyping.\\nPatient phenotyping is a classification task for determining whether a patient\\nhas a medical condition, and is a crucial part of secondary analysis of\\nhealthcare data. We assess the performance of deep learning algorithms and\\ncompare them with classical NLP approaches.\\n  Materials and Methods: We compare convolutional neural networks (CNNs),\\nn-gram models, and approaches based on cTAKES that extract pre-defined medical\\nconcepts from clinical notes and use them to predict patient phenotypes. The\\nperformance is tested on 10 different phenotyping tasks using 1,610 discharge\\nsummaries extracted from the MIMIC-III database.\\n  Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The\\naverage F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our\\nmodel having an F1-score up to 37 points higher than alternative approaches. We\\nadditionally assess the interpretability of our model by presenting a method\\nthat extracts the most salient phrases for a particular prediction.\\n  Conclusion: We show that NLP methods based on deep learning improve the\\nperformance of patient phenotyping. Our CNN-based algorithm automatically\\nlearns the phrases associated with each patient phenotype. As such, it reduces\\nthe annotation complexity for clinical domain experts, who are normally\\nrequired to develop task-specific annotation rules and identify relevant\\nphrases. Our method performs well in terms of both performance and\\ninterpretability, which indicates that deep learning is an effective approach\\nto patient phenotyping based on clinicians' notes.\",\n",
              " 'MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional\\n  Neural Networks\\nOver 50 million scholarly articles have been published: they constitute a\\nunique repository of knowledge. In particular, one may infer from them\\nrelations between scientific concepts, such as synonyms and hyponyms.\\nArtificial neural networks have been recently explored for relation extraction.\\nIn this work, we continue this line of work and present a system based on a\\nconvolutional neural network to extract relations. Our model ranked first in\\nthe SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific\\narticles (subtask C).',\n",
              " 'Transfer Learning for Named-Entity Recognition with Neural Networks\\nRecent approaches based on artificial neural networks (ANNs) have shown\\npromising results for named-entity recognition (NER). In order to achieve high\\nperformances, ANNs need to be trained on a large labeled dataset. However,\\nlabels might be difficult to obtain for the dataset on which the user wants to\\nperform NER: label scarcity is particularly pronounced for patient note\\nde-identification, which is an instance of NER. In this work, we analyze to\\nwhat extent transfer learning may address this issue. In particular, we\\ndemonstrate that transferring an ANN model trained on a large labeled dataset\\nto another dataset with a limited number of labels improves upon the\\nstate-of-the-art results on two different datasets for patient note\\nde-identification.',\n",
              " 'Adversarial Generation of Natural Language\\nGenerative Adversarial Networks (GANs) have gathered a lot of attention from\\nthe computer vision community, yielding impressive results for image\\ngeneration. Advances in the adversarial generation of natural language from\\nnoise however are not commensurate with the progress made in generating images,\\nand still lag far behind likelihood based methods. In this paper, we take a\\nstep towards generating natural language with a GAN objective alone. We\\nintroduce a simple baseline that addresses the discrete output space problem\\nwithout relying on gradient estimators and show that it is able to achieve\\nstate-of-the-art results on a Chinese poem generation dataset. We present\\nquantitative results on generating sentences from context-free and\\nprobabilistic context-free grammars, and qualitative language modeling results.\\nA conditional version is also described that can generate sequences conditioned\\non sentence characteristics.',\n",
              " 'Explaining Recurrent Neural Network Predictions in Sentiment Analysis\\nRecently, a technique called Layer-wise Relevance Propagation (LRP) was shown\\nto deliver insightful explanations in the form of input space relevances for\\nunderstanding feed-forward neural network classification decisions. In the\\npresent work, we extend the usage of LRP to recurrent neural networks. We\\npropose a specific propagation rule applicable to multiplicative connections as\\nthey arise in recurrent network architectures such as LSTMs and GRUs. We apply\\nour technique to a word-based bi-directional LSTM model on a five-class\\nsentiment prediction task, and evaluate the resulting LRP relevances both\\nqualitatively and quantitatively, obtaining better results than a\\ngradient-based related method which was used in previous work.',\n",
              " 'Text Compression for Sentiment Analysis via Evolutionary Algorithms\\nCan textual data be compressed intelligently without losing accuracy in\\nevaluating sentiment? In this study, we propose a novel evolutionary\\ncompression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression),\\nwhich makes use of Parts-of-Speech tags to compress text in a way that\\nsacrifices minimal classification accuracy when used in conjunction with\\nsentiment analysis algorithms. An analysis of PARSEC with eight commercial and\\nnon-commercial sentiment analysis algorithms on twelve English sentiment data\\nsets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss\\nin sentiment classification accuracy for (20%, 50%, 75%) data compression with\\nPARSEC using LingPipe, the most accurate of the sentiment algorithms. Other\\nsentiment analysis algorithms are more severely affected by compression. We\\nconclude that significant compression of text data is possible for sentiment\\nanalysis depending on the accuracy demands of the specific application and the\\nspecific sentiment analysis algorithm used.',\n",
              " 'Building competitive direct acoustics-to-word models for English\\n  conversational speech recognition\\nDirect acoustics-to-word (A2W) models in the end-to-end paradigm have\\nreceived increasing attention compared to conventional sub-word based automatic\\nspeech recognition models using phones, characters, or context-dependent hidden\\nMarkov model states. This is because A2W models recognize words from speech\\nwithout any decoder, pronunciation lexicon, or externally-trained language\\nmodel, making training and decoding with such models simple. Prior work has\\nshown that A2W models require orders of magnitude more training data in order\\nto perform comparably to conventional models. Our work also showed this\\naccuracy gap when using the English Switchboard-Fisher data set. This paper\\ndescribes a recipe to train an A2W model that closes this gap and is at-par\\nwith state-of-the-art sub-word based models. We achieve a word error rate of\\n8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder\\nor language model. We find that model initialization, training data order, and\\nregularization have the most impact on the A2W model performance. Next, we\\npresent a joint word-character A2W model that learns to first spell the word\\nand then recognize it. This model provides a rich output to the user instead of\\nsimple word hypotheses, making it especially useful in the case of words unseen\\nor rarely-seen during training.',\n",
              " 'Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for\\n  Visual Question Answering\\nWe address the problem of Visual Question Answering (VQA), which requires\\njoint image and language understanding to answer a question about a given\\nphotograph. Recent approaches have applied deep image captioning methods based\\non convolutional-recurrent networks to this problem, but have failed to model\\nspatial inference. To remedy this, we propose a model we call the Spatial\\nMemory Network and apply it to the VQA task. Memory networks are recurrent\\nneural networks with an explicit attention mechanism that selects certain parts\\nof the information stored in memory. Our Spatial Memory Network stores neuron\\nactivations from different spatial regions of the image in its memory, and uses\\nthe question to choose relevant regions for computing the answer, a process of\\nwhich constitutes a single \"hop\" in the network. We propose a novel spatial\\nattention architecture that aligns words with image patches in the first hop,\\nand obtain improved results by adding a second attention hop which considers\\nthe whole question to choose visual evidence based on the results of the first\\nhop. To better understand the inference process learned by the network, we\\ndesign synthetic questions that specifically require spatial inference and\\nvisualize the attention weights. We evaluate our model on two published visual\\nquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improved\\nresults compared to a strong deep baseline model (iBOWIMG) which concatenates\\nimage and question features to predict the answer [3].',\n",
              " 'Task-driven Visual Saliency and Attention-based Visual Question\\n  Answering\\nVisual question answering (VQA) has witnessed great progress since May, 2015\\nas a classic problem unifying visual and textual data into a system. Many\\nenlightening VQA works explore deep into the image and question encodings and\\nfusing methods, of which attention is the most effective and infusive\\nmechanism. Current attention based methods focus on adequate fusion of visual\\nand textual features, but lack the attention to where people focus to ask\\nquestions about the image. Traditional attention based methods attach a single\\nvalue to the feature at each spatial location, which losses many useful\\ninformation. To remedy these problems, we propose a general method to perform\\nsaliency-like pre-selection on overlapped region features by the interrelation\\nof bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication\\nbased attention method to capture more competent correlation information\\nbetween visual and textual features. We conduct experiments on the large-scale\\nCOCO-VQA dataset and analyze the effectiveness of our model demonstrated by\\nstrong empirical results.',\n",
              " 'Optimising The Input Window Alignment in CD-DNN Based Phoneme\\n  Recognition for Low Latency Processing\\nWe present a systematic analysis on the performance of a phonetic recogniser\\nwhen the window of input features is not symmetric with respect to the current\\nframe. The recogniser is based on Context Dependent Deep Neural Networks\\n(CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the\\nlatency of the system by reducing the number of future feature frames required\\nto estimate the current output. Our tests performed on the TIMIT database show\\nthat the performance does not degrade when the input window is shifted up to 5\\nframes in the past compared to common practice (no future frame). This\\ncorresponds to improving the latency by 50 ms in our settings. Our tests also\\nshow that the best results are not obtained with the symmetric window commonly\\nemployed, but with an asymmetric window with eight past and two future context\\nframes, although this observation should be confirmed on other data sets. The\\nreduction in latency suggested by our results is critical for specific\\napplications such as real-time lip synchronisation for tele-presence, but may\\nalso be beneficial in general applications to improve the lag in human-machine\\nspoken interaction.',\n",
              " 'Bridging LSTM Architecture and the Neural Dynamics during Reading\\nRecently, the long short-term memory neural network (LSTM) has attracted wide\\ninterest due to its success in many tasks. LSTM architecture consists of a\\nmemory cell and three gates, which looks similar to the neuronal networks in\\nthe brain. However, there still lacks the evidence of the cognitive\\nplausibility of LSTM architecture as well as its working mechanism. In this\\npaper, we study the cognitive plausibility of LSTM by aligning its internal\\narchitecture with the brain activity observed via fMRI when the subjects read a\\nstory. Experiment results show that the artificial memory vector in LSTM can\\naccurately predict the observed sequential brain activities, indicating the\\ncorrelation between LSTM architecture and the cognitive process of story\\nreading.',\n",
              " 'Feature Weight Tuning for Recursive Neural Networks\\nThis paper addresses how a recursive neural network model can automatically\\nleave out useless information and emphasize important evidence, in other words,\\nto perform \"weight tuning\" for higher-level representation acquisition. We\\npropose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural\\nNetwork (BENN), which automatically control how much one specific unit\\ncontributes to the higher-level representation. The proposed model can be\\nviewed as incorporating a more powerful compositional function for embedding\\nacquisition in recursive neural networks. Experimental results demonstrate the\\nsignificant improvement over standard neural models.',\n",
              " 'A New Data Representation Based on Training Data Characteristics to\\n  Extract Drug Named-Entity in Medical Text\\nOne essential task in information extraction from the medical corpus is drug\\nname recognition. Compared with text sources come from other domains, the\\nmedical text is special and has unique characteristics. In addition, the\\nmedical text mining poses more challenges, e.g., more unstructured text, the\\nfast growing of new terms addition, a wide range of name variation for the same\\ndrug. The mining is even more challenging due to the lack of labeled dataset\\nsources and external knowledge, as well as multiple token representations for a\\nsingle drug name that is more common in the real application setting. Although\\nmany approaches have been proposed to overwhelm the task, some problems\\nremained with poor F-score performance (less than 0.75). This paper presents a\\nnew treatment in data representation techniques to overcome some of those\\nchallenges. We propose three data representation techniques based on the\\ncharacteristics of word distribution and word similarities as a result of word\\nembedding training. The first technique is evaluated with the standard NN\\nmodel, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two\\ndeep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked\\nDenoising Encoders). The third technique represents the sentence as a sequence\\nthat is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term\\nMemory). In extracting the drug name entities, the third technique gives the\\nbest F-score performance compared to the state of the art, with its average\\nF-score being 0.8645.',\n",
              " 'DopeLearning: A Computational Approach to Rap Lyrics Generation\\nWriting rap lyrics requires both creativity to construct a meaningful,\\ninteresting story and lyrical skills to produce complex rhyme patterns, which\\nform the cornerstone of good flow. We present a rap lyrics generation method\\nthat captures both of these aspects. First, we develop a prediction model to\\nidentify the next line of existing lyrics from a set of candidate next lines.\\nThis model is based on two machine-learning techniques: the RankSVM algorithm\\nand a deep neural network model with a novel structure. Results show that the\\nprediction model can identify the true next line among 299 randomly selected\\nlines with an accuracy of 17%, i.e., over 50 times more likely than by random.\\nSecond, we employ the prediction model to combine lines from existing songs,\\nproducing lyrics with rhyme and a meaning. An evaluation of the produced lyrics\\nshows that in terms of quantitative rhyme density, the method outperforms the\\nbest human rappers by 21%. The rap lyrics generator has been deployed as an\\nonline tool called DeepBeat, and the performance of the tool has been assessed\\nby analyzing its usage logs. This analysis shows that machine-learned rankings\\ncorrelate with user preferences.',\n",
              " 'Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN\\nSemantic matching, which aims to determine the matching degree between two\\ntexts, is a fundamental problem for many NLP applications. Recently, deep\\nlearning approach has been applied to this problem and significant improvements\\nhave been achieved. In this paper, we propose to view the generation of the\\nglobal interaction between two texts as a recursive process: i.e. the\\ninteraction of two texts at each position is a composition of the interactions\\nbetween their prefixes as well as the word level interaction at the current\\nposition. Based on this idea, we propose a novel deep architecture, namely\\nMatch-SRNN, to model the recursive matching structure. Firstly, a tensor is\\nconstructed to capture the word level interactions. Then a spatial RNN is\\napplied to integrate the local interactions recursively, with importance\\ndetermined by four types of gates. Finally, the matching score is calculated\\nbased on the global interaction. We show that, after degenerated to the exact\\nmatching scenario, Match-SRNN can approximate the dynamic programming process\\nof longest common subsequence. Thus, there exists a clear interpretation for\\nMatch-SRNN. Our experiments on two semantic matching tasks showed the\\neffectiveness of Match-SRNN, and its ability of visualizing the learned\\nmatching structure.',\n",
              " 'Piecewise Latent Variables for Neural Variational Text Processing\\nAdvances in neural variational inference have facilitated the learning of\\npowerful directed graphical models with continuous latent variables, such as\\nvariational autoencoders. The hope is that such models will learn to represent\\nrich, multi-modal latent factors in real-world data, such as natural language\\ntext. However, current models often assume simplistic priors on the latent\\nvariables - such as the uni-modal Gaussian distribution - which are incapable\\nof representing complex latent factors efficiently. To overcome this\\nrestriction, we propose the simple, but highly flexible, piecewise constant\\ndistribution. This distribution has the capacity to represent an exponential\\nnumber of modes of a latent target distribution, while remaining mathematically\\ntractable. Our results demonstrate that incorporating this new latent\\ndistribution into different models yields substantial improvements in natural\\nlanguage processing tasks such as document modeling and natural language\\ngeneration for dialogue.',\n",
              " 'Recurrent Neural Networks with External Memory for Language\\n  Understanding\\nRecurrent Neural Networks (RNNs) have become increasingly popular for the\\ntask of language understanding. In this task, a semantic tagger is deployed to\\nassociate a semantic label to each word in an input sequence. The success of\\nRNN may be attributed to its ability to memorize long-term dependence that\\nrelates the current-time semantic label prediction to the observations many\\ntime instances away. However, the memory capacity of simple RNNs is limited\\nbecause of the gradient vanishing and exploding problem. We propose to use an\\nexternal memory to improve memorization capability of RNNs. We conducted\\nexperiments on the ATIS dataset, and observed that the proposed model was able\\nto achieve the state-of-the-art results. We compare our proposed model with\\nalternative models and report analysis results that may provide insights for\\nfuture research.',\n",
              " 'A Neural Network Approach to Context-Sensitive Generation of\\n  Conversational Responses\\nWe present a novel response generation system that can be trained end to end\\non large quantities of unstructured Twitter conversations. A neural network\\narchitecture is used to address sparsity issues that arise when integrating\\ncontextual information into classic statistical models, allowing the system to\\ntake into account previous dialog utterances. Our dynamic-context generative\\nmodels show consistent gains over both context-sensitive and\\nnon-context-sensitive Machine Translation and Information Retrieval baselines.',\n",
              " 'The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured\\n  Multi-Turn Dialogue Systems\\nThis paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost\\n1 million multi-turn dialogues, with a total of over 7 million utterances and\\n100 million words. This provides a unique resource for research into building\\ndialogue managers based on neural language models that can make use of large\\namounts of unlabeled data. The dataset has both the multi-turn property of\\nconversations in the Dialog State Tracking Challenge datasets, and the\\nunstructured nature of interactions from microblog services such as Twitter. We\\nalso describe two neural learning architectures suitable for analyzing this\\ndataset, and provide benchmark performance on the task of selecting the best\\nnext response.',\n",
              " 'Building End-To-End Dialogue Systems Using Generative Hierarchical\\n  Neural Network Models\\nWe investigate the task of building open domain, conversational dialogue\\nsystems based on large dialogue corpora using generative models. Generative\\nmodels produce system responses that are autonomously generated word-by-word,\\nopening up the possibility for realistic, flexible interactions. In support of\\nthis goal, we extend the recently proposed hierarchical recurrent\\nencoder-decoder neural network to the dialogue domain, and demonstrate that\\nthis model is competitive with state-of-the-art neural language models and\\nback-off n-gram models. We investigate the limitations of this and similar\\napproaches, and show how its performance can be improved by bootstrapping the\\nlearning from a larger question-answer pair corpus and from pretrained word\\nembeddings.',\n",
              " 'End-to-End Attention-based Large Vocabulary Speech Recognition\\nMany of the current state-of-the-art Large Vocabulary Continuous Speech\\nRecognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov\\nModels (HMMs). Most of these systems contain separate components that deal with\\nthe acoustic modelling, language modelling and sequence decoding. We\\ninvestigate a more direct approach in which the HMM is replaced with a\\nRecurrent Neural Network (RNN) that performs sequence prediction directly at\\nthe character level. Alignment between the input features and the desired\\ncharacter sequence is learned automatically by an attention mechanism built\\ninto the RNN. For each predicted character, the attention mechanism scans the\\ninput sequence and chooses relevant frames. We propose two methods to speed up\\nthis operation: limiting the scan to a subset of most promising frames and\\npooling over time the information contained in neighboring frames, thereby\\nreducing source sequence length. Integrating an n-gram language model into the\\ndecoding process yields recognition accuracies similar to other HMM-free\\nRNN-based approaches.',\n",
              " 'Towards Neural Network-based Reasoning\\nWe propose Neural Reasoner, a framework for neural network-based reasoning\\nover natural language sentences. Given a question, Neural Reasoner can infer\\nover multiple supporting facts and find an answer to the question in specific\\nforms. Neural Reasoner has 1) a specific interaction-pooling mechanism,\\nallowing it to examine multiple facts, and 2) a deep architecture, allowing it\\nto model the complicated logical relations in reasoning tasks. Assuming no\\nparticular structure exists in the question and facts, Neural Reasoner is able\\nto accommodate different types of reasoning and different forms of language\\nexpressions. Despite the model complexity, Neural Reasoner can still be trained\\neffectively in an end-to-end manner. Our empirical studies show that Neural\\nReasoner can outperform existing neural reasoning systems with remarkable\\nmargins on two difficult artificial tasks (Positional Reasoning and Path\\nFinding) proposed in [8]. For example, it improves the accuracy on Path\\nFinding(10K) from 33.4% [6] to over 98%.',\n",
              " 'What to talk about and how? Selective Generation using LSTMs with\\n  Coarse-to-Fine Alignment\\nWe propose an end-to-end, domain-independent neural encoder-aligner-decoder\\nmodel for selective generation, i.e., the joint task of content selection and\\nsurface realization. Our model first encodes a full set of over-determined\\ndatabase event records via an LSTM-based recurrent neural network, then\\nutilizes a novel coarse-to-fine aligner to identify the small subset of salient\\nrecords to talk about, and finally employs a decoder to generate free-form\\ndescriptions of the aligned, selected records. Our model achieves the best\\nselection and generation results reported to-date (with 59% relative\\nimprovement in generation) on the benchmark WeatherGov dataset, despite using\\nno specialized features or linguistic resources. Using an improved k-nearest\\nneighbor beam filter helps further. We also perform a series of ablations and\\nvisualizations to elucidate the contributions of our key model components.\\nLastly, we evaluate the generalizability of our model on the RoboCup dataset,\\nand get results that are competitive with or better than the state-of-the-art,\\ndespite being severely data-starved.',\n",
              " 'Reasoning about Entailment with Neural Attention\\nWhile most approaches to automatically recognizing entailment relations have\\nused classifiers employing hand engineered features derived from complex\\nnatural language processing pipelines, in practice their performance has been\\nonly slightly better than bag-of-word pair classifiers using only lexical\\nsimilarity. The only attempt so far to build an end-to-end differentiable\\nneural network for entailment failed to outperform such a simple similarity\\nclassifier. In this paper, we propose a neural model that reads two sentences\\nto determine entailment using long short-term memory units. We extend this\\nmodel with a word-by-word neural attention mechanism that encourages reasoning\\nover entailments of pairs of words and phrases. Furthermore, we present a\\nqualitative analysis of attention weights produced by this model, demonstrating\\nsuch reasoning capabilities. On a large entailment dataset this model\\noutperforms the previous best neural model and a classifier with engineered\\nfeatures by a substantial margin. It is the first generic end-to-end\\ndifferentiable system that achieves state-of-the-art accuracy on a textual\\nentailment dataset.',\n",
              " 'Highway Long Short-Term Memory RNNs for Distant Speech Recognition\\nIn this paper, we extend the deep long short-term memory (DLSTM) recurrent\\nneural networks by introducing gated direct connections between memory cells in\\nadjacent layers. These direct links, called highway connections, enable\\nunimpeded information flow across different layers and thus alleviate the\\ngradient vanishing problem when building deeper LSTMs. We further introduce the\\nlatency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole\\nhistory while keeping the latency under control. Efficient algorithms are\\nproposed to train these novel networks using both frame and sequence\\ndiscriminative criteria. Experiments on the AMI distant speech recognition\\n(DSR) task indicate that we can train deeper LSTMs and achieve better\\nimprovement from sequence training with highway LSTMs (HLSTMs). Our novel model\\nobtains $43.9/47.7\\\\%$ WER on AMI (SDM) dev and eval sets, outperforming all\\nprevious works. It beats the strong DNN and DLSTM baselines with $15.7\\\\%$ and\\n$5.3\\\\%$ relative improvement respectively.',\n",
              " 'Neural Enquirer: Learning to Query Tables with Natural Language\\nWe proposed Neural Enquirer as a neural network architecture to execute a\\nnatural language (NL) query on a knowledge-base (KB) for answers. Basically,\\nNeural Enquirer finds the distributed representation of a query and then\\nexecutes it on knowledge-base tables to obtain the answer as one of the values\\nin the tables. Unlike similar efforts in end-to-end training of semantic\\nparsers, Neural Enquirer is fully \"neuralized\": it not only gives\\ndistributional representation of the query and the knowledge-base, but also\\nrealizes the execution of compositional queries as a series of differentiable\\noperations, with intermediate results (consisting of annotations of the tables\\nat different levels) saved on multiple layers of memory. Neural Enquirer can be\\ntrained with gradient descent, with which not only the parameters of the\\ncontrolling components and semantic parsing component, but also the embeddings\\nof the tables and query words can be learned from scratch. The training can be\\ndone in an end-to-end fashion, but it can take stronger guidance, e.g., the\\nstep-by-step supervision for complicated queries, and benefit from it. Neural\\nEnquirer is one step towards building neural network systems which seek to\\nunderstand language by executing it on real-world. Our experiments show that\\nNeural Enquirer can learn to execute fairly complicated NL queries on tables\\nwith rich structures.',\n",
              " 'Sentence Pair Scoring: Towards Unified Framework for Text Comprehension\\nWe review the task of Sentence Pair Scoring, popular in the literature in\\nvarious forms - viewed as Answer Sentence Selection, Semantic Text Scoring,\\nNext Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a\\ncomponent of Memory Networks.\\n  We argue that all such tasks are similar from the model perspective and\\npropose new baselines by comparing the performance of common IR metrics and\\npopular convolutional, recurrent and attention-based neural models across many\\nSentence Pair Scoring tasks and datasets. We discuss the problem of evaluating\\nrandomized models, propose a statistically grounded methodology, and attempt to\\nimprove comparisons by releasing new datasets that are much harder than some of\\nthe currently used well explored benchmarks. We introduce a unified open source\\nsoftware framework with easily pluggable models and tasks, which enables us to\\nexperiment with multi-task reusability of trained sentence model. We set a new\\nstate-of-art in performance on the Ubuntu Dialogue dataset.',\n",
              " 'Incorporating Copying Mechanism in Sequence-to-Sequence Learning\\nWe address an important problem in sequence-to-sequence (Seq2Seq) learning\\nreferred to as copying, in which certain segments in the input sequence are\\nselectively replicated in the output sequence. A similar phenomenon is\\nobservable in human language communication. For example, humans tend to repeat\\nentity names or even long phrases in conversation. The challenge with regard to\\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\\nthe operation. In this paper, we incorporate copying into neural network-based\\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\\nstructure. CopyNet can nicely integrate the regular way of word generation in\\nthe decoder with the new copying mechanism which can choose sub-sequences in\\nthe input sequence and put them at proper places in the output sequence. Our\\nempirical study on both synthetic data sets and real world data sets\\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\\nregular RNN-based model with remarkable margins on text summarization tasks.',\n",
              " 'Generating Factoid Questions With Recurrent Neural Networks: The 30M\\n  Factoid Question-Answer Corpus\\nOver the past decade, large-scale supervised learning corpora have enabled\\nmachine learning researchers to make substantial advances. However, to this\\ndate, there are no large-scale question-answer corpora available. In this paper\\nwe present the 30M Factoid Question-Answer Corpus, an enormous question answer\\npair corpus produced by applying a novel neural network architecture on the\\nknowledge base Freebase to transduce facts into natural language questions. The\\nproduced question answer pairs are evaluated both by human evaluators and using\\nautomatic evaluation metrics, including well-established machine translation\\nand sentence similarity metrics. Across all evaluation criteria the\\nquestion-generation model outperforms the competing template-based baseline.\\nFurthermore, when presented to human evaluators, the generated questions appear\\ncomparable in quality to real human-generated questions.',\n",
              " \"How NOT To Evaluate Your Dialogue System: An Empirical Study of\\n  Unsupervised Evaluation Metrics for Dialogue Response Generation\\nWe investigate evaluation metrics for dialogue response generation systems\\nwhere supervised labels, such as task completion, are not available. Recent\\nworks in response generation have adopted metrics from machine translation to\\ncompare a model's generated response to a single target response. We show that\\nthese metrics correlate very weakly with human judgements in the non-technical\\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\\nquantitative and qualitative results highlighting specific weaknesses in\\nexisting metrics, and provide recommendations for future development of better\\nautomatic evaluation metrics for dialogue systems.\",\n",
              " 'A Hierarchical Latent Variable Encoder-Decoder Model for Generating\\n  Dialogues\\nSequential data often possesses a hierarchical structure with complex\\ndependencies between subsequences, such as found between the utterances in a\\ndialogue. In an effort to model this kind of generative process, we propose a\\nneural network-based generative architecture, with latent stochastic variables\\nthat span a variable number of time steps. We apply the proposed model to the\\ntask of dialogue response generation and compare it with recent neural network\\narchitectures. We evaluate the model performance through automatic evaluation\\nmetrics and by carrying out a human evaluation. The experiments demonstrate\\nthat our model improves upon recently proposed models and that the latent\\nvariables facilitate the generation of long outputs and maintain the context.',\n",
              " 'Neural Associative Memory for Dual-Sequence Modeling\\nMany important NLP problems can be posed as dual-sequence or\\nsequence-to-sequence modeling tasks. Recent advances in building end-to-end\\nneural architectures have been highly successful in solving such tasks. In this\\nwork we propose a new architecture for dual-sequence modeling that is based on\\nassociative memory. We derive AM-RNNs, a recurrent associative memory (AM)\\nwhich augments generic recurrent neural networks (RNN). This architecture is\\nextended to the Dual AM-RNN which operates on two AMs at once. Our models\\nachieve very competitive results on textual entailment. A qualitative analysis\\ndemonstrates that long range dependencies between source and target-sequence\\ncan be bridged effectively using Dual AM-RNNs. However, an initial experiment\\non auto-encoding reveals that these benefits are not exploited by the system\\nwhen learning to solve sequence-to-sequence tasks which indicates that\\nadditional supervision or regularization is needed.',\n",
              " 'Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior\\n  Knowledge\\nWe introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural\\nNetworks that replaces the softmax output layer by a log-linear output layer,\\nof which the softmax is a special case. This conceptually simple move has two\\nmain advantages. First, it allows the learner to combat training data sparsity\\nby allowing it to model words (or more generally, output symbols) as complex\\ncombinations of attributes without requiring that each combination is directly\\nobserved in the training data (as the softmax does). Second, it permits the\\ninclusion of flexible prior knowledge in the form of a priori specified modular\\nfeatures, where the neural network component learns to dynamically control the\\nweights of a log-linear distribution exploiting these features.\\n  We conduct experiments in the domain of language modelling of French, that\\nexploit morphological prior knowledge and show an important decrease in\\nperplexity relative to a baseline RNN.\\n  We provide other motivating iillustrations, and finally argue that the\\nlog-linear and the neural-network components contribute complementary strengths\\nto the LL-RNN: the LL aspect allows the model to incorporate rich prior\\nknowledge, while the NN aspect, according to the \"representation learning\"\\nparadigm, allows the model to discover novel combination of characteristics.',\n",
              " \"Embracing data abundance: BookTest Dataset for Reading Comprehension\\nThere is a practically unlimited amount of natural language data available.\\nStill, recent work in text comprehension has focused on datasets which are\\nsmall relative to current computing possibilities. This article is making a\\ncase for the community to move to larger data and as a step in that direction\\nit is proposing the BookTest, a new dataset similar to the popular Children's\\nBook Test (CBT), however more than 60 times larger. We show that training on\\nthe new data improves the accuracy of our Attention-Sum Reader model on the\\noriginal CBT test data by a much larger margin than many recent attempts to\\nimprove the model architecture. On one version of the dataset our ensemble even\\nexceeds the human baseline provided by Facebook. We then show in our own human\\nstudy that there is still space for further improvement.\",\n",
              " \"Quasi-Recurrent Neural Networks\\nRecurrent neural networks are a powerful tool for modeling sequential data,\\nbut the dependence of each timestep's computation on the previous timestep's\\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\\nsequence modeling that alternates convolutional layers, which apply in parallel\\nacross timesteps, and a minimalist recurrent pooling function that applies in\\nparallel across channels. Despite lacking trainable recurrent layers, stacked\\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\\nsize. Due to their increased parallelism, they are up to 16 times faster at\\ntrain and test time. Experiments on language modeling, sentiment\\nclassification, and character-level neural machine translation demonstrate\\nthese advantages and underline the viability of QRNNs as a basic building block\\nfor a variety of sequence tasks.\",\n",
              " \"Input Switched Affine Networks: An RNN Architecture Designed for\\n  Interpretability\\nThere exist many problem domains where the interpretability of neural network\\nmodels is essential for deployment. Here we introduce a recurrent architecture\\ncomposed of input-switched affine transformations - in other words an RNN\\nwithout any explicit nonlinearities, but with input-dependent recurrent\\nweights. This simple form allows the RNN to be analyzed via straightforward\\nlinear methods: we can exactly characterize the linear contribution of each\\ninput to the model predictions; we can use a change-of-basis to disentangle\\ninput, output, and computational hidden unit subspaces; we can fully\\nreverse-engineer the architecture's solution to a simple task. Despite this\\nease of interpretation, the input switched affine network achieves reasonable\\nperformance on a text modeling tasks, and allows greater computational\\nefficiency than networks with standard nonlinearities.\",\n",
              " 'Frustratingly Short Attention Spans in Neural Language Modeling\\nNeural language models predict the next token using a latent representation\\nof the immediate token history. Recently, various methods for augmenting neural\\nlanguage models with an attention mechanism over a differentiable memory have\\nbeen proposed. For predicting the next token, these models query information\\nfrom a memory of the recent history which can facilitate learning mid- and\\nlong-range dependencies. However, conventional attention mechanisms used in\\nmemory-augmented neural language models produce a single output vector per time\\nstep. This vector is used both for predicting the next token as well as for the\\nkey and value of a differentiable memory of a token history. In this paper, we\\npropose a neural language model with a key-value attention mechanism that\\noutputs separate representations for the key and value of a differentiable\\nmemory, as well as for encoding the next-word distribution. This model\\noutperforms existing memory-augmented neural language models on two corpora.\\nYet, we found that our method mainly utilizes a memory of the five most recent\\noutput representations. This led to the unexpected main finding that a much\\nsimpler model based only on the concatenation of recent output representations\\nfrom previous time steps is on par with more sophisticated memory-augmented\\nneural language models.',\n",
              " 'A Structured Self-attentive Sentence Embedding\\nThis paper proposes a new model for extracting an interpretable sentence\\nembedding by introducing self-attention. Instead of using a vector, we use a\\n2-D matrix to represent the embedding, with each row of the matrix attending on\\na different part of the sentence. We also propose a self-attention mechanism\\nand a special regularization term for the model. As a side effect, the\\nembedding comes with an easy way of visualizing what specific parts of the\\nsentence are encoded into the embedding. We evaluate our model on 3 different\\ntasks: author profiling, sentiment classification, and textual entailment.\\nResults show that our model yields a significant performance gain compared to\\nother sentence embedding methods in all of the 3 tasks.',\n",
              " \"A Recurrent Neural Model with Attention for the Recognition of Chinese\\n  Implicit Discourse Relations\\nWe introduce an attention-based Bi-LSTM for Chinese implicit discourse\\nrelations and demonstrate that modeling argument pairs as a joint sequence can\\noutperform word order-agnostic approaches. Our model benefits from a partial\\nsampling scheme and is conceptually simple, yet achieves state-of-the-art\\nperformance on the Chinese Discourse Treebank. We also visualize its attention\\nactivity to illustrate the model's ability to selectively focus on the relevant\\nparts of an input sequence.\",\n",
              " 'Event Representations for Automated Story Generation with Deep Neural\\n  Nets\\nAutomated story generation is the problem of automatically selecting a\\nsequence of events, actions, or words that can be told as a story. We seek to\\ndevelop a system that can generate stories by learning everything it needs to\\nknow from textual story corpora. To date, recurrent neural networks that learn\\nlanguage models at character, word, or sentence levels have had little success\\ngenerating coherent stories. We explore the question of event representations\\nthat provide a mid-level of abstraction between words and sentences in order to\\nretain the semantic information of the original data while minimizing event\\nsparsity. We present a technique for preprocessing textual story data into\\nevent sequences. We then present a technique for automated story generation\\nwhereby we decompose the problem into the generation of successive events\\n(event2event) and the generation of natural language sentences from events\\n(event2sentence). We give empirical results comparing different event\\nrepresentations and their effects on event successor generation and the\\ntranslation of events to natural language.',\n",
              " \"A Joint Model for Question Answering and Question Generation\\nWe propose a generative machine comprehension model that learns jointly to\\nask and answer questions based on documents. The proposed model uses a\\nsequence-to-sequence framework that encodes the document and generates a\\nquestion (answer) given an answer (question). Significant improvement in model\\nperformance is observed empirically on the SQuAD corpus, confirming our\\nhypothesis that the model benefits from jointly learning to perform both tasks.\\nWe believe the joint model's novelty offers a new perspective on machine\\ncomprehension beyond architectural engineering, and serves as a first step\\ntowards autonomous information seeking.\",\n",
              " 'Learning Intrinsic Sparse Structures within Long Short-Term Memory\\nModel compression is significant for the wide adoption of Recurrent Neural\\nNetworks (RNNs) in both user devices possessing limited resources and business\\nclusters requiring quick responses to large-scale service requests. This work\\naims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the\\nsizes of basic structures within LSTM units, including input updates, gates,\\nhidden states, cell states and outputs. Independently reducing the sizes of\\nbasic structures can result in inconsistent dimensions among them, and\\nconsequently, end up with invalid LSTM units. To overcome the problem, we\\npropose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS\\nwill simultaneously decrease the sizes of all basic structures by one and\\nthereby always maintain the dimension consistency. By learning ISS within LSTM\\nunits, the obtained LSTMs remain regular while having much smaller basic\\nstructures. Based on group Lasso regularization, our method achieves 10.59x\\nspeedup without losing any perplexity of a language modeling of Penn TreeBank\\ndataset. It is also successfully evaluated through a compact model with only\\n2.69M weights for machine Question Answering of SQuAD dataset. Our approach is\\nsuccessfully extended to non- LSTM RNNs, like Recurrent Highway Networks\\n(RHNs). Our source code is publicly available at\\nhttps://github.com/wenwei202/iss-rnns',\n",
              " 'Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational\\n  Compositional Operators for Analogy Detection\\nRepresenting the semantic relations that exist between two given words (or\\nentities) is an important first step in a wide-range of NLP applications such\\nas analogical reasoning, knowledge base completion and relational information\\nretrieval. A simple, yet surprisingly accurate method for representing a\\nrelation between two words is to compute the vector offset (\\\\PairDiff) between\\ntheir corresponding word embeddings. Despite the empirical success, it remains\\nunclear as to whether \\\\PairDiff is the best operator for obtaining a relational\\nrepresentation from word embeddings. We conduct a theoretical analysis of\\ngeneralised bilinear operators that can be used to measure the $\\\\ell_{2}$\\nrelational distance between two word-pairs. We show that, if the word\\nembeddings are standardised and uncorrelated, such an operator will be\\nindependent of bilinear terms, and can be simplified to a linear form, where\\n\\\\PairDiff is a special case. For numerous word embedding types, we empirically\\nverify the uncorrelation assumption, demonstrating the general applicability of\\nour theoretical result. Moreover, we experimentally discover \\\\PairDiff from the\\nbilinear relation composition operator on several benchmark analogy datasets.',\n",
              " 'Object-oriented Neural Programming (OONP) for Document Understanding\\nWe propose Object-oriented Neural Programming (OONP), a framework for\\nsemantically parsing documents in specific domains. Basically, OONP reads a\\ndocument and parses it into a predesigned object-oriented data structure\\n(referred to as ontology in this paper) that reflects the domain-specific\\nsemantics of the document. An OONP parser models semantic parsing as a decision\\nprocess: a neural net-based Reader sequentially goes through the document, and\\nduring the process it builds and updates an intermediate ontology to summarize\\nits partial understanding of the text it covers. OONP supports a rich family of\\noperations (both symbolic and differentiable) for composing the ontology, and a\\nbig variety of forms (both symbolic and differentiable) for representing the\\nstate and the document. An OONP parser can be trained with supervision of\\ndifferent forms and strength, including supervised learning (SL) ,\\nreinforcement learning (RL) and hybrid of the two. Our experiments on both\\nsynthetic and real-world document parsing tasks have shown that OONP can learn\\nto handle fairly complicated ontology with training data of modest sizes.',\n",
              " 'A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering\\nThis paper proposes a novel neural machine reading model for open-domain\\nquestion answering at scale. Existing machine comprehension models typically\\nassume that a short piece of relevant text containing answers is already\\nidentified and given to the models, from which the models are designed to\\nextract answers. This assumption, however, is not realistic for building a\\nlarge-scale open-domain question answering system which requires both deep text\\nunderstanding and identifying relevant text from corpus simultaneously.\\n  In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates\\nboth passage ranking and answer extraction in one single framework. A Q&A\\nsystem based on this framework allows users to issue an open-domain question\\nwithout needing to provide a piece of text that must contain the answer.\\nExperiments show that the unified NCR model is able to outperform the\\nstates-of-the-art in both retrieval of relevant text and answer extraction.',\n",
              " 'Improving speech recognition by revising gated recurrent units\\nSpeech recognition is largely taking advantage of deep learning, showing that\\nsubstantial benefits can be obtained by modern Recurrent Neural Networks\\n(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which\\ntypically reach state-of-the-art performance in many tasks thanks to their\\nability to learn long-term dependencies and robustness to vanishing gradients.\\nNevertheless, LSTMs have a rather complex design with three multiplicative\\ngates, that might impair their efficient implementation. An attempt to simplify\\nLSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just\\ntwo multiplicative gates.\\n  This paper builds on these efforts by further revising GRUs and proposing a\\nsimplified architecture potentially more suitable for speech recognition. The\\ncontribution of this work is two-fold. First, we suggest to remove the reset\\ngate in the GRU design, resulting in a more efficient single-gate architecture.\\nSecond, we propose to replace tanh with ReLU activations in the state update\\nequations. Results show that, in our implementation, the revised architecture\\nreduces the per-epoch training time with more than 30% and consistently\\nimproves recognition performance across different tasks, input features, and\\nnoisy conditions when compared to a standard GRU.',\n",
              " 'Integrating planning for task-completion dialogue policy learning\\nTraining a task-completion dialogue agent with real users via reinforcement\\nlearning (RL) could be prohibitively expensive, because it requires many\\ninteractions with users. One alternative is to resort to a user simulator,\\nwhile the discrepancy of between simulated and real users makes the learned\\npolicy unreliable in practice. This paper addresses these challenges by\\nintegrating planning into the dialogue policy learning based on Dyna-Q\\nframework, and provides a more sample-efficient approach to learn the dialogue\\npolices. The proposed agent consists of a planner trained on-line with limited\\nreal user experience that can generate large amounts of simulated experience to\\nsupplement with limited real user experience, and a policy model trained on\\nthese hybrid experiences. The effectiveness of our approach is validated on a\\nmovie-booking task in both a simulation setting and a human-in-the-loop\\nsetting.',\n",
              " 'Building DNN Acoustic Models for Large Vocabulary Speech Recognition\\nDeep neural networks (DNNs) are now a central component of nearly all\\nstate-of-the-art speech recognition systems. Building neural network acoustic\\nmodels requires several design decisions including network architecture, size,\\nand training loss function. This paper offers an empirical investigation on\\nwhich aspects of DNN acoustic model design are most important for speech\\nrecognition system performance. We report DNN classifier performance and final\\nspeech recognizer word error rates, and compare DNNs using several metrics to\\nquantify factors influencing differences in task performance. Our first set of\\nexperiments use the standard Switchboard benchmark corpus, which contains\\napproximately 300 hours of conversational telephone speech. We compare standard\\nDNNs to convolutional networks, and present the first experiments using\\nlocally-connected, untied neural networks for acoustic modeling. We\\nadditionally build systems on a corpus of 2,100 hours of training data by\\ncombining the Switchboard and Fisher corpora. This larger corpus allows us to\\nmore thoroughly examine performance of large DNN models -- with up to ten times\\nmore parameters than those typically used in speech recognition systems. Our\\nresults suggest that a relatively simple DNN architecture and optimization\\ntechnique produces strong results. These findings, along with previous work,\\nhelp establish a set of best practices for building DNN hybrid speech\\nrecognition systems with maximum likelihood training. Our experiments in DNN\\noptimization additionally serve as a case study for training DNNs with\\ndiscriminative loss functions for speech tasks, as well as DNN classifiers more\\ngenerally.',\n",
              " 'Deep Recurrent Neural Networks for Acoustic Modelling\\nWe present a novel deep Recurrent Neural Network (RNN) model for acoustic\\nmodelling in Automatic Speech Recognition (ASR). We term our contribution as a\\nTC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with\\nTime Convolution (TC), followed by a Bidirectional Long Short-Term Memory\\n(BLSTM), and a final DNN. The first DNN acts as a feature processor to our\\nmodel, the BLSTM then generates a context from the sequence acoustic signal,\\nand the final DNN takes the context and models the posterior probabilities of\\nthe acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)\\neval92 task or more than 8% relative improvement over the baseline DNN models.',\n",
              " \"Regularizing RNNs by Stabilizing Activations\\nWe stabilize the activations of Recurrent Neural Networks (RNNs) by\\npenalizing the squared distance between successive hidden states' norms.\\n  This penalty term is an effective regularizer for RNNs including LSTMs and\\nIRNNs, improving performance on character-level language modeling and phoneme\\nrecognition, and outperforming weight noise and dropout.\\n  We achieve competitive performance (18.6\\\\% PER) on the TIMIT phoneme\\nrecognition task for RNNs evaluated without beam search or an RNN transducer.\\n  With this penalty term, IRNN can achieve similar performance to LSTM on\\nlanguage modeling, although adding the penalty term to the LSTM results in\\nsuperior performance.\\n  Our penalty term also prevents the exponential growth of IRNN's activations\\noutside of their training horizon, allowing them to generalize to much longer\\nsequences.\",\n",
              " 'Outrageously Large Neural Networks: The Sparsely-Gated\\n  Mixture-of-Experts Layer\\nThe capacity of a neural network to absorb information is limited by its\\nnumber of parameters. Conditional computation, where parts of the network are\\nactive on a per-example basis, has been proposed in theory as a way of\\ndramatically increasing model capacity without a proportional increase in\\ncomputation. In practice, however, there are significant algorithmic and\\nperformance challenges. In this work, we address these challenges and finally\\nrealize the promise of conditional computation, achieving greater than 1000x\\nimprovements in model capacity with only minor losses in computational\\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\\nsub-networks. A trainable gating network determines a sparse combination of\\nthese experts to use for each example. We apply the MoE to the tasks of\\nlanguage modeling and machine translation, where model capacity is critical for\\nabsorbing the vast quantities of knowledge available in the training corpora.\\nWe present model architectures in which a MoE with up to 137 billion parameters\\nis applied convolutionally between stacked LSTM layers. On large language\\nmodeling and machine translation benchmarks, these models achieve significantly\\nbetter results than state-of-the-art at lower computational cost.',\n",
              " 'Discourse-Based Objectives for Fast Unsupervised Sentence Representation\\n  Learning\\nThis work presents a novel objective function for the unsupervised training\\nof neural network sentence encoders. It exploits signals from paragraph-level\\ndiscourse coherence to train these models to understand text. Our objective is\\npurely discriminative, allowing us to train models many times faster than was\\npossible under prior methods, and it yields models which perform well in\\nextrinsic evaluations.',\n",
              " 'Learning Convolutional Text Representations for Visual Question\\n  Answering\\nVisual question answering is a recently proposed artificial intelligence task\\nthat requires a deep understanding of both images and texts. In deep learning,\\nimages are typically modeled through convolutional neural networks, and texts\\nare typically modeled through recurrent neural networks. While the requirement\\nfor modeling images is similar to traditional computer vision tasks, such as\\nobject recognition and image classification, visual question answering raises a\\ndifferent need for textual representation as compared to other natural language\\nprocessing tasks. In this work, we perform a detailed analysis on natural\\nlanguage questions in visual question answering. Based on the analysis, we\\npropose to rely on convolutional neural networks for learning textual\\nrepresentations. By exploring the various properties of convolutional neural\\nnetworks specialized for text data, such as width and depth, we present our\\n\"CNN Inception + Gate\" model. We show that our model improves question\\nrepresentations and thus the overall accuracy of visual question answering\\nmodels. We also show that the text representation requirement in visual\\nquestion answering is more complicated and comprehensive than that in\\nconventional natural language processing tasks, making it a better task to\\nevaluate textual representation methods. Shallow models like fastText, which\\ncan obtain comparable results with deep learning models in tasks like text\\nclassification, are not suitable in visual question answering.',\n",
              " 'Learning Phrase Representations using RNN Encoder-Decoder for\\n  Statistical Machine Translation\\nIn this paper, we propose a novel neural network model called RNN\\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\\nencodes a sequence of symbols into a fixed-length vector representation, and\\nthe other decodes the representation into another sequence of symbols. The\\nencoder and decoder of the proposed model are jointly trained to maximize the\\nconditional probability of a target sequence given a source sequence. The\\nperformance of a statistical machine translation system is empirically found to\\nimprove by using the conditional probabilities of phrase pairs computed by the\\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\\nQualitatively, we show that the proposed model learns a semantically and\\nsyntactically meaningful representation of linguistic phrases.',\n",
              " 'Recurrent Neural Network Training with Dark Knowledge Transfer\\nRecurrent neural networks (RNNs), particularly long short-term memory (LSTM),\\nhave gained much attention in automatic speech recognition (ASR). Although some\\nsuccessful stories have been reported, training RNNs remains highly\\nchallenging, especially with limited training data. Recent research found that\\na well-trained model can be used as a teacher to train other child models, by\\nusing the predictions generated by the teacher model as supervision. This\\nknowledge transfer learning has been employed to train simple neural nets with\\na complex one, so that the final performance can reach a level that is\\ninfeasible to obtain by regular training. In this paper, we employ the\\nknowledge transfer learning approach to train RNNs (precisely LSTM) using a\\ndeep neural network (DNN) model as the teacher. This is different from most of\\nthe existing research on knowledge transfer learning, since the teacher (DNN)\\nis assumed to be weaker than the child (RNN); however, our experiments on an\\nASR task showed that it works fairly well: without applying any tricks on the\\nlearning scheme, this approach can train RNNs successfully even with limited\\ntraining data.',\n",
              " 'Long Short-Term Memory Based Recurrent Neural Network Architectures for\\n  Large Vocabulary Speech Recognition\\nLong Short-Term Memory (LSTM) is a recurrent neural network (RNN)\\narchitecture that has been designed to address the vanishing and exploding\\ngradient problems of conventional RNNs. Unlike feedforward neural networks,\\nRNNs have cyclic connections making them powerful for modeling sequences. They\\nhave been successfully used for sequence labeling and sequence prediction\\ntasks, such as handwriting recognition, language modeling, phonetic labeling of\\nacoustic frames. However, in contrast to the deep neural networks, the use of\\nRNNs in speech recognition has been limited to phone recognition in small scale\\ntasks. In this paper, we present novel LSTM based RNN architectures which make\\nmore effective use of model parameters to train acoustic models for large\\nvocabulary speech recognition. We train and compare LSTM, RNN and DNN models at\\nvarious numbers of parameters and configurations. We show that LSTM models\\nconverge quickly and give state of the art speech recognition performance for\\nrelatively small sized models.',\n",
              " 'Monitoring Term Drift Based on Semantic Consistency in an Evolving\\n  Vector Field\\nBased on the Aristotelian concept of potentiality vs. actuality allowing for\\nthe study of energy and dynamics in language, we propose a field approach to\\nlexical analysis. Falling back on the distributional hypothesis to\\nstatistically model word meaning, we used evolving fields as a metaphor to\\nexpress time-dependent changes in a vector space model by a combination of\\nrandom indexing and evolving self-organizing maps (ESOM). To monitor semantic\\ndrifts within the observation period, an experiment was carried out on the term\\nspace of a collection of 12.8 million Amazon book reviews. For evaluation, the\\nsemantic consistency of ESOM term clusters was compared with their respective\\nneighbourhoods in WordNet, and contrasted with distances among term vectors by\\nrandom indexing. We found that at 0.05 level of significance, the terms in the\\nclusters showed a high level of semantic consistency. Tracking the drift of\\ndistributional patterns in the term space across time periods, we found that\\nconsistency decreased, but not at a statistically significant level. Our method\\nis highly scalable, with interpretations in philosophy.',\n",
              " 'Towards better decoding and language model integration in sequence to\\n  sequence models\\nThe recently proposed Sequence-to-Sequence (seq2seq) framework advocates\\nreplacing complex data processing pipelines, such as an entire automatic speech\\nrecognition system, with a single neural network trained in an end-to-end\\nfashion. In this contribution, we analyse an attention-based seq2seq speech\\nrecognition system that directly transcribes recordings into characters. We\\nobserve two shortcomings: overconfidence in its predictions and a tendency to\\nproduce incomplete transcriptions when language models are used. We propose\\npractical solutions to both problems achieving competitive speaker independent\\nword error rates on the Wall Street Journal dataset: without separate language\\nmodels we reach 10.6% WER, while together with a trigram language model, we\\nreach 6.7% WER.',\n",
              " 'Neural Machine Translation by Jointly Learning to Align and Translate\\nNeural machine translation is a recently proposed approach to machine\\ntranslation. Unlike the traditional statistical machine translation, the neural\\nmachine translation aims at building a single neural network that can be\\njointly tuned to maximize the translation performance. The models proposed\\nrecently for neural machine translation often belong to a family of\\nencoder-decoders and consists of an encoder that encodes a source sentence into\\na fixed-length vector from which a decoder generates a translation. In this\\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\\nimproving the performance of this basic encoder-decoder architecture, and\\npropose to extend this by allowing a model to automatically (soft-)search for\\nparts of a source sentence that are relevant to predicting a target word,\\nwithout having to form these parts as a hard segment explicitly. With this new\\napproach, we achieve a translation performance comparable to the existing\\nstate-of-the-art phrase-based system on the task of English-to-French\\ntranslation. Furthermore, qualitative analysis reveals that the\\n(soft-)alignments found by the model agree well with our intuition.',\n",
              " 'Overcoming the Curse of Sentence Length for Neural Machine Translation\\n  using Automatic Segmentation\\nThe authors of (Cho et al., 2014a) have shown that the recently introduced\\nneural network translation systems suffer from a significant drop in\\ntranslation quality when translating long sentences, unlike existing\\nphrase-based translation systems. In this paper, we propose a way to address\\nthis issue by automatically segmenting an input sentence into phrases that can\\nbe easily translated by the neural network translation model. Once each segment\\nhas been independently translated by the neural machine translation model, the\\ntranslated clauses are concatenated to form a final translation. Empirical\\nresults show a significant improvement in translation quality for long\\nsentences.',\n",
              " 'Transferring Knowledge from a RNN to a DNN\\nDeep Neural Network (DNN) acoustic models have yielded many state-of-the-art\\nresults in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent\\nNeural Network (RNN) models have been shown to outperform DNNs counterparts.\\nHowever, state-of-the-art DNN and RNN models tend to be impractical to deploy\\non embedded systems with limited computational capacity. Traditionally, the\\napproach for embedded platforms is to either train a small DNN directly, or to\\ntrain a small DNN that learns the output distribution of a large DNN. In this\\npaper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We\\nuse the RNN model to generate soft alignments and minimize the Kullback-Leibler\\ndivergence against the small DNN. The small DNN trained on the soft RNN\\nalignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task\\ncompared to a baseline 4.54 WER or more than 13% relative improvement.',\n",
              " 'Correlational Neural Networks\\nCommon Representation Learning (CRL), wherein different descriptions (or\\nviews) of the data are embedded in a common subspace, is receiving a lot of\\nattention recently. Two popular paradigms here are Canonical Correlation\\nAnalysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA\\nbased approaches learn a joint representation by maximizing correlation of the\\nviews when projected to the common subspace. AE based methods learn a common\\nrepresentation by minimizing the error of reconstructing the two views. Each of\\nthese approaches has its own advantages and disadvantages. For example, while\\nCCA based approaches outperform AE based approaches for the task of transfer\\nlearning, they are not as scalable as the latter. In this work we propose an AE\\nbased approach called Correlational Neural Network (CorrNet), that explicitly\\nmaximizes correlation among the views when projected to the common subspace.\\nThrough a series of experiments, we demonstrate that the proposed CorrNet is\\nbetter than the above mentioned approaches with respect to its ability to learn\\ncorrelated common representations. Further, we employ CorrNet for several cross\\nlanguage tasks and show that the representations learned using CorrNet perform\\nbetter than the ones learned using other state of the art approaches.',\n",
              " 'Attention-Based Models for Speech Recognition\\nRecurrent sequence generators conditioned on input data through an attention\\nmechanism have recently shown very good performance on a range of tasks in-\\ncluding machine translation, handwriting synthesis and image caption gen-\\neration. We extend the attention-mechanism with features needed for speech\\nrecognition. We show that while an adaptation of the model used for machine\\ntranslation in reaches a competitive 18.7% phoneme error rate (PER) on the\\nTIMIT phoneme recognition task, it can only be applied to utterances which are\\nroughly as long as the ones it was trained on. We offer a qualitative\\nexplanation of this failure and propose a novel and generic method of adding\\nlocation-awareness to the attention mechanism to alleviate this issue. The new\\nmethod yields a model that is robust to long inputs and achieves 18% PER in\\nsingle utterances and 20% in 10-times longer (repeated) utterances. Finally, we\\npropose a change to the at- tention mechanism that prevents it from\\nconcentrating too much on single frames, which further reduces PER to 17.6%\\nlevel.',\n",
              " 'Fast and Accurate Recurrent Neural Network Acoustic Models for Speech\\n  Recognition\\nWe have recently shown that deep Long Short-Term Memory (LSTM) recurrent\\nneural networks (RNNs) outperform feed forward deep neural networks (DNNs) as\\nacoustic models for speech recognition. More recently, we have shown that the\\nperformance of sequence trained context dependent (CD) hidden Markov model\\n(HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained\\nphone models initialized with connectionist temporal classification (CTC). In\\nthis paper, we present techniques that further improve performance of LSTM RNN\\nacoustic models for large vocabulary speech recognition. We show that frame\\nstacking and reduced frame rate lead to more accurate models and faster\\ndecoding. CD phone modeling leads to further improvements. We also present\\ninitial results for LSTM RNN models outputting words directly.',\n",
              " 'Listen, Attend and Spell\\nWe present Listen, Attend and Spell (LAS), a neural network that learns to\\ntranscribe speech utterances to characters. Unlike traditional DNN-HMM models,\\nthis model learns all the components of a speech recognizer jointly. Our system\\nhas two components: a listener and a speller. The listener is a pyramidal\\nrecurrent network encoder that accepts filter bank spectra as inputs. The\\nspeller is an attention-based recurrent network decoder that emits characters\\nas outputs. The network produces character sequences without making any\\nindependence assumptions between the characters. This is the key improvement of\\nLAS over previous end-to-end CTC models. On a subset of the Google voice search\\ntask, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a\\nlanguage model, and 10.3% with language model rescoring over the top 32 beams.\\nBy comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.',\n",
              " 'BlackOut: Speeding up Recurrent Neural Network Language Models With Very\\n  Large Vocabularies\\nWe propose BlackOut, an approximation algorithm to efficiently train massive\\nrecurrent neural network language models (RNNLMs) with million word\\nvocabularies. BlackOut is motivated by using a discriminative loss, and we\\ndescribe a new sampling strategy which significantly reduces computation while\\nimproving stability, sample efficiency, and rate of convergence. One way to\\nunderstand BlackOut is to view it as an extension of the DropOut strategy to\\nthe output layer, wherein we use a discriminative training loss and a weighted\\nsampling scheme. We also establish close connections between BlackOut,\\nimportance sampling, and noise contrastive estimation (NCE). Our experiments,\\non the recently released one billion word language modeling benchmark,\\ndemonstrate scalability and accuracy of BlackOut; we outperform the\\nstate-of-the art, and achieve the lowest perplexity scores on this dataset.\\nMoreover, unlike other established methods which typically require GPUs or CPU\\nclusters, we show that a carefully implemented version of BlackOut requires\\nonly 1-10 days on a single machine to train a RNNLM with a million word\\nvocabulary and billions of parameters on one billion words. Although we\\ndescribe BlackOut in the context of RNNLM training, it can be used to any\\nnetworks with large softmax output layers.',\n",
              " 'Character-based Neural Machine Translation\\nNeural Machine Translation (MT) has reached state-of-the-art results.\\nHowever, one of the main challenges that neural MT still faces is dealing with\\nvery large vocabularies and morphologically rich languages. In this paper, we\\npropose a neural MT system using character-based embeddings in combination with\\nconvolutional and highway layers to replace the standard lookup-based word\\nrepresentations. The resulting unlimited-vocabulary and affix-aware source word\\nembeddings are tested in a state-of-the-art neural MT based on an\\nattention-based bidirectional recurrent neural network. The proposed MT scheme\\nprovides improved results even when the source language is not morphologically\\nrich. Improvements up to 3 BLEU points are obtained in the German-English WMT\\ntask.',\n",
              " 'A Latent Variable Recurrent Neural Network for Discourse Relation\\n  Language Models\\nThis paper presents a novel latent variable recurrent neural network\\narchitecture for jointly modeling sequences of words and (possibly latent)\\ndiscourse relations between adjacent sentences. A recurrent neural network\\ngenerates individual words, thus reaping the benefits of\\ndiscriminatively-trained vector representations. The discourse relations are\\nrepresented with a latent variable, which can be predicted or marginalized,\\ndepending on the task. The resulting model can therefore employ a training\\nobjective that includes not only discourse relation classification, but also\\nword prediction. As a result, it outperforms state-of-the-art alternatives for\\ntwo tasks: implicit discourse relation classification in the Penn Discourse\\nTreebank, and dialog act classification in the Switchboard corpus. Furthermore,\\nby marginalizing over latent discourse relations at test time, we obtain a\\ndiscourse informed language model, which improves over a strong LSTM baseline.',\n",
              " 'Multi-task Recurrent Model for Speech and Speaker Recognition\\nAlthough highly correlated, speech and speaker recognition have been regarded\\nas two independent tasks and studied by two communities. This is certainly not\\nthe way that people behave: we decipher both speech content and speaker traits\\nat the same time. This paper presents a unified model to perform speech and\\nspeaker recognition simultaneously and altogether. The model is based on a\\nunified neural network where the output of one task is fed to the input of the\\nother, leading to a multi-task recurrent network. Experiments show that the\\njoint model outperforms the task-specific models on both the two tasks.',\n",
              " 'Hierarchical Memory Networks\\nMemory networks are neural networks with an explicit memory component that\\ncan be both read and written to by the network. The memory is often addressed\\nin a soft way using a softmax function, making end-to-end training with\\nbackpropagation possible. However, this is not computationally scalable for\\napplications which require the network to read from extremely large memories.\\nOn the other hand, it is well known that hard attention mechanisms based on\\nreinforcement learning are challenging to train successfully. In this paper, we\\nexplore a form of hierarchical memory network, which can be considered as a\\nhybrid between hard and soft attention memory networks. The memory is organized\\nin a hierarchical structure such that reading from it is done with less\\ncomputation than soft attention over a flat memory, while also being easier to\\ntrain than hard attention over a flat memory. Specifically, we propose to\\nincorporate Maximum Inner Product Search (MIPS) in the training and inference\\nprocedures for our hierarchical memory network. We explore the use of various\\nstate-of-the art approximate MIPS techniques and report results on\\nSimpleQuestions, a challenging large scale factoid question answering task.',\n",
              " 'Sequence-to-Sequence Learning as Beam-Search Optimization\\nSequence-to-Sequence (seq2seq) modeling has rapidly become an important\\ngeneral-purpose NLP tool that has proven effective for many text-generation and\\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\\ninherits its remarkable accuracy in estimating local, next-word distributions.\\nIn this work, we introduce a model and beam-search training scheme, based on\\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\\nsequence scores. This structured approach avoids classical biases associated\\nwith local training and unifies the training loss with the test-time usage,\\nwhile preserving the proven model architecture of seq2seq and its efficient\\ntraining approach. We show that our system outperforms a highly-optimized\\nattention-based seq2seq system and other baselines on three different sequence\\nto sequence tasks: word ordering, parsing, and machine translation.',\n",
              " 'Grounded Recurrent Neural Networks\\nIn this work, we present the Grounded Recurrent Neural Network (GRNN), a\\nrecurrent neural network architecture for multi-label prediction which\\nexplicitly ties labels to specific dimensions of the recurrent hidden state (we\\ncall this process \"grounding\"). The approach is particularly well-suited for\\nextracting large numbers of concepts from text. We apply the new model to\\naddress an important problem in healthcare of understanding what medical\\nconcepts are discussed in clinical text. Using a publicly available dataset\\nderived from Intensive Care Units, we learn to label a patient\\'s diagnoses and\\nprocedures from their discharge summary. Our evaluation shows a clear advantage\\nto using our proposed architecture over a variety of strong baselines.',\n",
              " 'Latent Intention Dialogue Models\\nDeveloping a dialogue agent that is capable of making autonomous decisions\\nand communicating by natural language is one of the long-term goals of machine\\nlearning research. Traditional approaches either rely on hand-crafting a small\\nstate-action set for applying reinforcement learning that is not scalable or\\nconstructing deterministic models for learning dialogue sentences that fail to\\ncapture natural conversational variability. In this paper, we propose a Latent\\nIntention Dialogue Model (LIDM) that employs a discrete latent variable to\\nlearn underlying dialogue intentions in the framework of neural variational\\ninference. In a goal-oriented dialogue scenario, these latent intentions can be\\ninterpreted as actions guiding the generation of machine responses, which can\\nbe further refined autonomously by reinforcement learning. The experimental\\nevaluation of LIDM shows that the model out-performs published benchmarks for\\nboth corpus-based and human evaluation, demonstrating the effectiveness of\\ndiscrete latent variable models for learning goal-oriented dialogues.',\n",
              " \"Transfer Learning for Speech Recognition on a Budget\\nEnd-to-end training of automated speech recognition (ASR) systems requires\\nmassive data and compute resources. We explore transfer learning based on model\\nadaptation as an approach for training ASR models under constrained GPU memory,\\nthroughput and training data. We conduct several systematic experiments\\nadapting a Wav2Letter convolutional neural network originally trained for\\nEnglish ASR to the German language. We show that this technique allows faster\\ntraining on consumer-grade resources while requiring less training data in\\norder to achieve the same accuracy, thereby lowering the cost of training ASR\\nmodels in other languages. Model introspection revealed that small adaptations\\nto the network's weights were sufficient for good performance, especially for\\ninner layers.\",\n",
              " 'Optimizing expected word error rate via sampling for speech recognition\\nState-level minimum Bayes risk (sMBR) training has become the de facto\\nstandard for sequence-level training of speech recognition acoustic models. It\\nhas an elegant formulation using the expectation semiring, and gives large\\nimprovements in word error rate (WER) over models trained solely using\\ncross-entropy (CE) or connectionist temporal classification (CTC). sMBR\\ntraining optimizes the expected number of frames at which the reference and\\nhypothesized acoustic states differ. It may be preferable to optimize the\\nexpected WER, but WER does not interact well with the expectation semiring, and\\nprevious approaches based on computing expected WER exactly involve expanding\\nthe lattices used during training. In this paper we show how to perform\\noptimization of the expected WER by sampling paths from the lattices used\\nduring conventional sMBR training. The gradient of the expected WER is itself\\nan expectation, and so may be approximated using Monte Carlo sampling. We show\\nexperimentally that optimizing WER during acoustic model training gives 5%\\nrelative improvement in WER over a well-tuned sMBR baseline on a 2-channel\\nquery recognition task (Google Home).',\n",
              " 'Neural Networks Compression for Language Modeling\\nIn this paper, we consider several compression techniques for the language\\nmodeling problem based on recurrent neural networks (RNNs). It is known that\\nconventional RNNs, e.g, LSTM-based networks in language modeling, are\\ncharacterized with either high space complexity or substantial inference time.\\nThis problem is especially crucial for mobile applications, in which the\\nconstant interaction with the remote server is inappropriate. By using the Penn\\nTreebank (PTB) dataset we compare pruning, quantization, low-rank\\nfactorization, tensor train decomposition for LSTM networks in terms of model\\nsize and suitability for fast inference.',\n",
              " 'Avoiding Your Teacher\\'s Mistakes: Training Neural Networks with\\n  Controlled Weak Supervision\\nTraining deep neural networks requires massive amounts of training data, but\\nfor many tasks only limited labeled data is available. This makes weak\\nsupervision attractive, using weak or noisy signals like the output of\\nheuristic methods or user click-through data for training. In a semi-supervised\\nsetting, we can use a large set of data with weak labels to pretrain a neural\\nnetwork and then fine-tune the parameters with a small amount of data with true\\nlabels. This feels intuitively sub-optimal as these two independent stages\\nleave the model unaware about the varying label quality. What if we could\\nsomehow inform the model about the label quality? In this paper, we propose a\\nsemi-supervised learning method where we train two neural networks in a\\nmulti-task fashion: a \"target network\" and a \"confidence network\". The target\\nnetwork is optimized to perform a given task and is trained using a large set\\nof unlabeled data that are weakly annotated. We propose to weight the gradient\\nupdates to the target network using the scores provided by the second\\nconfidence network, which is trained on a small amount of supervised data. Thus\\nwe avoid that the weight updates computed from noisy labels harm the quality of\\nthe target network model. We evaluate our learning strategy on two different\\ntasks: document ranking and sentiment classification. The results demonstrate\\nthat our approach not only enhances the performance compared to the baselines\\nbut also speeds up the learning process from weak labels.',\n",
              " 'Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy\\n  Optimisation\\nIn statistical dialogue management, the dialogue manager learns a policy that\\nmaps a belief state to an action for the system to perform. Efficient\\nexploration is key to successful policy optimisation. Current deep\\nreinforcement learning methods are very promising but rely on epsilon-greedy\\nexploration, thus subjecting the user to a random choice of action during\\nlearning. Alternative approaches such as Gaussian Process SARSA (GPSARSA)\\nestimate uncertainties and are sample efficient, leading to better user\\nexperience, but on the expense of a greater computational complexity. This\\npaper examines approaches to extract uncertainty estimates from deep Q-networks\\n(DQN) in the context of dialogue management. We perform an extensive benchmark\\nof deep Bayesian methods to extract uncertainty estimates, namely\\nBayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and\\nalpha-divergences, combining it with DQN algorithm.',\n",
              " 'On Extended Long Short-term Memory and Dependent Bidirectional Recurrent\\n  Neural Network\\nIn this work, we investigate the memory capability of recurrent neural\\nnetworks (RNNs), where this capability is defined as a function that maps an\\nelement in a sequence to the current output. We first analyze the system\\nfunction of a recurrent neural network (RNN) cell, and provide analytical\\nresults for three RNNs. They are the simple recurrent neural network (SRN), the\\nlong short-term memory (LSTM), and the gated recurrent unit (GRU). Based on the\\nanalysis, we propose a new design to extend the memory length of a cell, and\\ncall it the extended long short-term memory (ELSTM). Next, we present a\\ndependent bidirectional recurrent neural network (DBRNN) for the\\nsequence-in-sequence-out (SISO) problem, which is more robust to previous\\nerroneous predictions. Extensive experiments are carried out on different\\nlanguage tasks to demonstrate the superiority of our proposed ELSTM and DBRNN\\nsolutions.',\n",
              " 'Learning to Answer Questions From Image Using Convolutional Neural\\n  Network\\nIn this paper, we propose to employ the convolutional neural network (CNN)\\nfor the image question answering (QA). Our proposed CNN provides an end-to-end\\nframework with convolutional architectures for learning not only the image and\\nquestion representations, but also their inter-modal interactions to produce\\nthe answer. More specifically, our model consists of three CNNs: one image CNN\\nto encode the image content, one sentence CNN to compose the words of the\\nquestion, and one multimodal convolution layer to learn their joint\\nrepresentation for the classification in the space of candidate answer words.\\nWe demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA\\ndatasets, which are two benchmark datasets for the image QA, with the\\nperformances significantly outperforming the state-of-the-art.',\n",
              " 'Stacked Attention Networks for Image Question Answering\\nThis paper presents stacked attention networks (SANs) that learn to answer\\nnatural language questions from images. SANs use semantic representation of a\\nquestion as query to search for the regions in an image that are related to the\\nanswer. We argue that image question answering (QA) often requires multiple\\nsteps of reasoning. Thus, we develop a multiple-layer SAN in which we query an\\nimage multiple times to infer the answer progressively. Experiments conducted\\non four image QA data sets demonstrate that the proposed SANs significantly\\noutperform previous state-of-the-art approaches. The visualization of the\\nattention layers illustrates the progress that the SAN locates the relevant\\nvisual clues that lead to the answer of the question layer-by-layer.',\n",
              " 'Neural Module Networks\\nVisual question answering is fundamentally compositional in nature---a\\nquestion like \"where is the dog?\" shares substructure with questions like \"what\\ncolor is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously\\nexploit the representational capacity of deep networks and the compositional\\nlinguistic structure of questions. We describe a procedure for constructing and\\nlearning *neural module networks*, which compose collections of jointly-trained\\nneural \"modules\" into deep networks for question answering. Our approach\\ndecomposes questions into their linguistic substructures, and uses these\\nstructures to dynamically instantiate modular networks (with reusable\\ncomponents for recognizing dogs, classifying colors, etc.). The resulting\\ncompound networks are jointly trained. We evaluate our approach on two\\nchallenging datasets for visual question answering, achieving state-of-the-art\\nresults on both the VQA natural image dataset and a new dataset of complex\\nquestions about abstract shapes.',\n",
              " 'Symbol Grounding Association in Multimodal Sequences with Missing\\n  Elements\\nIn this paper, we extend a symbolic association framework for being able to\\nhandle missing elements in multimodal sequences. The general scope of the work\\nis the symbolic associations of object-word mappings as it happens in language\\ndevelopment in infants. In other words, two different representations of the\\nsame abstract concepts can associate in both directions. This scenario has been\\nlong interested in Artificial Intelligence, Psychology, and Neuroscience. In\\nthis work, we extend a recent approach for multimodal sequences (visual and\\naudio) to also cope with missing elements in one or both modalities. Our method\\nuses two parallel Long Short-Term Memories (LSTMs) with a learning rule based\\non EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We\\npropose to include an extra step for the combination with the max operation for\\nexploiting the common elements between both sequences. The motivation behind is\\nthat the combination acts as a condition selector for choosing the best\\nrepresentation from both LSTMs. We evaluated the proposed extension in the\\nfollowing scenarios: missing elements in one modality (visual or audio) and\\nmissing elements in both modalities (visual and sound). The performance of our\\nextension reaches better results than the original model and similar results to\\nindividual LSTM trained in each modality.',\n",
              " 'Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe\\n  Noise\\nThe growing importance of massive datasets with the advent of deep learning\\nmakes robustness to label noise a critical property for classifiers to have.\\nSources of label noise include automatic labeling for large datasets,\\nnon-expert labeling, and label corruption by data poisoning adversaries. In the\\nlatter case, corruptions may be arbitrarily bad, even so bad that a classifier\\npredicts the wrong labels with high confidence. To protect against such sources\\nof noise, we leverage the fact that a small set of clean labels is often easy\\nto procure. We demonstrate that robustness to label noise up to severe\\nstrengths can be achieved by using a set of trusted data with clean labels, and\\npropose a loss correction that utilizes trusted examples in a data-efficient\\nmanner to mitigate the effects of label noise on deep neural network\\nclassifiers. Across vision and natural language processing tasks, we experiment\\nwith various label noises at several strengths, and show that our method\\nsignificantly outperforms existing methods.',\n",
              " 'Describing Multimedia Content using Attention-based Encoder--Decoder\\n  Networks\\nWhereas deep neural networks were first mostly used for classification tasks,\\nthey are rapidly expanding in the realm of structured output problems, where\\nthe observed target is composed of multiple random variables that have a rich\\njoint distribution, given the input. We focus in this paper on the case where\\nthe input also has a rich structure and the input and output structures are\\nsomehow related. We describe systems that learn to attend to different places\\nin the input, for each element of the output, for a variety of tasks: machine\\ntranslation, image caption generation, video clip description and speech\\nrecognition. All these systems are based on a shared set of building blocks:\\ngated recurrent neural networks and convolutional neural networks, along with\\ntrained attention mechanisms. We report on experimental results with these\\nsystems, showing impressively good performance and the advantage of the\\nattention mechanism.',\n",
              " 'Multilingual Image Description with Neural Sequence Models\\nIn this paper we present an approach to multi-language image description\\nbringing together insights from neural machine translation and neural image\\ndescription. To create a description of an image for a given target language,\\nour sequence generation models condition on feature vectors from the image, the\\ndescription from the source language, and/or a multimodal vector computed over\\nthe image and a description in the source language. In image description\\nexperiments on the IAPR-TC12 dataset of images aligned with English and German\\nsentences, we find significant and substantial improvements in BLEU4 and Meteor\\nscores for models trained over multiple languages, compared to a monolingual\\nbaseline.',\n",
              " 'Deep Embedding for Spatial Role Labeling\\nThis paper introduces the visually informed embedding of word (VIEW), a\\ncontinuous vector representation for a word extracted from a deep neural model\\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\\nbetween visual objects, given a textual description. The model is composed of a\\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\\napplied to transferring multimodal background knowledge to Spatial Role\\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\\nmentioned in the text. This work also contributes with a new method to select\\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.',\n",
              " 'Image-to-Markup Generation with Coarse-to-Fine Attention\\nWe present a neural encoder-decoder model to convert images into\\npresentational markup based on a scalable coarse-to-fine attention mechanism.\\nOur method is evaluated in the context of image-to-LaTeX generation, and we\\nintroduce a new dataset of real-world rendered mathematical expressions paired\\nwith LaTeX markup. We show that unlike neural OCR techniques using CTC-based\\nmodels, attention-based approaches can tackle this non-standard OCR task. Our\\napproach outperforms classical mathematical OCR systems by a large margin on\\nin-domain rendered data, and, with pretraining, also performs well on\\nout-of-domain handwritten data. To reduce the inference complexity associated\\nwith the attention-based approaches, we introduce a new coarse-to-fine\\nattention layer that selects a support region before applying attention.',\n",
              " 'Teaching Machines to Code: Neural Markup Generation with Visual\\n  Attention\\nWe present a deep recurrent neural network model with soft visual attention\\nthat learns to generate LaTeX markup of real-world math formulas given their\\nimages. Applying neural sequence generation techniques that have been very\\nsuccessful in the fields of machine translation and image/handwriting/speech\\ncaptioning, recognition, transcription and synthesis, we construct an\\nimage-to-markup model that learns to produce syntactically and semantically\\ncorrect LaTeX markup code of over 150 words long and achieves a BLEU score of\\n89%; the best reported so far for the Im2Latex problem. We also visually\\ndemonstrate that the model learns to scan the image left-right / up-down much\\nas a human would read it.',\n",
              " 'Evolution in Groups: A deeper look at synaptic cluster driven evolution\\n  of deep neural networks\\nA promising paradigm for achieving highly efficient deep neural networks is\\nthe idea of evolutionary deep intelligence, which mimics biological evolution\\nprocesses to progressively synthesize more efficient networks. A crucial design\\nfactor in evolutionary deep intelligence is the genetic encoding scheme used to\\nsimulate heredity and determine the architectures of offspring networks. In\\nthis study, we take a deeper look at the notion of synaptic cluster-driven\\nevolution of deep neural networks which guides the evolution process towards\\nthe formation of a highly sparse set of synaptic clusters in offspring\\nnetworks. Utilizing a synaptic cluster-driven genetic encoding, the\\nprobabilistic encoding of synaptic traits considers not only individual\\nsynaptic properties but also inter-synaptic relationships within a deep neural\\nnetwork. This process results in highly sparse offspring networks which are\\nparticularly tailored for parallel computational devices such as GPUs and deep\\nneural network accelerator chips. Comprehensive experimental results using four\\nwell-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and\\nDetectNet) on two different tasks (object categorization and object detection)\\ndemonstrate the efficiency of the proposed method. Cluster-driven genetic\\nencoding scheme synthesizes networks that can achieve state-of-the-art\\nperformance with significantly smaller number of synapses than that of the\\noriginal ancestor network. ($\\\\sim$125-fold decrease in synapses for MNIST).\\nFurthermore, the improved cluster efficiency in the generated offspring\\nnetworks ($\\\\sim$9.71-fold decrease in clusters for MNIST and a $\\\\sim$8.16-fold\\ndecrease in clusters for KITTI) is particularly useful for accelerated\\nperformance on parallel computing hardware architectures such as those in GPUs\\nand deep neural network accelerator chips.',\n",
              " 'Mesh Learning for Classifying Cognitive Processes\\nA relatively recent advance in cognitive neuroscience has been multi-voxel\\npattern analysis (MVPA), which enables researchers to decode brain states\\nand/or the type of information represented in the brain during a cognitive\\noperation. MVPA methods utilize machine learning algorithms to distinguish\\namong types of information or cognitive states represented in the brain, based\\non distributed patterns of neural activity. In the current investigation, we\\npropose a new approach for representation of neural data for pattern analysis,\\nnamely a Mesh Learning Model. In this approach, at each time instant, a star\\nmesh is formed around each voxel, such that the voxel corresponding to the\\ncenter node is surrounded by its p-nearest neighbors. The arc weights of each\\nmesh are estimated from the voxel intensity values by least squares method. The\\nestimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs),\\nare then used to train a classifier, such as Neural Networks, k-Nearest\\nNeighbor, Na\\\\\"ive Bayes and Support Vector Machines. The proposed Mesh Model\\nwas tested on neuroimaging data acquired via functional magnetic resonance\\nimaging (fMRI) during a recognition memory experiment using categorized word\\nlists, employing a previously established experimental paradigm (\\\\\"Oztekin &\\nBadre, 2011). Results suggest that the proposed Mesh Learning approach can\\nprovide an effective algorithm for pattern analysis of brain activity during\\ncognitive processing.',\n",
              " 'Synthesizing Deep Neural Network Architectures using Biological Synaptic\\n  Strength Distributions\\nIn this work, we perform an exploratory study on synthesizing deep neural\\nnetworks using biological synaptic strength distributions, and the potential\\ninfluence of different distributions on modelling performance particularly for\\nthe scenario associated with small data sets. Surprisingly, a CNN with\\nconvolutional layer synaptic strengths drawn from biologically-inspired\\ndistributions such as log-normal or correlated center-surround distributions\\nperformed relatively well suggesting a possibility for designing deep neural\\nnetwork architectures that do not require many data samples to learn, and can\\nsidestep current training procedures while maintaining or boosting modelling\\nperformance.',\n",
              " 'A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters\\n  Optimization\\nAddressing the issue of SVMs parameters optimization, this study proposes an\\nefficient memetic algorithm based on Particle Swarm Optimization algorithm\\n(PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is\\nresponsible for exploration of the search space and the detection of the\\npotential regions with optimum solutions, while pattern search (PS) is used to\\nproduce an effective exploitation on the potential regions obtained by PSO.\\nMoreover, a novel probabilistic selection strategy is proposed to select the\\nappropriate individuals among the current population to undergo local\\nrefinement, keeping a well balance between exploration and exploitation.\\nExperimental results confirm that the local refinement with PS and our proposed\\nselection strategy are effective, and finally demonstrate effectiveness and\\nrobustness of the proposed PSO-PS based MA for SVMs parameters optimization.',\n",
              " 'Density estimation using Real NVP\\nUnsupervised learning of probabilistic models is a central yet challenging\\nproblem in machine learning. Specifically, designing models with tractable\\nlearning, sampling, inference and evaluation is crucial in solving this task.\\nWe extend the space of such models using real-valued non-volume preserving\\n(real NVP) transformations, a set of powerful invertible and learnable\\ntransformations, resulting in an unsupervised learning algorithm with exact\\nlog-likelihood computation, exact sampling, exact inference of latent\\nvariables, and an interpretable latent space. We demonstrate its ability to\\nmodel natural images on four datasets through sampling, log-likelihood\\nevaluation and latent variable manipulations.',\n",
              " 'Evolution Strategies as a Scalable Alternative to Reinforcement Learning\\nWe explore the use of Evolution Strategies (ES), a class of black box\\noptimization algorithms, as an alternative to popular MDP-based RL techniques\\nsuch as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show\\nthat ES is a viable solution strategy that scales extremely well with the\\nnumber of CPUs available: By using a novel communication strategy based on\\ncommon random numbers, our ES implementation only needs to communicate scalars,\\nmaking it possible to scale to over a thousand parallel workers. This allows us\\nto solve 3D humanoid walking in 10 minutes and obtain competitive results on\\nmost Atari games after one hour of training. In addition, we highlight several\\nadvantages of ES as a black box optimization technique: it is invariant to\\naction frequency and delayed rewards, tolerant of extremely long horizons, and\\ndoes not need temporal discounting or value function approximation.',\n",
              " 'QMDP-Net: Deep Learning for Planning under Partial Observability\\nThis paper introduces the QMDP-net, a neural network architecture for\\nplanning under partial observability. The QMDP-net combines the strengths of\\nmodel-free learning and model-based planning. It is a recurrent policy network,\\nbut it represents a policy for a parameterized set of tasks by connecting a\\nmodel with a planning algorithm that solves the model, thus embedding the\\nsolution structure of planning in a network learning architecture. The QMDP-net\\nis fully differentiable and allows for end-to-end training. We train a QMDP-net\\non different tasks so that it can generalize to new ones in the parameterized\\ntask set and \"transfer\" to other similar tasks beyond the set. In preliminary\\nexperiments, QMDP-net showed strong performance on several robotic tasks in\\nsimulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it\\nsometimes outperforms the QMDP algorithm in the experiments, as a result of\\nend-to-end learning.',\n",
              " 'TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep\\n  Reinforcement Learning\\nCombining deep model-free reinforcement learning with on-line planning is a\\npromising approach to building on the successes of deep RL. On-line planning\\nwith look-ahead trees has proven successful in environments where transition\\nmodels are known a priori. However, in complex environments where transition\\nmodels need to be learned from data, the deficiencies of learned models have\\nlimited their utility for planning. To address these challenges, we propose\\nTreeQN, a differentiable, recursive, tree-structured model that serves as a\\ndrop-in replacement for any value function network in deep RL with discrete\\nactions. TreeQN dynamically constructs a tree by recursively applying a\\ntransition model in a learned abstract state space and then aggregating\\npredicted rewards and state-values using a tree backup to estimate Q-values. We\\nalso propose ATreeC, an actor-critic variant that augments TreeQN with a\\nsoftmax layer to form a stochastic policy network. Both approaches are trained\\nend-to-end, such that the learned model is optimised for its actual use in the\\ntree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a\\nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et\\nal. 2017) on multiple Atari games. Furthermore, we present ablation studies\\nthat demonstrate the effect of different auxiliary losses on learning\\ntransition models.',\n",
              " 'Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent\\n  Networks\\nA major drawback of backpropagation through time (BPTT) is the difficulty of\\nlearning long-term dependencies, coming from having to propagate credit\\ninformation backwards through every single step of the forward computation.\\nThis makes BPTT both computationally impractical and biologically implausible.\\nFor this reason, full backpropagation through time is rarely used on long\\nsequences, and truncated backpropagation through time is used as a heuristic.\\nHowever, this usually leads to biased estimates of the gradient in which longer\\nterm dependencies are ignored. Addressing this issue, we propose an alternative\\nalgorithm, Sparse Attentive Backtracking, which might also be related to\\nprinciples used by brains to learn long-term dependencies. Sparse Attentive\\nBacktracking learns an attention mechanism over the hidden states of the past\\nand selectively backpropagates through paths with high attention weights. This\\nallows the model to learn long term dependencies while only backtracking for a\\nsmall number of time steps, not just from the recent past but also from\\nattended relevant past states.',\n",
              " 'Stochastic Deep Learning in Memristive Networks\\nWe study the performance of stochastically trained deep neural networks\\n(DNNs) whose synaptic weights are implemented using emerging memristive devices\\nthat exhibit limited dynamic range, resolution, and variability in their\\nprogramming characteristics. We show that a key device parameter to optimize\\nthe learning efficiency of DNNs is the variability in its programming\\ncharacteristics. DNNs with such memristive synapses, even with dynamic range as\\nlow as $15$ and only $32$ discrete levels, when trained based on stochastic\\nupdates suffer less than $3\\\\%$ loss in accuracy compared to floating point\\nsoftware baseline. We also study the performance of stochastic memristive DNNs\\nwhen used as inference engines with noise corrupted data and find that if the\\ndevice variability can be minimized, the relative degradation in performance\\nfor the Stochastic DNN is better than that of the software baseline. Hence, our\\nstudy presents a new optimization corner for memristive devices for building\\nlarge noise-immune deep learning systems.',\n",
              " 'PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction\\nMulti-step-ahead time series prediction is one of the most challenging\\nresearch topics in the field of time series modeling and prediction, and is\\ncontinually under research. Recently, the multiple-input several\\nmultiple-outputs (MISMO) modeling strategy has been proposed as a promising\\nalternative for multi-step-ahead time series prediction, exhibiting advantages\\ncompared with the two currently dominating strategies, the iterated and the\\ndirect strategies. Built on the established MISMO strategy, this study proposes\\na particle swarm optimization (PSO)-based MISMO modeling strategy, which is\\ncapable of determining the number of sub-models in a self-adaptive mode, with\\nvarying prediction horizons. Rather than deriving crisp divides with equal-size\\ns prediction horizons from the established MISMO, the proposed PSO-MISMO\\nstrategy, implemented with neural networks, employs a heuristic to create\\nflexible divides with varying sizes of prediction horizons and to generate\\ncorresponding sub-models, providing considerable flexibility in model\\nconstruction, which has been validated with simulated and real datasets.',\n",
              " 'Norm-Based Capacity Control in Neural Networks\\nWe investigate the capacity, convexity and characterization of a general\\nfamily of norm-constrained feed-forward networks.',\n",
              " 'Improving the Performance of Neural Networks in Regression Tasks Using\\n  Drawering\\nThe method presented extends a given regression neural network to make its\\nperformance improve. The modification affects the learning procedure only,\\nhence the extension may be easily omitted during evaluation without any change\\nin prediction. It means that the modified model may be evaluated as quickly as\\nthe original one but tends to perform better.\\n  This improvement is possible because the modification gives better expressive\\npower, provides better behaved gradients and works as a regularization. The\\nknowledge gained by the temporarily extended neural network is contained in the\\nparameters shared with the original neural network.\\n  The only cost is an increase in learning time.',\n",
              " 'Learning unbiased features\\nA key element in transfer learning is representation learning; if\\nrepresentations can be developed that expose the relevant factors underlying\\nthe data, then new tasks and domains can be learned readily based on mappings\\nof these salient factors. We propose that an important aim for these\\nrepresentations are to be unbiased. Different forms of representation learning\\ncan be derived from alternative definitions of unwanted bias, e.g., bias to\\nparticular tasks, domains, or irrelevant underlying data dimensions. One very\\nuseful approach to estimating the amount of bias in a representation comes from\\nmaximum mean discrepancy (MMD) [5], a measure of distance between probability\\ndistributions. We are not the first to suggest that MMD can be a useful\\ncriterion in developing representations that apply across multiple domains or\\ntasks [1]. However, in this paper we describe a number of novel applications of\\nthis criterion that we have devised, all based on the idea of developing\\nunbiased representations. These formulations include: a standard domain\\nadaptation framework; a method of learning invariant representations; an\\napproach based on noise-insensitive autoencoders; and a novel form of\\ngenerative model.',\n",
              " \"Compatible Value Gradients for Reinforcement Learning of Continuous Deep\\n  Policies\\nThis paper proposes GProp, a deep reinforcement learning algorithm for\\ncontinuous policies with compatible function approximation. The algorithm is\\nbased on two innovations. Firstly, we present a temporal-difference based\\nmethod for learning the gradient of the value-function. Secondly, we present\\nthe deviator-actor-critic (DAC) model, which comprises three neural networks\\nthat estimate the value function, its gradient, and determine the actor's\\npolicy respectively. We evaluate GProp on two challenging tasks: a contextual\\nbandit problem constructed from nonparametric regression datasets that is\\ndesigned to probe the ability of reinforcement learning algorithms to\\naccurately estimate gradients; and the octopus arm, a challenging reinforcement\\nlearning benchmark. GProp is competitive with fully supervised methods on the\\nbandit task and achieves the best performance to date on the octopus arm.\",\n",
              " 'Learning dynamic Boltzmann machines with spike-timing dependent\\n  plasticity\\nWe propose a particularly structured Boltzmann machine, which we refer to as\\na dynamic Boltzmann machine (DyBM), as a stochastic model of a\\nmulti-dimensional time-series. The DyBM can have infinitely many layers of\\nunits but allows exact and efficient inference and learning when its parameters\\nhave a proposed structure. This proposed structure is motivated by postulates\\nand observations, from biological neural networks, that the synaptic weight is\\nstrengthened or weakened, depending on the timing of spikes (i.e., spike-timing\\ndependent plasticity or STDP). We show that the learning rule of updating the\\nparameters of the DyBM in the direction of maximizing the likelihood of given\\ntime-series can be interpreted as STDP with long term potentiation and long\\nterm depression. The learning rule has a guarantee of convergence and can be\\nperformed in a distributed matter (i.e., local in space) with limited memory\\n(i.e., local in time).',\n",
              " 'Gated Graph Sequence Neural Networks\\nGraph-structured data appears frequently in domains including chemistry,\\nnatural language semantics, social networks, and knowledge bases. In this work,\\nwe study feature learning techniques for graph-structured inputs. Our starting\\npoint is previous work on Graph Neural Networks (Scarselli et al., 2009), which\\nwe modify to use gated recurrent units and modern optimization techniques and\\nthen extend to output sequences. The result is a flexible and broadly useful\\nclass of neural network models that has favorable inductive biases relative to\\npurely sequence-based models (e.g., LSTMs) when the problem is\\ngraph-structured. We demonstrate the capabilities on some simple AI (bAbI) and\\ngraph algorithm learning tasks. We then show it achieves state-of-the-art\\nperformance on a problem from program verification, in which subgraphs need to\\nbe matched to abstract data structures.',\n",
              " \"Deep Reinforcement Learning in Large Discrete Action Spaces\\nBeing able to reason in an environment with a large number of discrete\\nactions is essential to bringing reinforcement learning to a larger class of\\nproblems. Recommender systems, industrial plants and language models are only\\nsome of the many real-world tasks involving large numbers of discrete actions\\nfor which current methods are difficult or even often impossible to apply. An\\nability to generalize over the set of actions as well as sub-linear complexity\\nrelative to the size of the set are both necessary to handle such tasks.\\nCurrent approaches are not able to provide both of these, which motivates the\\nwork in this paper. Our proposed approach leverages prior information about the\\nactions to embed them in a continuous space upon which it can generalize.\\nAdditionally, approximate nearest-neighbor methods allow for logarithmic-time\\nlookup complexity relative to the number of actions, which is necessary for\\ntime-wise tractable training. This combined approach allows reinforcement\\nlearning methods to be applied to large-scale learning problems previously\\nintractable with current methods. We demonstrate our algorithm's abilities on a\\nseries of tasks having up to one million actions.\",\n",
              " \"Value Iteration Networks\\nWe introduce the value iteration network (VIN): a fully differentiable neural\\nnetwork with a `planning module' embedded within. VINs can learn to plan, and\\nare suitable for predicting outcomes that involve planning-based reasoning,\\nsuch as policies for reinforcement learning. Key to our approach is a novel\\ndifferentiable approximation of the value-iteration algorithm, which can be\\nrepresented as a convolutional neural network, and trained end-to-end using\\nstandard backpropagation. We evaluate VIN based policies on discrete and\\ncontinuous path-planning domains, and on a natural-language based search task.\\nWe show that by learning an explicit planning computation, VIN policies\\ngeneralize better to new, unseen domains.\",\n",
              " 'Recurrent Orthogonal Networks and Long-Memory Tasks\\nAlthough RNNs have been shown to be powerful tools for processing sequential\\ndata, finding architectures or optimization strategies that allow them to model\\nvery long term dependencies is still an active area of research. In this work,\\nwe carefully analyze two synthetic datasets originally outlined in (Hochreiter\\nand Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store\\ninformation over many time steps. We explicitly construct RNN solutions to\\nthese problems, and using these constructions, illuminate both the problems\\nthemselves and the way in which RNNs store different types of information in\\ntheir hidden states. These constructions furthermore explain the success of\\nrecent methods that specify unitary initializations or constraints on the\\ntransition matrices.',\n",
              " 'Learning values across many orders of magnitude\\nMost learning algorithms are not invariant to the scale of the function that\\nis being approximated. We propose to adaptively normalize the targets used in\\nlearning. This is useful in value-based reinforcement learning, where the\\nmagnitude of appropriate value approximations can change over time when we\\nupdate the policy of behavior. Our main motivation is prior work on learning to\\nplay Atari games, where the rewards were all clipped to a predetermined range.\\nThis clipping facilitates learning across many different games with a single\\nlearning algorithm, but a clipped reward function can result in qualitatively\\ndifferent behavior. Using the adaptive normalization we can remove this\\ndomain-specific heuristic without diminishing overall performance.',\n",
              " 'Genetic Architect: Discovering Genomic Structure with Learned Neural\\n  Architectures\\nEach human genome is a 3 billion base pair set of encoding instructions.\\nDecoding the genome using deep learning fundamentally differs from most tasks,\\nas we do not know the full structure of the data and therefore cannot design\\narchitectures to suit it. As such, architectures that fit the structure of\\ngenomics should be learned not prescribed. Here, we develop a novel search\\nalgorithm, applicable across domains, that discovers an optimal architecture\\nwhich simultaneously learns general genomic patterns and identifies the most\\nimportant sequence motifs in predicting functional genomic outcomes. The\\narchitectures we find using this algorithm succeed at using only RNA expression\\ndata to predict gene regulatory structure, learn human-interpretable\\nvisualizations of key sequence motifs, and surpass state-of-the-art results on\\nbenchmark genomics challenges.',\n",
              " 'Deep Successor Reinforcement Learning\\nLearning robust value functions given raw observations and rewards is now\\npossible with model-free and model-based deep reinforcement learning\\nalgorithms. There is a third alternative, called Successor Representations\\n(SR), which decomposes the value function into two components -- a reward\\npredictor and a successor map. The successor map represents the expected future\\nstate occupancy from any given state and the reward predictor maps states to\\nscalar rewards. The value function of a state can be computed as the inner\\nproduct between the successor map and the reward weights. In this paper, we\\npresent DSR, which generalizes SR within an end-to-end deep reinforcement\\nlearning framework. DSR has several appealing properties including: increased\\nsensitivity to distal reward changes due to factorization of reward and world\\ndynamics, and the ability to extract bottleneck states (subgoals) given\\nsuccessor maps trained under a random policy. We show the efficacy of our\\napproach on two diverse environments given raw pixel observations -- simple\\ngrid-world domains (MazeBase) and the Doom game engine.',\n",
              " 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning\\nDeep reinforcement learning (deep RL) has been successful in learning\\nsophisticated behaviors automatically; however, the learning process requires a\\nhuge number of trials. In contrast, animals can learn new tasks in just a few\\ntrials, benefiting from their prior knowledge about the world. This paper seeks\\nto bridge this gap. Rather than designing a \"fast\" reinforcement learning\\nalgorithm, we propose to represent it as a recurrent neural network (RNN) and\\nlearn it from data. In our proposed method, RL$^2$, the algorithm is encoded in\\nthe weights of the RNN, which are learned slowly through a general-purpose\\n(\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm\\nwould receive, including observations, actions, rewards, and termination flags;\\nand it retains its state across episodes in a given Markov Decision Process\\n(MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on\\nthe current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both\\nsmall-scale and large-scale problems. On the small-scale side, we train it to\\nsolve randomly generated multi-arm bandit problems and finite MDPs. After\\nRL$^2$ is trained, its performance on new MDPs is close to human-designed\\nalgorithms with optimality guarantees. On the large-scale side, we test RL$^2$\\non a vision-based navigation task and show that it scales up to\\nhigh-dimensional problems.',\n",
              " 'Capacity and Trainability in Recurrent Neural Networks\\nTwo potential bottlenecks on the expressiveness of recurrent neural networks\\n(RNNs) are their ability to store information about the task in their\\nparameters, and to store information about the input history in their units. We\\nshow experimentally that all common RNN architectures achieve nearly the same\\nper-task and per-unit capacity bounds with careful training, for a variety of\\ntasks and stacking depths. They can store an amount of task information which\\nis linear in the number of parameters, and is approximately 5 bits per\\nparameter. They can additionally store approximately one real number from their\\ninput history per hidden unit. We further find that for several tasks it is the\\nper-task parameter capacity bound that determines performance. These results\\nsuggest that many previous results comparing RNN architectures are driven\\nprimarily by differences in training effectiveness, rather than differences in\\ncapacity. Supporting this observation, we compare training difficulty for\\nseveral architectures, and show that vanilla RNNs are far more difficult to\\ntrain, yet have slightly higher capacity. Finally, we propose two novel RNN\\narchitectures, one of which is easier to train than the LSTM or GRU for deeply\\nstacked architectures.',\n",
              " 'Causal Regularization\\nIn application domains such as healthcare, we want accurate predictive models\\nthat are also causally interpretable. In pursuit of such models, we propose a\\ncausal regularizer to steer predictive models towards causally-interpretable\\nsolutions and theoretically study its properties. In a large-scale analysis of\\nElectronic Health Records (EHR), our causally-regularized model outperforms its\\nL1-regularized counterpart in causal accuracy and is competitive in predictive\\nperformance. We perform non-linear causality analysis by causally regularizing\\na special neural network architecture. We also show that the proposed causal\\nregularizer can be used together with neural representation learning algorithms\\nto yield up to 20% improvement over multilayer perceptron in detecting\\nmultivariate causation, a situation common in healthcare, where many causal\\nfactors should occur simultaneously to have an effect on the target variable.',\n",
              " 'On the Behavior of Convolutional Nets for Feature Extraction\\nDeep neural networks are representation learning techniques. During training,\\na deep net is capable of generating a descriptive language of unprecedented\\nsize and detail in machine learning. Extracting the descriptive language coded\\nwithin a trained CNN model (in the case of image data), and reusing it for\\nother purposes is a field of interest, as it provides access to the visual\\ndescriptors previously learnt by the CNN after processing millions of images,\\nwithout requiring an expensive training phase. Contributions to this field\\n(commonly known as feature representation transfer or transfer learning) have\\nbeen purely empirical so far, extracting all CNN features from a single layer\\nclose to the output and testing their performance by feeding them to a\\nclassifier. This approach has provided consistent results, although its\\nrelevance is limited to classification tasks. In a completely different\\napproach, in this paper we statistically measure the discriminative power of\\nevery single feature found within a deep CNN, when used for characterizing\\nevery class of 11 datasets. We seek to provide new insights into the behavior\\nof CNN features, particularly the ones from convolutional layers, as this can\\nbe relevant for their application to knowledge representation and reasoning.\\nOur results confirm that low and middle level features may behave differently\\nto high level features, but only under certain conditions. We find that all CNN\\nfeatures can be used for knowledge representation purposes both by their\\npresence or by their absence, doubling the information a single CNN feature may\\nprovide. We also study how much noise these features may include, and propose a\\nthresholding approach to discard most of it. All these insights have a direct\\napplication to the generation of CNN embedding spaces.',\n",
              " 'Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in\\n  Generative Models\\nAdversarial learning of probabilistic models has recently emerged as a\\npromising alternative to maximum likelihood. Implicit models such as generative\\nadversarial networks (GAN) often generate better samples compared to explicit\\nmodels trained by maximum likelihood. Yet, GANs sidestep the characterization\\nof an explicit density which makes quantitative evaluations challenging. To\\nbridge this gap, we propose Flow-GANs, a generative adversarial network for\\nwhich we can perform exact likelihood evaluation, thus supporting both\\nadversarial and maximum likelihood training. When trained adversarially,\\nFlow-GANs generate high-quality samples but attain extremely poor\\nlog-likelihood scores, inferior even to a mixture model memorizing the training\\ndata; the opposite is true when trained by maximum likelihood. Results on MNIST\\nand CIFAR-10 demonstrate that hybrid training can attain high held-out\\nlikelihoods while retaining visual fidelity in the generated samples.',\n",
              " \"Filtering Variational Objectives\\nWhen used as a surrogate objective for maximum likelihood estimation in\\nlatent variable models, the evidence lower bound (ELBO) produces\\nstate-of-the-art results. Inspired by this, we consider the extension of the\\nELBO to a family of lower bounds defined by a particle filter's estimator of\\nthe marginal likelihood, the filtering variational objectives (FIVOs). FIVOs\\ntake the same arguments as the ELBO, but can exploit a model's sequential\\nstructure to form tighter bounds. We present results that relate the tightness\\nof FIVO's bound to the variance of the particle filter's estimator by\\nconsidering the generic case of bounds defined as log-transformed likelihood\\nestimators. Experimentally, we show that training with FIVO results in\\nsubstantial improvements over training the same model architecture with the\\nELBO on sequential data.\",\n",
              " 'Kernel Implicit Variational Inference\\nRecent progress in variational inference has paid much attention to the\\nflexibility of variational posteriors. One promising direction is to use\\nimplicit distributions, i.e., distributions without tractable densities as the\\nvariational posterior. However, existing methods on implicit posteriors still\\nface challenges of noisy estimation and computational infeasibility when\\napplied to models with high-dimensional latent variables. In this paper, we\\npresent a new approach named Kernel Implicit Variational Inference that\\naddresses these challenges. As far as we know, for the first time implicit\\nvariational inference is successfully applied to Bayesian neural networks,\\nwhich shows promising results on both regression and classification tasks.',\n",
              " 'Non-Markovian Control with Gated End-to-End Memory Policy Networks\\nPartially observable environments present an important open challenge in the\\ndomain of sequential control learning with delayed rewards. Despite numerous\\nattempts during the two last decades, the majority of reinforcement learning\\nalgorithms and associated approximate models, applied to this context, still\\nassume Markovian state transitions. In this paper, we explore the use of a\\nrecently proposed attention-based model, the Gated End-to-End Memory Network,\\nfor sequential control. We call the resulting model the Gated End-to-End Memory\\nPolicy Network. More precisely, we use a model-free value-based algorithm to\\nlearn policies for partially observed domains using this memory-enhanced neural\\nnetwork. This model is end-to-end learnable and it features unbounded memory.\\nIndeed, because of its attention mechanism and associated non-parametric\\nmemory, the proposed model allows us to define an attention mechanism over the\\nobservation stream unlike recurrent models. We show encouraging results that\\nillustrate the capability of our attention-based model in the context of the\\ncontinuous-state non-stationary control problem of stock trading. We also\\npresent an OpenAI Gym environment for simulated stock exchange and explain its\\nrelevance as a benchmark for the field of non-Markovian decision process\\nlearning.',\n",
              " 'Automated Problem Identification: Regression vs Classification via\\n  Evolutionary Deep Networks\\nRegression or classification? This is perhaps the most basic question faced\\nwhen tackling a new supervised learning problem. We present an Evolutionary\\nDeep Learning (EDL) algorithm that automatically solves this by identifying the\\nquestion type with high accuracy, along with a proposed deep architecture.\\nTypically, a significant amount of human insight and preparation is required\\nprior to executing machine learning algorithms. For example, when creating deep\\nneural networks, the number of parameters must be selected in advance and\\nfurthermore, a lot of these choices are made based upon pre-existing knowledge\\nof the data such as the use of a categorical cross entropy loss function.\\nHumans are able to study a dataset and decide whether it represents a\\nclassification or a regression problem, and consequently make decisions which\\nwill be applied to the execution of the neural network. We propose the\\nAutomated Problem Identification (API) algorithm, which uses an evolutionary\\nalgorithm interface to TensorFlow to manipulate a deep neural network to decide\\nif a dataset represents a classification or a regression problem. We test API\\non 16 different classification, regression and sentiment analysis datasets with\\nup to 10,000 features and up to 17,000 unique target values. API achieves an\\naverage accuracy of $96.3\\\\%$ in identifying the problem type without hardcoding\\nany insights about the general characteristics of regression or classification\\nproblems. For example, API successfully identifies classification problems even\\nwith 1000 target values. Furthermore, the algorithm recommends which loss\\nfunction to use and also recommends a neural network architecture. Our work is\\ntherefore a step towards fully automated machine learning.',\n",
              " 'A Simple Neural Attentive Meta-Learner\\nDeep neural networks excel in regimes with large amounts of data, but tend to\\nstruggle when data is scarce or when they need to adapt quickly to changes in\\nthe task. In response, recent work in meta-learning proposes training a\\nmeta-learner on a distribution of similar tasks, in the hopes of generalization\\nto novel but related tasks by learning a high-level strategy that captures the\\nessence of the problem it is asked to solve. However, many recent meta-learning\\napproaches are extensively hand-designed, either using architectures\\nspecialized to a particular application, or hard-coding algorithmic components\\nthat constrain how the meta-learner solves the task. We propose a class of\\nsimple and generic meta-learner architectures that use a novel combination of\\ntemporal convolutions and soft attention; the former to aggregate information\\nfrom past experience and the latter to pinpoint specific pieces of information.\\nIn the most extensive set of meta-learning experiments to date, we evaluate the\\nresulting Simple Neural AttentIve Learner (or SNAIL) on several\\nheavily-benchmarked tasks. On all tasks, in both supervised and reinforcement\\nlearning, SNAIL attains state-of-the-art performance by significant margins.',\n",
              " 'Kafnets: kernel-based non-parametric activation functions for neural\\n  networks\\nNeural networks are generally built by interleaving (adaptable) linear layers\\nwith (fixed) nonlinear activation functions. To increase their flexibility,\\nseveral authors have proposed methods for adapting the activation functions\\nthemselves, endowing them with varying degrees of flexibility. None of these\\napproaches, however, have gained wide acceptance in practice, and research in\\nthis topic remains open. In this paper, we introduce a novel family of flexible\\nactivation functions that are based on an inexpensive kernel expansion at every\\nneuron. Leveraging over several properties of kernel-based models, we propose\\nmultiple variations for designing and initializing these kernel activation\\nfunctions (KAFs), including a multidimensional scheme allowing to nonlinearly\\ncombine information from different paths in the network. The resulting KAFs can\\napproximate any mapping defined over a subset of the real line, either convex\\nor nonconvex. Furthermore, they are smooth over their entire domain, linear in\\ntheir parameters, and they can be regularized using any known scheme, including\\nthe use of $\\\\ell_1$ penalties to enforce sparseness. To the best of our\\nknowledge, no other known model satisfies all these properties simultaneously.\\nIn addition, we provide a relatively complete overview on alternative\\ntechniques for adapting the activation functions, which is currently lacking in\\nthe literature. A large set of experiments validates our proposal.',\n",
              " 'Learning model-based planning from scratch\\nConventional wisdom holds that model-based planning is a powerful approach to\\nsequential decision-making. It is often very challenging in practice, however,\\nbecause while a model can be used to evaluate a plan, it does not prescribe how\\nto construct a plan. Here we introduce the \"Imagination-based Planner\", the\\nfirst model-based, sequential decision-making agent that can learn to\\nconstruct, evaluate, and execute plans. Before any action, it can perform a\\nvariable number of imagination steps, which involve proposing an imagined\\naction and evaluating it with its model-based imagination. All imagined actions\\nand outcomes are aggregated, iteratively, into a \"plan context\" which\\nconditions future real and imagined actions. The agent can even decide how to\\nimagine: testing out alternative imagined actions, chaining sequences of\\nactions together, or building a more complex \"imagination tree\" by navigating\\nflexibly among the previously imagined states using a learned policy. And our\\nagent can learn to plan economically, jointly optimizing for external rewards\\nand computational costs associated with using its imagination. We show that our\\narchitecture can learn to solve a challenging continuous control problem, and\\nalso learn elaborate planning strategies in a discrete maze-solving task. Our\\nwork opens a new direction toward learning the components of a model-based\\nplanning system and how to use them.',\n",
              " 'Recurrent Ladder Networks\\nWe propose a recurrent extension of the Ladder networks whose structure is\\nmotivated by the inference required in hierarchical latent variable models. We\\ndemonstrate that the recurrent Ladder is able to handle a wide variety of\\ncomplex learning tasks that benefit from iterative inference and temporal\\nmodeling. The architecture shows close-to-optimal results on temporal modeling\\nof video data, competitive results on music modeling, and improved perceptual\\ngrouping based on higher order abstractions, such as stochastic textures and\\nmotion cues. We present results for fully supervised, semi-supervised, and\\nunsupervised tasks. The results suggest that the proposed architecture and\\nprinciples are powerful tools for learning a hierarchy of abstractions,\\nlearning iterative inference and handling temporal information.',\n",
              " 'Generalization in Deep Learning\\nWith a direct analysis of neural networks, this paper presents a\\nmathematically tight generalization theory to partially address an open problem\\nregarding the generalization of deep learning. Unlike previous bound-based\\ntheory, our main theory is quantitatively as tight as possible for every\\ndataset individually, while producing qualitative insights competitively. Our\\nresults give insight into why and how deep learning can generalize well,\\ndespite its large capacity, complexity, possible algorithmic instability,\\nnonrobustness, and sharp minima, answering to an open question in the\\nliterature. We also discuss limitations of our results and propose additional\\nopen problems.',\n",
              " 'Parametrizing filters of a CNN with a GAN\\nIt is commonly agreed that the use of relevant invariances as a good\\nstatistical bias is important in machine-learning. However, most approaches\\nthat explicitly incorporate invariances into a model architecture only make use\\nof very simple transformations, such as translations and rotations. Hence,\\nthere is a need for methods to model and extract richer transformations that\\ncapture much higher-level invariances. To that end, we introduce a tool\\nallowing to parametrize the set of filters of a trained convolutional neural\\nnetwork with the latent space of a generative adversarial network. We then show\\nthat the method can capture highly non-linear invariances of the data by\\nvisualizing their effect in the data space.',\n",
              " 'Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence\\n  Learning\\nLong Short-Term Memory (LSTM) is a popular approach to boosting the ability\\nof Recurrent Neural Networks to store longer term temporal information. The\\ncapacity of an LSTM network can be increased by widening and adding layers.\\nHowever, usually the former introduces additional parameters, while the latter\\nincreases the runtime. As an alternative we propose the Tensorized LSTM in\\nwhich the hidden states are represented by tensors and updated via a\\ncross-layer convolution. By increasing the tensor size, the network can be\\nwidened efficiently without additional parameters since the parameters are\\nshared across different locations in the tensor; by delaying the output, the\\nnetwork can be deepened implicitly with little additional runtime since deep\\ncomputations for each timestep are merged into temporal computations of the\\nsequence. Experiments conducted on five challenging sequence learning tasks\\nshow the potential of the proposed model.',\n",
              " 'Learning and Real-time Classification of Hand-written Digits With\\n  Spiking Neural Networks\\nWe describe a novel spiking neural network (SNN) for automated, real-time\\nhandwritten digit classification and its implementation on a GP-GPU platform.\\nInformation processing within the network, from feature extraction to\\nclassification is implemented by mimicking the basic aspects of neuronal spike\\ninitiation and propagation in the brain. The feature extraction layer of the\\nSNN uses fixed synaptic weight maps to extract the key features of the image\\nand the classifier layer uses the recently developed NormAD approximate\\ngradient descent based supervised learning algorithm for spiking neural\\nnetworks to adjust the synaptic weights. On the standard MNIST database images\\nof handwritten digits, our network achieves an accuracy of 99.80% on the\\ntraining set and 98.06% on the test set, with nearly 7x fewer parameters\\ncompared to the state-of-the-art spiking networks. We further use this network\\nin a GPU based user-interface system demonstrating real-time SNN simulation to\\ninfer digits written by different users. On a test set of 500 such images, this\\nreal-time platform achieves an accuracy exceeding 97% while making a prediction\\nwithin an SNN emulation time of less than 100ms.',\n",
              " \"Overcoming catastrophic forgetting with hard attention to the task\\nCatastrophic forgetting occurs when a neural network loses the information\\nlearned in a previous task after training on subsequent tasks. This problem\\nremains a hurdle for artificial intelligence systems with sequential learning\\ncapabilities. In this paper, we propose a task-based hard attention mechanism\\nthat preserves previous tasks' information without affecting the current task's\\nlearning. A hard attention mask is learned concurrently to every task, through\\nstochastic gradient descent, and previous masks are exploited to condition such\\nlearning. We show that the proposed mechanism is effective for reducing\\ncatastrophic forgetting, cutting current rates by 45 to 80%. We also show that\\nit is robust to different hyperparameter choices, and that it offers a number\\nof monitoring capabilities. The approach features the possibility to control\\nboth the stability and compactness of the learned knowledge, which we believe\\nmakes it also attractive for online learning or network compression\\napplications.\",\n",
              " \"Detecting and Correcting for Label Shift with Black Box Predictors\\nFaced with distribution shift between training and test set, we wish to\\ndetect and quantify the shift, and to correct our classifiers without test set\\nlabels. Motivated by medical diagnosis, where diseases (targets), cause\\nsymptoms (observations), we focus on label shift, where the label marginal\\n$p(y)$ changes but the conditional $p(x|y)$ does not. We propose Black Box\\nShift Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits\\narbitrary black box predictors to reduce dimensionality prior to shift\\ncorrection. While better predictors give tighter estimates, BBSE works even\\nwhen predictors are biased, inaccurate, or uncalibrated, so long as their\\nconfusion matrices are invertible. We prove BBSE's consistency, bound its\\nerror, and introduce a statistical test that uses BBSE to detect shift. We also\\nleverage BBSE to correct classifiers. Experiments demonstrate accurate\\nestimates and improved prediction, even on high-dimensional datasets of natural\\nimages.\",\n",
              " 'Generalization in Machine Learning via Analytical Learning Theory\\nThis paper introduces a novel measure-theoretic learning theory to analyze\\ngeneralization behaviors of practical interest. The proposed learning theory\\nhas the following abilities: 1) to utilize the qualities of each learned\\nrepresentation on the path from raw inputs to outputs in representation\\nlearning, 2) to guarantee good generalization errors possibly with arbitrarily\\nrich hypothesis spaces (e.g., arbitrarily large capacity and Rademacher\\ncomplexity) and non-stable/non-robust learning algorithms, and 3) to clearly\\ndistinguish each individual problem instance from each other. Our\\ngeneralization bounds are relative to a representation of the data, and hold\\ntrue even if the representation is learned. We discuss several consequences of\\nour results on deep learning, one-shot learning and curriculum learning. Unlike\\nstatistical learning theory, the proposed learning theory analyzes each problem\\ninstance individually via measure theory, rather than a set of problem\\ninstances via statistics. Because of the differences in the assumptions and the\\nobjectives, the proposed learning theory is meant to be complementary to\\nprevious learning theory and is not designed to compete with it.',\n",
              " 'Sensitivity and Generalization in Neural Networks: an Empirical Study\\nIn practice it is often found that large over-parameterized neural networks\\ngeneralize better than their smaller counterparts, an observation that appears\\nto conflict with classical notions of function complexity, which typically\\nfavor smaller models. In this work, we investigate this tension between\\ncomplexity and generalization through an extensive empirical exploration of two\\nnatural metrics of complexity related to sensitivity to input perturbations.\\nOur experiments survey thousands of models with various fully-connected\\narchitectures, optimizers, and other hyper-parameters, as well as four\\ndifferent image classification datasets.\\n  We find that trained neural networks are more robust to input perturbations\\nin the vicinity of the training data manifold, as measured by the norm of the\\ninput-output Jacobian of the network, and that it correlates well with\\ngeneralization. We further establish that factors associated with poor\\ngeneralization $-$ such as full-batch training or using random labels $-$\\ncorrespond to lower robustness, while factors associated with good\\ngeneralization $-$ such as data augmentation and ReLU non-linearities $-$ give\\nrise to more robust functions. Finally, we demonstrate how the input-output\\nJacobian norm can be predictive of generalization at the level of individual\\ntest points.',\n",
              " \"On the importance of single directions for generalization\\nDespite their ability to memorize large datasets, deep neural networks often\\nachieve good generalization performance. However, the differences between the\\nlearned solutions of networks which generalize and those which do not remain\\nunclear. Additionally, the tuning properties of single directions (defined as\\nthe activation of a single unit or some linear combination of units in response\\nto some input) have been highlighted, but their importance has not been\\nevaluated. Here, we connect these lines of inquiry to demonstrate that a\\nnetwork's reliance on single directions is a good predictor of its\\ngeneralization performance, across networks trained on datasets with different\\nfractions of corrupted labels, across ensembles of networks trained on datasets\\nwith unmodified labels, across different hyperparameters, and over the course\\nof training. While dropout only regularizes this quantity up to a point, batch\\nnormalization implicitly discourages single direction reliance, in part by\\ndecreasing the class selectivity of individual units. Finally, we find that\\nclass selectivity is a poor predictor of task importance, suggesting not only\\nthat networks which generalize well minimize their dependence on individual\\nunits by reducing their selectivity, but also that individually selective units\\nmay not be necessary for strong network performance.\",\n",
              " 'Maximin affinity learning of image segmentation\\nImages can be segmented by first using a classifier to predict an affinity\\ngraph that reflects the degree to which image pixels must be grouped together\\nand then partitioning the graph to yield a segmentation. Machine learning has\\nbeen applied to the affinity classifier to produce affinity graphs that are\\ngood in the sense of minimizing edge misclassification rates. However, this\\nerror measure is only indirectly related to the quality of segmentations\\nproduced by ultimately partitioning the affinity graph. We present the first\\nmachine learning algorithm for training a classifier to produce affinity graphs\\nthat are good in the sense of producing segmentations that directly minimize\\nthe Rand index, a well known segmentation performance measure. The Rand index\\nmeasures segmentation performance by quantifying the classification of the\\nconnectivity of image pixel pairs after segmentation. By using the simple graph\\npartitioning algorithm of finding the connected components of the thresholded\\naffinity graph, we are able to train an affinity classifier to directly\\nminimize the Rand index of segmentations resulting from the graph partitioning.\\nOur learning algorithm corresponds to the learning of maximin affinities\\nbetween image pixel pairs, which are predictive of the pixel-pair connectivity.',\n",
              " 'A General Framework for Development of the Cortex-like Visual Object\\n  Recognition System: Waves of Spikes, Predictive Coding and Universal\\n  Dictionary of Features\\nThis study is focused on the development of the cortex-like visual object\\nrecognition system. We propose a general framework, which consists of three\\nhierarchical levels (modules). These modules functionally correspond to the V1,\\nV4 and IT areas. Both bottom-up and top-down connections between the\\nhierarchical levels V4 and IT are employed. The higher the degree of matching\\nbetween the input and the preferred stimulus, the shorter the response time of\\nthe neuron. Therefore information about a single stimulus is distributed in\\ntime and is transmitted by the waves of spikes. The reciprocal connections and\\nwaves of spikes implement predictive coding: an initial hypothesis is generated\\non the basis of information delivered by the first wave of spikes and is tested\\nwith the information carried by the consecutive waves. The development is\\nconsidered as extraction and accumulation of features in V4 and objects in IT.\\nOnce stored a feature can be disposed, if rarely activated. This cause update\\nof feature repository. Consequently, objects in IT are also updated. This\\nillustrates the growing process and dynamical change of topological structures\\nof V4, IT and connections between these areas.',\n",
              " 'Handwritten Digit Recognition with a Committee of Deep Neural Nets on\\n  GPUs\\nThe competitive MNIST handwritten digit recognition benchmark has a long\\nhistory of broken records since 1998. The most recent substantial improvement\\nby others dates back 7 years (error rate 0.4%) . Recently we were able to\\nsignificantly improve this result, using graphics cards to greatly speed up\\ntraining of simple but deep MLPs, which achieved 0.35%, outperforming all the\\nprevious more complex methods. Here we report another substantial improvement:\\n0.31% obtained using a committee of MLPs.',\n",
              " 'Eclectic Extraction of Propositional Rules from Neural Networks\\nArtificial Neural Network is among the most popular algorithm for supervised\\nlearning. However, Neural Networks have a well-known drawback of being a \"Black\\nBox\" learner that is not comprehensible to the Users. This lack of transparency\\nmakes it unsuitable for many high risk tasks such as medical diagnosis that\\nrequires a rational justification for making a decision. Rule Extraction\\nmethods attempt to curb this limitation by extracting comprehensible rules from\\na trained Network. Many such extraction algorithms have been developed over the\\nyears with their respective strengths and weaknesses. They have been broadly\\ncategorized into three types based on their approach to use internal model of\\nthe Network. Eclectic Methods are hybrid algorithms that combine the other\\napproaches to attain more performance. In this paper, we present an Eclectic\\nmethod called HERETIC. Our algorithm uses Inductive Decision Tree learning\\ncombined with information of the neural network structure for extracting\\nlogical rules. Experiments and theoretical analysis show HERETIC to be better\\nin terms of speed and performance.',\n",
              " 'Message Passing Multi-Agent GANs\\nCommunicating and sharing intelligence among agents is an important facet of\\nachieving Artificial General Intelligence. As a first step towards this\\nchallenge, we introduce a novel framework for image generation: Message Passing\\nMulti-Agent Generative Adversarial Networks (MPM GANs). While GANs have\\nrecently been shown to be very effective for image generation and other tasks,\\nthese networks have been limited to mostly single generator-discriminator\\nnetworks. We show that we can obtain multi-agent GANs that communicate through\\nmessage passing to achieve better image generation. The objectives of the\\nindividual agents in this framework are two fold: a co-operation objective and\\na competing objective. The co-operation objective ensures that the message\\nsharing mechanism guides the other generator to generate better than itself\\nwhile the competing objective encourages each generator to generate better than\\nits counterpart. We analyze and visualize the messages that these GANs share\\namong themselves in various scenarios. We quantitatively show that the message\\nsharing formulation serves as a regularizer for the adversarial training.\\nQualitatively, we show that the different generators capture different traits\\nof the underlying data distribution.',\n",
              " 'Mode Regularized Generative Adversarial Networks\\nAlthough Generative Adversarial Networks achieve state-of-the-art results on\\na variety of generative tasks, they are regarded as highly unstable and prone\\nto miss modes. We argue that these bad behaviors of GANs are due to the very\\nparticular functional shape of the trained discriminators in high dimensional\\nspaces, which can easily make training stuck or push probability mass in the\\nwrong direction, towards that of higher concentration than that of the data\\ngenerating distribution. We introduce several ways of regularizing the\\nobjective, which can dramatically stabilize the training of GAN models. We also\\nshow that our regularizers can help the fair distribution of probability mass\\nacross the modes of the data generating distribution, during the early phases\\nof training and thus providing a unified solution to the missing modes problem.',\n",
              " 'Layer-Specific Adaptive Learning Rates for Deep Networks\\nThe increasing complexity of deep learning architectures is resulting in\\ntraining time requiring weeks or even months. This slow training is due in part\\nto vanishing gradients, in which the gradients used by back-propagation are\\nextremely large for weights connecting deep layers (layers near the output\\nlayer), and extremely small for shallow layers (near the input layer); this\\nresults in slow learning in the shallow layers. Additionally, it has also been\\nshown that in highly non-convex problems, such as deep neural networks, there\\nis a proliferation of high-error low curvature saddle points, which slows down\\nlearning dramatically. In this paper, we attempt to overcome the two above\\nproblems by proposing an optimization method for training deep neural networks\\nwhich uses learning rates which are both specific to each layer in the network\\nand adaptive to the curvature of the function, increasing the learning rate at\\nlow curvature points. This enables us to speed up learning in the shallow\\nlayers of the network and quickly escape high-error low curvature saddle\\npoints. We test our method on standard image classification datasets such as\\nMNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy\\nas well as reduces the required training time over standard algorithms.',\n",
              " 'Return of Frustratingly Easy Domain Adaptation\\nUnlike human learning, machine learning often fails to handle changes between\\ntraining (source) and test (target) input distributions. Such domain shifts,\\ncommon in practical scenarios, severely damage the performance of conventional\\nmachine learning methods. Supervised domain adaptation methods have been\\nproposed for the case when the target data have labels, including some that\\nperform very well despite being \"frustratingly easy\" to implement. However, in\\npractice, the target domain is often unlabeled, requiring unsupervised\\nadaptation. We propose a simple, effective, and efficient method for\\nunsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL\\nminimizes domain shift by aligning the second-order statistics of source and\\ntarget distributions, without requiring any target labels. Even though it is\\nextraordinarily simple--it can be implemented in four lines of Matlab\\ncode--CORAL performs remarkably well in extensive evaluations on standard\\nbenchmark datasets.',\n",
              " 'Origami: A 803 GOp/s/W Convolutional Network Accelerator\\nAn ever increasing number of computer vision and image/video processing\\nchallenges are being approached using deep convolutional neural networks,\\nobtaining state-of-the-art results in object recognition and detection,\\nsemantic segmentation, action recognition, optical flow and superresolution.\\nHardware acceleration of these algorithms is essential to adopt these\\nimprovements in embedded and mobile computer vision systems. We present a new\\narchitecture, design and implementation as well as the first reported silicon\\nmeasurements of such an accelerator, outperforming previous work in terms of\\npower-, area- and I/O-efficiency. The manufactured device provides up to 196\\nGOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power\\nefficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it\\nthe first architecture scalable to TOp/s performance.',\n",
              " 'Option Discovery in Hierarchical Reinforcement Learning using\\n  Spatio-Temporal Clustering\\nThis paper introduces an automated skill acquisition framework in\\nreinforcement learning which involves identifying a hierarchical description of\\nthe given task in terms of abstract states and extended actions between\\nabstract states. Identifying such structures present in the task provides ways\\nto simplify and speed up reinforcement learning algorithms. These structures\\nalso help to generalize such algorithms over multiple tasks without relearning\\npolicies from scratch. We use ideas from dynamical systems to find metastable\\nregions in the state space and associate them with abstract states. The\\nspectral clustering algorithm PCCA+ is used to identify suitable abstractions\\naligned to the underlying structure. Skills are defined in terms of the\\nsequence of actions that lead to transitions between such abstract states. The\\nconnectivity information from PCCA+ is used to generate these skills or\\noptions. These skills are independent of the learning task and can be\\nefficiently reused across a variety of tasks defined over the same model. This\\napproach works well even without the exact model of the environment by using\\nsample trajectories to construct an approximate estimate. We also present our\\napproach to scaling the skill acquisition framework to complex tasks with large\\nstate spaces for which we perform state aggregation using the representation\\nlearned from an action conditional video prediction network and use the skill\\nacquisition framework on the aggregated state space.',\n",
              " 'Residual Networks Behave Like Ensembles of Relatively Shallow Networks\\nIn this work we propose a novel interpretation of residual networks showing\\nthat they can be seen as a collection of many paths of differing length.\\nMoreover, residual networks seem to enable very deep networks by leveraging\\nonly the short paths during training. To support this observation, we rewrite\\nresidual networks as an explicit collection of paths. Unlike traditional\\nmodels, paths through residual networks vary in length. Further, a lesion study\\nreveals that these paths show ensemble-like behavior in the sense that they do\\nnot strongly depend on each other. Finally, and most surprising, most paths are\\nshorter than one might expect, and only the short paths are needed during\\ntraining, as longer paths do not contribute any gradient. For example, most of\\nthe gradient in a residual network with 110 layers comes from paths that are\\nonly 10-34 layers deep. Our results reveal one of the key characteristics that\\nseem to enable the training of very deep networks: Residual networks avoid the\\nvanishing gradient problem by introducing short paths which can carry gradient\\nthroughout the extent of very deep networks.',\n",
              " 'Synthesizing the preferred inputs for neurons in neural networks via\\n  deep generator networks\\nDeep neural networks (DNNs) have demonstrated state-of-the-art results on\\nmany pattern recognition tasks, especially vision classification problems.\\nUnderstanding the inner workings of such computational brains is both\\nfascinating basic science that is interesting in its own right - similar to why\\nwe study the human brain - and will enable researchers to further improve DNNs.\\nOne path to understanding how a neural network functions internally is to study\\nwhat each of its neurons has learned to detect. One such method is called\\nactivation maximization (AM), which synthesizes an input (e.g. an image) that\\nhighly activates a neuron. Here we dramatically improve the qualitative state\\nof the art of activation maximization by harnessing a powerful, learned prior:\\na deep generator network (DGN). The algorithm (1) generates qualitatively\\nstate-of-the-art synthetic images that look almost real, (2) reveals the\\nfeatures learned by each neuron in an interpretable way, (3) generalizes well\\nto new datasets and somewhat well to different network architectures without\\nrequiring the prior to be relearned, and (4) can be considered as a\\nhigh-quality generative method (in this case, by generating novel, creative,\\ninteresting, recognizable images).',\n",
              " 'Structured Convolution Matrices for Energy-efficient Deep learning\\nWe derive a relationship between network representation in energy-efficient\\nneuromorphic architectures and block Toplitz convolutional matrices. Inspired\\nby this connection, we develop deep convolutional networks using a family of\\nstructured convolutional matrices and achieve state-of-the-art trade-off\\nbetween energy efficiency and classification accuracy for well-known image\\nrecognition tasks. We also put forward a novel method to train binary\\nconvolutional networks by utilising an existing connection between\\nnoisy-rectified linear units and binary activations.',\n",
              " 'Deep CORAL: Correlation Alignment for Deep Domain Adaptation\\nDeep neural networks are able to learn powerful representations from large\\nquantities of labeled input data, however they cannot always generalize well\\nacross changes in input distributions. Domain adaptation algorithms have been\\nproposed to compensate for the degradation in performance due to domain shift.\\nIn this paper, we address the case when the target domain is unlabeled,\\nrequiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised\\ndomain adaptation method that aligns the second-order statistics of the source\\nand target distributions with a linear transformation. Here, we extend CORAL to\\nlearn a nonlinear transformation that aligns correlations of layer activations\\nin deep neural networks (Deep CORAL). Experiments on standard benchmark\\ndatasets show state-of-the-art performance.',\n",
              " 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition\\n3D action recognition - analysis of human actions based on 3D skeleton data -\\nbecomes popular recently due to its succinctness, robustness, and\\nview-invariant representation. Recent attempts on this problem suggested to\\ndevelop RNN-based learning methods to model the contextual dependency in the\\ntemporal domain. In this paper, we extend this idea to spatio-temporal domains\\nto analyze the hidden sources of action-related information within the input\\ndata over both domains concurrently. Inspired by the graphical structure of the\\nhuman skeleton, we further propose a more powerful tree-structure based\\ntraversal method. To handle the noise and occlusion in 3D skeleton data, we\\nintroduce new gating mechanism within LSTM to learn the reliability of the\\nsequential input data and accordingly adjust its effect on updating the\\nlong-term context information stored in the memory cell. Our method achieves\\nstate-of-the-art performance on 4 challenging benchmark datasets for 3D human\\naction analysis.',\n",
              " 'Generalized Dropout\\nDeep Neural Networks often require good regularizers to generalize well.\\nDropout is one such regularizer that is widely used among Deep Learning\\npractitioners. Recent work has shown that Dropout can also be viewed as\\nperforming Approximate Bayesian Inference over the network parameters. In this\\nwork, we generalize this notion and introduce a rich family of regularizers\\nwhich we call Generalized Dropout. One set of methods in this family, called\\nDropout++, is a version of Dropout with trainable parameters. Classical Dropout\\nemerges as a special case of this method. Another member of this family selects\\nthe width of neural network layers. Experiments show that these methods help in\\nimproving generalization performance over Dropout.',\n",
              " 'Parsimonious Inference on Convolutional Neural Networks: Learning and\\n  applying on-line kernel activation rules\\nA new, radical CNN design approach is presented in this paper, considering\\nthe reduction of the total computational load during inference. This is\\nachieved by a new holistic intervention on both the CNN architecture and the\\ntraining procedure, which targets to the parsimonious inference by learning to\\nexploit or remove the redundant capacity of a CNN architecture. This is\\naccomplished, by the introduction of a new structural element that can be\\ninserted as an add-on to any contemporary CNN architecture, whilst preserving\\nor even improving its recognition accuracy. Our approach formulates a\\nsystematic and data-driven method for developing CNNs that are trained to\\neventually change size and form in real-time during inference, targeting to the\\nsmaller possible computational footprint. Results are provided for the optimal\\nimplementation on a few modern, high-end mobile computing platforms indicating\\na significant speed-up of up to x3 times.',\n",
              " 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\\nWe propose an algorithm for meta-learning that is model-agnostic, in the\\nsense that it is compatible with any model trained with gradient descent and\\napplicable to a variety of different learning problems, including\\nclassification, regression, and reinforcement learning. The goal of\\nmeta-learning is to train a model on a variety of learning tasks, such that it\\ncan solve new learning tasks using only a small number of training samples. In\\nour approach, the parameters of the model are explicitly trained such that a\\nsmall number of gradient steps with a small amount of training data from a new\\ntask will produce good generalization performance on that task. In effect, our\\nmethod trains the model to be easy to fine-tune. We demonstrate that this\\napproach leads to state-of-the-art performance on two few-shot image\\nclassification benchmarks, produces good results on few-shot regression, and\\naccelerates fine-tuning for policy gradient reinforcement learning with neural\\nnetwork policies.',\n",
              " 'WRPN: Training and Inference using Wide Reduced-Precision Networks\\nFor computer vision applications, prior works have shown the efficacy of\\nreducing the numeric precision of model parameters (network weights) in deep\\nneural networks but also that reducing the precision of activations hurts model\\naccuracy much more than reducing the precision of model parameters. We study\\nschemes to train networks from scratch using reduced-precision activations\\nwithout hurting the model accuracy. We reduce the precision of activation maps\\n(along with model parameters) using a novel quantization scheme and increase\\nthe number of filter maps in a layer, and find that this scheme compensates or\\nsurpasses the accuracy of the baseline full-precision network. As a result, one\\ncan significantly reduce the dynamic memory footprint, memory bandwidth,\\ncomputational energy and speed up the training and inference process with\\nappropriate hardware support. We call our scheme WRPN - wide reduced-precision\\nnetworks. We report results using our proposed schemes and show that our\\nresults are better than previously reported accuracies on ILSVRC-12 dataset\\nwhile being computationally less expensive compared to previously reported\\nreduced-precision networks.',\n",
              " 'Deep Learning is Robust to Massive Label Noise\\nDeep neural networks trained on large supervised datasets have led to\\nimpressive results in image classification and other tasks. However,\\nwell-annotated datasets can be time-consuming and expensive to collect, lending\\nincreased interest to larger but noisy datasets that are more easily obtained.\\nIn this paper, we show that deep neural networks are capable of generalizing\\nfrom training data for which true labels are massively outnumbered by incorrect\\nlabels. We demonstrate remarkably high test performance after training on\\ncorrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain\\ntest accuracy above 90 percent even after each clean training example has been\\ndiluted with 100 randomly-labeled examples. Such behavior holds across multiple\\npatterns of label noise, even when erroneous labels are biased towards\\nconfusing classes. We show that training in this regime requires a significant\\nbut manageable increase in dataset size that is related to the factor by which\\ncorrect labels have been diluted. Finally, we provide an analysis of our\\nresults that shows how increasing noise decreases the effective batch size.',\n",
              " 'Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object\\n  Rotation\\nContent-invariance in mapping codes learned by GAEs is a useful feature for\\nvarious relation learning tasks. In this paper we show that the\\ncontent-invariance of mapping codes for images of 2D and 3D rotated objects can\\nbe substantially improved by extending the standard GAE loss (symmetric\\nreconstruction error) with a regularization term that penalizes the symmetric\\ncross-reconstruction error. This error term involves reconstruction of pairs\\nwith mapping codes obtained from other pairs exhibiting similar\\ntransformations. Although this would principally require knowledge of the\\ntransformations exhibited by training pairs, our experiments show that a\\nbootstrapping approach can sidestep this issue, and that the regularization\\nterm can effectively be used in an unsupervised setting.',\n",
              " 'Deep Learning for Sensor-based Activity Recognition: A Survey\\nSensor-based activity recognition seeks the profound high-level knowledge\\nabout human activities from multitudes of low-level sensor readings.\\nConventional pattern recognition approaches have made tremendous progress in\\nthe past years. However, those methods often heavily rely on heuristic\\nhand-crafted feature extraction, which could hinder their generalization\\nperformance. Additionally, existing methods are undermined for unsupervised and\\nincremental learning tasks. Recently, the recent advancement of deep learning\\nmakes it possible to perform automatic high-level feature extraction thus\\nachieves promising performance in many areas. Since then, deep learning based\\nmethods have been widely adopted for the sensor-based activity recognition\\ntasks. This paper surveys the recent advance of deep learning based\\nsensor-based activity recognition. We summarize existing literature from three\\naspects: sensor modality, deep model, and application. We also present detailed\\ninsights on existing work and propose grand challenges for future research.',\n",
              " 'On the Importance of Consistency in Training Deep Neural Networks\\nWe explain that the difficulties of training deep neural networks come from a\\nsyndrome of three consistency issues. This paper describes our efforts in their\\nanalysis and treatment. The first issue is the training speed inconsistency in\\ndifferent layers. We propose to address it with an intuitive,\\nsimple-to-implement, low footprint second-order method. The second issue is the\\nscale inconsistency between the layer inputs and the layer residuals. We\\nexplain how second-order information provides favorable convenience in removing\\nthis roadblock. The third and most challenging issue is the inconsistency in\\nresidual propagation. Based on the fundamental theorem of linear algebra, we\\nprovide a mathematical characterization of the famous vanishing gradient\\nproblem. Thus, an important design principle for future optimization and neural\\nnetwork design is derived. We conclude this paper with the construction of a\\nnovel contractive neural network.',\n",
              " 'UI-Net: Interactive Artificial Neural Networks for Iterative Image\\n  Segmentation Based on a User Model\\nFor complex segmentation tasks, fully automatic systems are inherently\\nlimited in their achievable accuracy for extracting relevant objects.\\nEspecially in cases where only few data sets need to be processed for a highly\\naccurate result, semi-automatic segmentation techniques exhibit a clear benefit\\nfor the user. One area of application is medical image processing during an\\nintervention for a single patient. We propose a learning-based cooperative\\nsegmentation approach which includes the computing entity as well as the user\\ninto the task. Our system builds upon a state-of-the-art fully convolutional\\nartificial neural network (FCN) as well as an active user model for training.\\nDuring the segmentation process, a user of the trained system can iteratively\\nadd additional hints in form of pictorial scribbles as seed points into the FCN\\nsystem to achieve an interactive and precise segmentation result. The\\nsegmentation quality of interactive FCNs is evaluated. Iterative FCN approaches\\ncan yield superior results compared to networks without the user input channel\\ncomponent, due to a consistent improvement in segmentation quality after each\\ninteraction.',\n",
              " 'Lightweight Neural Networks\\nMost of the weights in a Lightweight Neural Network have a value of zero,\\nwhile the remaining ones are either +1 or -1. These universal approximators\\nrequire approximately 1.1 bits/weight of storage, posses a quick forward pass\\nand achieve classification accuracies similar to conventional continuous-weight\\nnetworks. Their training regimen focuses on error reduction initially, but\\nlater emphasizes discretization of weights. They ignore insignificant inputs,\\nremove unnecessary weights, and drop unneeded hidden neurons. We have\\nsuccessfully tested them on the MNIST, credit card fraud, and credit card\\ndefaults data sets using networks having 2 to 16 hidden layers and up to 4.4\\nmillion weights.',\n",
              " 'Tensor Field Networks: Rotation- and Translation-Equivariant Neural\\n  Networks for 3D Point Clouds\\nWe introduce tensor field networks, which are locally equivariant to 3D\\nrotations, translations, and permutations of points at every layer. 3D rotation\\nequivariance removes the need for data augmentation to identify features in\\narbitrary orientations. Our network uses filters built from spherical\\nharmonics; due to the mathematical consequences of this filter choice, each\\nlayer accepts as input (and guarantees as output) scalars, vectors, and\\nhigher-order tensors, in the geometric sense of these terms. We demonstrate how\\ntensor field networks learn to model simple physics (Newtonian gravitation and\\nmoment of inertia), classify simple 3D shapes (trained on one orientation and\\ntested on shapes in arbitrary orientations), and, given a small organic\\nmolecule with an atom removed, replace the correct element at the correct\\nlocation in space.',\n",
              " \"Knowledge Matters: Importance of Prior Information for Optimization\\nWe explore the effect of introducing prior information into the intermediate\\nlevel of neural networks for a learning task on which all the state-of-the-art\\nmachine learning algorithms tested failed to learn. We motivate our work from\\nthe hypothesis that humans learn such intermediate concepts from other\\nindividuals via a form of supervision or guidance using a curriculum. The\\nexperiments we have conducted provide positive evidence in favor of this\\nhypothesis. In our experiments, a two-tiered MLP architecture is trained on a\\ndataset with 64x64 binary inputs images, each image with three sprites. The\\nfinal task is to decide whether all the sprites are the same or one of them is\\ndifferent. Sprites are pentomino tetris shapes and they are placed in an image\\nwith different locations using scaling and rotation transformations. The first\\npart of the two-tiered MLP is pre-trained with intermediate-level targets being\\nthe presence of sprites at each location, while the second part takes the\\noutput of the first part as input and predicts the final task's target binary\\nevent. The two-tiered MLP architecture, with a few tens of thousand examples,\\nwas able to learn the task perfectly, whereas all other algorithms (include\\nunsupervised pre-training, but also traditional algorithms like SVMs, decision\\ntrees and boosting) all perform no better than chance. We hypothesize that the\\noptimization difficulty involved when the intermediate pre-training is not\\nperformed is due to the {\\\\em composition} of two highly non-linear tasks. Our\\nfindings are also consistent with hypotheses on cultural learning inspired by\\nthe observations of optimization problems with deep learning, presumably\\nbecause of effective local minima.\",\n",
              " 'Zero-bias autoencoders and the benefits of co-adapting features\\nRegularized training of an autoencoder typically results in hidden unit\\nbiases that take on large negative values. We show that negative biases are a\\nnatural result of using a hidden layer whose responsibility is to both\\nrepresent the input data and act as a selection mechanism that ensures sparsity\\nof the representation. We then show that negative biases impede the learning of\\ndata distributions whose intrinsic dimensionality is high. We also propose a\\nnew activation function that decouples the two roles of the hidden layer and\\nthat allows us to learn representations on data with very high intrinsic\\ndimensionality, where standard autoencoders typically fail. Since the decoupled\\nactivation function acts like an implicit regularizer, the model can be trained\\nby minimizing the reconstruction error of training data, without requiring any\\nadditional regularization.',\n",
              " 'Theory and Tools for the Conversion of Analog to Spiking Convolutional\\n  Neural Networks\\nDeep convolutional neural networks (CNNs) have shown great potential for\\nnumerous real-world machine learning applications, but performing inference in\\nlarge CNNs in real-time remains a challenge. We have previously demonstrated\\nthat traditional CNNs can be converted into deep spiking neural networks\\n(SNNs), which exhibit similar accuracy while reducing both latency and\\ncomputational load as a consequence of their data-driven, event-based style of\\ncomputing. Here we provide a novel theory that explains why this conversion is\\nsuccessful, and derive from it several new tools to convert a larger and more\\npowerful class of deep networks into SNNs. We identify the main sources of\\napproximation errors in previous conversion methods, and propose simple\\nmechanisms to fix these issues. Furthermore, we develop spiking implementations\\nof common CNN operations such as max-pooling, softmax, and batch-normalization,\\nwhich allow almost loss-less conversion of arbitrary CNN architectures into the\\nspiking domain. Empirical evaluation of different network architectures on the\\nMNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.',\n",
              " 'Stacked Generative Adversarial Networks\\nIn this paper, we propose a novel generative model named Stacked Generative\\nAdversarial Networks (SGAN), which is trained to invert the hierarchical\\nrepresentations of a bottom-up discriminative network. Our model consists of a\\ntop-down stack of GANs, each learned to generate lower-level representations\\nconditioned on higher-level representations. A representation discriminator is\\nintroduced at each feature hierarchy to encourage the representation manifold\\nof the generator to align with that of the bottom-up discriminative network,\\nleveraging the powerful discriminative representations to guide the generative\\nmodel. In addition, we introduce a conditional loss that encourages the use of\\nconditional information from the layer above, and a novel entropy loss that\\nmaximizes a variational lower bound on the conditional entropy of generator\\noutputs. We first train each stack independently, and then train the whole\\nmodel end-to-end. Unlike the original GAN that uses a single noise vector to\\nrepresent all the variations, our SGAN decomposes variations into multiple\\nlevels and gradually resolves uncertainties in the top-down generative process.\\nBased on visual inspection, Inception scores and visual Turing test, we\\ndemonstrate that SGAN is able to generate images of much higher quality than\\nGANs without stacking.',\n",
              " 'Self-informed neural network structure learning\\nWe study the problem of large scale, multi-label visual recognition with a\\nlarge number of possible classes. We propose a method for augmenting a trained\\nneural network classifier with auxiliary capacity in a manner designed to\\nsignificantly improve upon an already well-performing model, while minimally\\nimpacting its computational footprint. Using the predictions of the network\\nitself as a descriptor for assessing visual similarity, we define a\\npartitioning of the label space into groups of visually similar entities. We\\nthen augment the network with auxilliary hidden layer pathways with\\nconnectivity only to these groups of label units. We report a significant\\nimprovement in mean average precision on a large-scale object recognition task\\nwith the augmented model, while increasing the number of multiply-adds by less\\nthan 3%.',\n",
              " 'Learning Activation Functions to Improve Deep Neural Networks\\nArtificial neural networks typically have a fixed, non-linear activation\\nfunction at each neuron. We have designed a novel form of piecewise linear\\nactivation function that is learned independently for each neuron using\\ngradient descent. With this adaptive activation function, we are able to\\nimprove upon deep neural network architectures composed of static rectified\\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\\nboson decay modes.',\n",
              " 'Denoising autoencoder with modulated lateral connections learns\\n  invariant representations of natural images\\nSuitable lateral connections between encoder and decoder are shown to allow\\nhigher layers of a denoising autoencoder (dAE) to focus on invariant\\nrepresentations. In regular autoencoders, detailed information needs to be\\ncarried through the highest layers but lateral connections from encoder to\\ndecoder relieve this pressure. It is shown that abstract invariant features can\\nbe translated to detailed reconstructions when invariant features are allowed\\nto modulate the strength of the lateral connection. Three dAE structures with\\nmodulated and additive lateral connections, and without lateral connections\\nwere compared in experiments using real-world images. The experiments verify\\nthat adding modulated lateral connections to the model 1) improves the accuracy\\nof the probability model for inputs, as measured by denoising performance; 2)\\nresults in representations whose degree of invariance grows faster towards the\\nhigher layers; and 3) supports the formation of diverse invariant poolings.',\n",
              " 'A Probabilistic Theory of Deep Learning\\nA grand challenge in machine learning is the development of computational\\nalgorithms that match or outperform humans in perceptual inference tasks that\\nare complicated by nuisance variation. For instance, visual object recognition\\ninvolves the unknown object position, orientation, and scale in object\\nrecognition while speech recognition involves the unknown voice pronunciation,\\npitch, and speed. Recently, a new breed of deep learning algorithms have\\nemerged for high-nuisance inference tasks that routinely yield pattern\\nrecognition systems with near- or super-human capabilities. But a fundamental\\nquestion remains: Why do they work? Intuitions abound, but a coherent framework\\nfor understanding, analyzing, and synthesizing deep learning architectures has\\nremained elusive. We answer this question by developing a new probabilistic\\nframework for deep learning based on the Deep Rendering Model: a generative\\nprobabilistic model that explicitly captures latent nuisance variation. By\\nrelaxing the generative model to a discriminative one, we can recover two of\\nthe current leading deep learning systems, deep convolutional neural networks\\nand random decision forests, providing insights into their successes and\\nshortcomings, as well as a principled route to their improvement.',\n",
              " 'Integrated Inference and Learning of Neural Factors in Structural\\n  Support Vector Machines\\nTackling pattern recognition problems in areas such as computer vision,\\nbioinformatics, speech or text recognition is often done best by taking into\\naccount task-specific statistical relations between output variables. In\\nstructured prediction, this internal structure is used to predict multiple\\noutputs simultaneously, leading to more accurate and coherent predictions.\\nStructural support vector machines (SSVMs) are nonprobabilistic models that\\noptimize a joint input-output function through margin-based learning. Because\\nSSVMs generally disregard the interplay between unary and interaction factors\\nduring the training phase, final parameters are suboptimal. Moreover, its\\nfactors are often restricted to linear combinations of input features, limiting\\nits generalization power. To improve prediction accuracy, this paper proposes:\\n(i) Joint inference and learning by integration of back-propagation and\\nloss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM\\nfactors to neural networks that form highly nonlinear functions of input\\nfeatures. Image segmentation benchmark results demonstrate improvements over\\nconventional SSVM training methods in terms of accuracy, highlighting the\\nfeasibility of end-to-end SSVM training with neural factors.',\n",
              " 'What Happened to My Dog in That Network: Unraveling Top-down Generators\\n  in Convolutional Neural Networks\\nTop-down information plays a central role in human perception, but plays\\nrelatively little role in many current state-of-the-art deep networks, such as\\nConvolutional Neural Networks (CNNs). This work seeks to explore a path by\\nwhich top-down information can have a direct impact within current deep\\nnetworks. We explore this path by learning and using \"generators\" corresponding\\nto the network internal effects of three types of transformation (each a\\nrestriction of a general affine transformation): rotation, scaling, and\\ntranslation. We demonstrate how these learned generators can be used to\\ntransfer top-down information to novel settings, as mediated by the \"feature\\nflows\" that the transformations (and the associated generators) correspond to\\ninside the network. Specifically, we explore three aspects: 1) using generators\\nas part of a method for synthesizing transformed images --- given a previously\\nunseen image, produce versions of that image corresponding to one or more\\nspecified transformations, 2) \"zero-shot learning\" --- when provided with a\\nfeature flow corresponding to the effect of a transformation of unknown amount,\\nleverage learned generators as part of a method by which to perform an accurate\\ncategorization of the amount of transformation, even for amounts never observed\\nduring training, and 3) (inside-CNN) \"data augmentation\" --- improve the\\nclassification performance of an existing network by using the learned\\ngenerators to directly provide additional training \"inside the CNN\".',\n",
              " 'Virtual Worlds as Proxy for Multi-Object Tracking Analysis\\nModern computer vision algorithms typically require expensive data\\nacquisition and accurate manual labeling. In this work, we instead leverage the\\nrecent progress in computer graphics to generate fully labeled, dynamic, and\\nphoto-realistic proxy virtual worlds. We propose an efficient real-to-virtual\\nworld cloning method, and validate our approach by building and publicly\\nreleasing a new video dataset, called Virtual KITTI (see\\nhttp://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),\\nautomatically labeled with accurate ground truth for object detection,\\ntracking, scene and instance segmentation, depth, and optical flow. We provide\\nquantitative experimental evidence suggesting that (i) modern deep learning\\nalgorithms pre-trained on real data behave similarly in real and virtual\\nworlds, and (ii) pre-training on virtual data improves performance. As the gap\\nbetween real and virtual worlds is small, virtual worlds enable measuring the\\nimpact of various weather and imaging conditions on recognition performance,\\nall other things being equal. We show these factors may affect drastically\\notherwise high-performing deep models for tracking.',\n",
              " 'Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet\\nVideo sequences contain rich dynamic patterns, such as dynamic texture\\npatterns that exhibit stationarity in the temporal domain, and action patterns\\nthat are non-stationary in either spatial or temporal domain. We show that a\\nspatial-temporal generative ConvNet can be used to model and synthesize dynamic\\npatterns. The model defines a probability distribution on the video sequence,\\nand the log probability is defined by a spatial-temporal ConvNet that consists\\nof multiple layers of spatial-temporal filters to capture spatial-temporal\\npatterns of different scales. The model can be learned from the training video\\nsequences by an \"analysis by synthesis\" learning algorithm that iterates the\\nfollowing two steps. Step 1 synthesizes video sequences from the currently\\nlearned model. Step 2 then updates the model parameters based on the difference\\nbetween the synthesized video sequences and the observed training sequences. We\\nshow that the learning algorithm can synthesize realistic dynamic patterns.',\n",
              " 'Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural\\n  Networks\\nTaking inspiration from biological evolution, we explore the idea of \"Can\\ndeep neural networks evolve naturally over successive generations into highly\\nefficient deep neural networks?\" by introducing the notion of synthesizing new\\nhighly efficient, yet powerful deep neural networks over successive generations\\nvia an evolutionary process from ancestor deep neural networks. The\\narchitectural traits of ancestor deep neural networks are encoded using\\nsynaptic probability models, which can be viewed as the `DNA\\' of these\\nnetworks. New descendant networks with differing network architectures are\\nsynthesized based on these synaptic probability models from the ancestor\\nnetworks and computational environmental factor models, in a random manner to\\nmimic heredity, natural selection, and random mutation. These offspring\\nnetworks are then trained into fully functional networks, like one would train\\na newborn, and have more efficient, more diverse network architectures than\\ntheir ancestor networks, while achieving powerful modeling capabilities.\\nExperimental results for the task of visual saliency demonstrated that the\\nsynthesized `evolved\\' offspring networks can achieve state-of-the-art\\nperformance while having network architectures that are significantly more\\nefficient (with a staggering $\\\\sim$48-fold decrease in synapses by the fourth\\ngeneration) compared to the original ancestor network.',\n",
              " 'Alternating Back-Propagation for Generator Network\\nThis paper proposes an alternating back-propagation algorithm for learning\\nthe generator network model. The model is a non-linear generalization of factor\\nanalysis. In this model, the mapping from the continuous latent factors to the\\nobserved signal is parametrized by a convolutional neural network. The\\nalternating back-propagation algorithm iterates the following two steps: (1)\\nInferential back-propagation, which infers the latent factors by Langevin\\ndynamics or gradient descent. (2) Learning back-propagation, which updates the\\nparameters given the inferred latent factors by gradient descent. The gradient\\ncomputations in both steps are powered by back-propagation, and they share most\\nof their code in common. We show that the alternating back-propagation\\nalgorithm can learn realistic generator models of natural images, video\\nsequences, and sounds. Moreover, it can also be used to learn from incomplete\\nor indirect training data.',\n",
              " 'Hyperparameter Transfer Learning through Surrogate Alignment for\\n  Efficient Deep Neural Network Training\\nRecently, several optimization methods have been successfully applied to the\\nhyperparameter optimization of deep neural networks (DNNs). The methods work by\\nmodeling the joint distribution of hyperparameter values and corresponding\\nerror. Those methods become less practical when applied to modern DNNs whose\\ntraining may take a few days and thus one cannot collect sufficient\\nobservations to accurately model the distribution. To address this challenging\\nissue, we propose a method that learns to transfer optimal hyperparameter\\nvalues for a small source dataset to hyperparameter values with comparable\\nperformance on a dataset of interest. As opposed to existing transfer learning\\nmethods, our proposed method does not use hand-designed features. Instead, it\\nuses surrogates to model the hyperparameter-error distributions of the two\\ndatasets and trains a neural network to learn the transfer function. Extensive\\nexperiments on three CV benchmark datasets clearly demonstrate the efficiency\\nof our method.',\n",
              " 'Towards Bayesian Deep Learning: A Framework and Some Existing Methods\\nWhile perception tasks such as visual object recognition and text\\nunderstanding play an important role in human intelligence, the subsequent\\ntasks that involve inference, reasoning and planning require an even higher\\nlevel of intelligence. The past few years have seen major advances in many\\nperception tasks using deep learning models. For higher-level inference,\\nhowever, probabilistic graphical models with their Bayesian nature are still\\nmore powerful and flexible. To achieve integrated intelligence that involves\\nboth perception and inference, it is naturally desirable to tightly integrate\\ndeep learning and Bayesian models within a principled probabilistic framework,\\nwhich we call Bayesian deep learning. In this unified framework, the perception\\nof text or images using deep learning can boost the performance of higher-level\\ninference and in return, the feedback from the inference process is able to\\nenhance the perception of text or images. This paper proposes a general\\nframework for Bayesian deep learning and reviews its recent applications on\\nrecommender systems, topic models, and control. In this paper, we also discuss\\nthe relationship and differences between Bayesian deep learning and other\\nrelated topics like Bayesian treatment of neural networks.',\n",
              " 'Deciding How to Decide: Dynamic Routing in Artificial Neural Networks\\nWe propose and systematically evaluate three strategies for training\\ndynamically-routed artificial neural networks: graphs of learned\\ntransformations through which different input signals may take different paths.\\nThough some approaches have advantages over others, the resulting networks are\\noften qualitatively similar. We find that, in dynamically-routed networks\\ntrained to classify images, layers and branches become specialized to process\\ndistinct categories of images. Additionally, given a fixed computational\\nbudget, dynamically-routed networks tend to perform better than comparable\\nstatically-routed networks.',\n",
              " 'Pixel Deconvolutional Networks\\nDeconvolutional layers have been widely used in a variety of deep models for\\nup-sampling, including encoder-decoder networks for semantic segmentation and\\ndeep generative models for unsupervised learning. One of the key limitations of\\ndeconvolutional operations is that they result in the so-called checkerboard\\nproblem. This is caused by the fact that no direct relationship exists among\\nadjacent pixels on the output feature map. To address this problem, we propose\\nthe pixel deconvolutional layer (PixelDCL) to establish direct relationships\\namong adjacent pixels on the up-sampled feature map. Our method is based on a\\nfresh interpretation of the regular deconvolution operation. The resulting\\nPixelDCL can be used to replace any deconvolutional layer in a plug-and-play\\nmanner without compromising the fully trainable capabilities of original\\nmodels. The proposed PixelDCL may result in slight decrease in efficiency, but\\nthis can be overcome by an implementation trick. Experimental results on\\nsemantic segmentation demonstrate that PixelDCL can consider spatial features\\nsuch as edges and shapes and yields more accurate segmentation outputs than\\ndeconvolutional layers. When used in image generation tasks, our PixelDCL can\\nlargely overcome the checkerboard problem suffered by regular deconvolution\\noperations.',\n",
              " 'Gaussian Prototypical Networks for Few-Shot Learning on Omniglot\\nWe propose a novel architecture for $k$-shot classification on the Omniglot\\ndataset. Building on prototypical networks, we extend their architecture to\\nwhat we call Gaussian prototypical networks. Prototypical networks learn a map\\nbetween images and embedding vectors, and use their clustering for\\nclassification. In our model, a part of the encoder output is interpreted as a\\nconfidence region estimate about the embedding point, and expressed as a\\nGaussian covariance matrix. Our network then constructs a direction and class\\ndependent distance metric on the embedding space, using uncertainties of\\nindividual data points as weights. We show that Gaussian prototypical networks\\nare a preferred architecture over vanilla prototypical networks with an\\nequivalent number of parameters. We report state-of-the-art performance in\\n1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot\\n5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.\\nWe explore artificially down-sampling a fraction of images in the training set,\\nwhich improves our performance even further. We therefore hypothesize that\\nGaussian prototypical networks might perform better in less homogeneous,\\nnoisier datasets, which are commonplace in real world applications.',\n",
              " 'Super-Convergence: Very Fast Training of Residual Networks Using Large\\n  Learning Rates\\nIn this paper, we show a phenomenon, which we named \"super-convergence\",\\nwhere residual networks can be trained using an order of magnitude fewer\\niterations than is used with standard training methods. The existence of\\nsuper-convergence is relevant to understanding why deep networks generalize\\nwell. One of the key elements of super-convergence is training with cyclical\\nlearning rates and a large maximum learning rate. Furthermore, we present\\nevidence that training with large learning rates improves performance by\\nregularizing the network. In addition, we show that super-convergence provides\\na greater boost in performance relative to standard training when the amount of\\nlabeled training data is limited. We also derive a simplification of the\\nHessian Free optimization method to compute an estimate of the optimal learning\\nrate. The architectures and code to replicate the figures in this paper are\\navailable at github.com/lnsmith54/super-convergence.',\n",
              " 'Generative learning for deep networks\\nLearning, taking into account full distribution of the data, referred to as\\ngenerative, is not feasible with deep neural networks (DNNs) because they model\\nonly the conditional distribution of the outputs given the inputs. Current\\nsolutions are either based on joint probability models facing difficult\\nestimation problems or learn two separate networks, mapping inputs to outputs\\n(recognition) and vice-versa (generation). We propose an intermediate approach.\\nFirst, we show that forward computation in DNNs with logistic sigmoid\\nactivations corresponds to a simplified approximate Bayesian inference in a\\ndirected probabilistic multi-layer model. This connection allows to interpret\\nDNN as a probabilistic model of the output and all hidden units given the\\ninput. Second, we propose that in order for the recognition and generation\\nnetworks to be more consistent with the joint model of the data, weights of the\\nrecognition and generator network should be related by transposition. We\\ndemonstrate in a tentative experiment that such a coupled pair can be learned\\ngeneratively, modelling the full distribution of the data, and has enough\\ncapacity to perform well in both recognition and generation.',\n",
              " 'Hierarchical Representations for Efficient Architecture Search\\nWe explore efficient neural architecture search methods and show that a\\nsimple yet powerful evolutionary algorithm can discover new architectures with\\nexcellent performance. Our approach combines a novel hierarchical genetic\\nrepresentation scheme that imitates the modularized design pattern commonly\\nadopted by human experts, and an expressive search space that supports complex\\ntopologies. Our algorithm efficiently discovers architectures that outperform a\\nlarge number of manually designed models for image classification, obtaining\\ntop-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which\\nis competitive with the best existing neural architecture search approaches. We\\nalso present results using random search, achieving 0.3% less top-1 accuracy on\\nCIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36\\nhours down to 1 hour.',\n",
              " 'Data Augmentation Generative Adversarial Networks\\nEffective training of neural networks requires much data. In the low-data\\nregime, parameters are underdetermined, and learnt networks generalise poorly.\\nData Augmentation alleviates this by using existing data more effectively.\\nHowever standard data augmentation produces only limited plausible alternative\\ndata. Given there is potential to generate a much broader set of augmentations,\\nwe design and train a generative model to do data augmentation. The model,\\nbased on image conditional Generative Adversarial Networks, takes data from a\\nsource domain and learns to take any data item and generalise it to generate\\nother within-class data items. As this generative process does not depend on\\nthe classes themselves, it can be applied to novel unseen classes of data. We\\nshow that a Data Augmentation Generative Adversarial Network (DAGAN) augments\\nstandard vanilla classifiers well. We also show a DAGAN can enhance few-shot\\nlearning systems such as Matching Networks. We demonstrate these approaches on\\nOmniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In\\nour experiments we can see over 13% increase in accuracy in the low-data regime\\nexperiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face\\n(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%\\n(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).',\n",
              " \"DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the\\n  Jigsaw Puzzle Problem\\nThis paper introduces the first deep neural network-based estimation metric\\nfor the jigsaw puzzle problem. Given two puzzle piece edges, the neural network\\npredicts whether or not they should be adjacent in the correct assembly of the\\npuzzle, using nothing but the pixels of each piece. The proposed metric\\nexhibits an extremely high precision even though no manual feature extraction\\nis performed. When incorporated into an existing puzzle solver, the solution's\\naccuracy increases significantly, achieving thereby a new state-of-the-art\\nstandard.\",\n",
              " 'DeepPainter: Painter Classification Using Deep Convolutional\\n  Autoencoders\\nIn this paper we describe the problem of painter classification, and propose\\na novel approach based on deep convolutional autoencoder neural networks. While\\nprevious approaches relied on image processing and manual feature extraction\\nfrom paintings, our approach operates on the raw pixel level, without any\\npreprocessing or manual feature extraction. We first train a deep convolutional\\nautoencoder on a dataset of paintings, and subsequently use it to initialize a\\nsupervised convolutional neural network for the classification phase.\\n  The proposed approach substantially outperforms previous methods, improving\\nthe previous state-of-the-art for the 3-painter classification problem from\\n90.44% accuracy (previous state-of-the-art) to 96.52% accuracy, i.e., a 63%\\nreduction in error rate.',\n",
              " 'DeepBrain: Functional Representation of Neural In-Situ Hybridization\\n  Images for Gene Ontology Classification Using Deep Convolutional Autoencoders\\nThis paper presents a novel deep learning-based method for learning a\\nfunctional representation of mammalian neural images. The method uses a deep\\nconvolutional denoising autoencoder (CDAE) for generating an invariant, compact\\nrepresentation of in situ hybridization (ISH) images. While most existing\\nmethods for bio-imaging analysis were not developed to handle images with\\nhighly complex anatomical structures, the results presented in this paper show\\nthat functional representation extracted by CDAE can help learn features of\\nfunctional gene ontology categories for their classification in a highly\\naccurate manner. Using this CDAE representation, our method outperforms the\\nprevious state-of-the-art classification rate, by improving the average AUC\\nfrom 0.92 to 0.98, i.e., achieving 75% reduction in error. The method operates\\non input images that were downsampled significantly with respect to the\\noriginal ones to make it computationally feasible.',\n",
              " 'Generative Adversarial Perturbations\\nIn this paper, we propose novel generative models for creating adversarial\\nexamples, slightly perturbed images resembling natural images but maliciously\\ncrafted to fool pre-trained models. We present trainable deep neural networks\\nfor transforming images to adversarial perturbations. Our proposed models can\\nproduce image-agnostic and image-dependent perturbations for both targeted and\\nnon-targeted attacks. We also demonstrate that similar architectures can\\nachieve impressive results in fooling classification and semantic segmentation\\nmodels, obviating the need for hand-crafting attack methods for each task.\\nUsing extensive experiments on challenging high-resolution datasets such as\\nImageNet and Cityscapes, we show that our perturbations achieve high fooling\\nrates with small perturbation norms. Moreover, our attacks are considerably\\nfaster than current iterative methods at inference time.',\n",
              " 'A Rotation and a Translation Suffice: Fooling CNNs with Simple\\n  Transformations\\nWe show that simple transformations, namely translations and rotations alone,\\nare sufficient to fool neural network-based vision models on a significant\\nfraction of inputs. This is in sharp contrast to previous work that relied on\\nmore complicated optimization approaches that are unlikely to appear outside of\\na truly adversarial setting. Moreover, fooling rotations and translations are\\neasy to find and require only a few black-box queries to the target model.\\nOverall, our findings emphasize the need for designing robust classifiers even\\nin natural, benign contexts.',\n",
              " \"Peephole: Predicting Network Performance Before Training\\nThe quest for performant networks has been a significant force that drives\\nthe advancements of deep learning in recent years. While rewarding, improving\\nnetwork design has never been an easy journey. The large design space combined\\nwith the tremendous cost required for network training poses a major obstacle\\nto this endeavor. In this work, we propose a new approach to this problem,\\nnamely, predicting the performance of a network before training, based on its\\narchitecture. Specifically, we develop a unified way to encode individual\\nlayers into vectors and bring them together to form an integrated description\\nvia LSTM. Taking advantage of the recurrent network's strong expressive power,\\nthis method can reliably predict the performances of various network\\narchitectures. Our empirical studies showed that it not only achieved accurate\\npredictions but also produced consistent rankings across datasets -- a key\\ndesideratum in performance prediction.\",\n",
              " 'An Architecture Combining Convolutional Neural Network (CNN) and Support\\n  Vector Machine (SVM) for Image Classification\\nConvolutional neural networks (CNNs) are similar to \"ordinary\" neural\\nnetworks in the sense that they are made up of hidden layers consisting of\\nneurons with \"learnable\" parameters. These neurons receive inputs, performs a\\ndot product, and then follows it with a non-linearity. The whole network\\nexpresses the mapping between raw image pixels and their class scores.\\nConventionally, the Softmax function is the classifier used at the last layer\\nof this network. However, there have been studies (Alalshekmubarak and Smith,\\n2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited\\nstudies introduce the usage of linear support vector machine (SVM) in an\\nartificial neural network architecture. This project is yet another take on the\\nsubject, and is inspired by (Tang, 2013). Empirical data has shown that the\\nCNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST\\ndataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax\\nwas able to achieve a test accuracy of ~99.23% using the same dataset. Both\\nmodels were also tested on the recently-published Fashion-MNIST dataset (Xiao,\\nRasul, and Vollgraf, 2017), which is suppose to be a more difficult image\\nclassification dataset than MNIST (Zalandoresearch, 2017). This proved to be\\nthe case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax\\nreached a test accuracy of ~91.86%. The said results may be improved if data\\npreprocessing techniques were employed on the datasets, and if the base CNN\\nmodel was a relatively more sophisticated than the one used in this study.',\n",
              " 'Benchmarking Decoupled Neural Interfaces with Synthetic Gradients\\nArtifical Neural Networks are a particular class of learning systems modeled\\nafter biological neural functions with an interesting penchant for Hebbian\\nlearning, that is \"neurons that wire together, fire together\". However, unlike\\ntheir natural counterparts, artificial neural networks have a close and\\nstringent coupling between the modules of neurons in the network. This coupling\\nor locking imposes upon the network a strict and inflexible structure that\\nprevent layers in the network from updating their weights until a full\\nfeed-forward and backward pass has occurred. Such a constraint though may have\\nsufficed for a while, is now no longer feasible in the era of very-large-scale\\nmachine learning, coupled with the increased desire for parallelization of the\\nlearning process across multiple computing infrastructures. To solve this\\nproblem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are\\nintroduced as a viable alternative to the backpropagation algorithm. This paper\\nperforms a speed benchmark to compare the speed and accuracy capabilities of\\nSG-DNI as opposed to a standard neural interface using multilayer perceptron\\nMLP. SG-DNI shows good promise, in that it not only captures the learning\\nproblem, it is also over 3-fold faster due to it asynchronous learning\\ncapabilities.',\n",
              " 'Segmentation hiérarchique faiblement supervisée\\nImage segmentation is the process of partitioning an image into a set of\\nmeaningful regions according to some criteria. Hierarchical segmentation has\\nemerged as a major trend in this regard as it favors the emergence of important\\nregions at different scales. On the other hand, many methods allow us to have\\nprior information on the position of structures of interest in the images. In\\nthis paper, we present a versatile hierarchical segmentation method that takes\\ninto account any prior spatial information and outputs a hierarchical\\nsegmentation that emphasizes the contours or regions of interest while\\npreserving the important structures in the image. An application of this method\\nto the weakly-supervised segmentation problem is presented.',\n",
              " 'Training wide residual networks for deployment using a single bit for\\n  each weight\\nFor fast and energy-efficient deployment of trained deep neural networks on\\nresource-constrained embedded hardware, each learned weight parameter should\\nideally be represented and stored using a single bit. Error-rates usually\\nincrease when this requirement is imposed. Here, we report large improvements\\nin error rates on multiple datasets, for deep convolutional neural networks\\ndeployed with 1-bit-per-weight. Using wide residual networks as our main\\nbaseline, our approach simplifies existing methods that binarize weights by\\napplying the sign function in training; we apply scaling factors for each layer\\nwith constant unlearned values equal to the layer-specific standard deviations\\nused for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with\\n1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve\\nerror rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We\\nalso considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test\\nresults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error\\nrates halve previously reported values, and are within about 1% of our\\nerror-rates for the same network with full-precision weights. For networks that\\noverfit, we also show significant improvements in error rate by not learning\\nbatch normalization scale and offset parameters. This applies to both full\\nprecision and 1-bit-per-weight networks. Using a warm-restart learning-rate\\nschedule, we found that training for 1-bit-per-weight is just as fast as\\nfull-precision networks, with better accuracy than standard schedules, and\\nachieved about 98%-99% of peak performance in just 62 training epochs for\\nCIFAR-10/100. For full training code and trained models in MATLAB, Keras and\\nPyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .',\n",
              " 'Deep Learning using Rectified Linear Units (ReLU)\\nWe introduce the use of rectified linear units (ReLU) as the classification\\nfunction in a deep neural network (DNN). Conventionally, ReLU is used as an\\nactivation function in DNNs, with Softmax function as their classification\\nfunction. However, there have been several studies on using a classification\\nfunction other than Softmax, and this study is an addition to those. We\\naccomplish this by taking the activation of the penultimate layer $h_{n - 1}$\\nin a neural network, then multiply it by weight parameters $\\\\theta$ to get the\\nraw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$,\\ni.e. $f(o) = \\\\max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide\\nclass predictions $\\\\hat{y}$ through argmax function, i.e. argmax $f(x)$.',\n",
              " \"Rectified Factor Networks\\nWe propose rectified factor networks (RFNs) to efficiently construct very\\nsparse, non-linear, high-dimensional representations of the input. RFN models\\nidentify rare and small events in the input, have a low interference between\\ncode units, have a small reconstruction error, and explain the data covariance\\nstructure. RFN learning is a generalized alternating minimization algorithm\\nderived from the posterior regularization method which enforces non-negative\\nand normalized posterior means. We proof convergence and correctness of the RFN\\nlearning algorithm. On benchmarks, RFNs are compared to other unsupervised\\nmethods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to\\nprevious sparse coding methods, RFNs yield sparser codes, capture the data's\\ncovariance structure more precisely, and have a significantly smaller\\nreconstruction error. We test RFNs as pretraining technique for deep networks\\non different vision datasets, where RFNs were superior to RBMs and\\nautoencoders. On gene expression data from two pharmaceutical drug discovery\\nstudies, RFNs detected small and rare gene modules that revealed highly\\nrelevant new biological insights which were so far missed by other unsupervised\\nmethods.\",\n",
              " 'From Maxout to Channel-Out: Encoding Information on Sparse Pathways\\nMotivated by an important insight from neural science, we propose a new\\nframework for understanding the success of the recently proposed \"maxout\"\\nnetworks. The framework is based on encoding information on sparse pathways and\\nrecognizing the correct pathway at inference time. Elaborating further on this\\ninsight, we propose a novel deep network architecture, called \"channel-out\"\\nnetwork, which takes a much better advantage of sparse pathway encoding. In\\nchannel-out networks, pathways are not only formed a posteriori, but they are\\nalso actively selected according to the inference outputs from the lower\\nlayers. From a mathematical perspective, channel-out networks can represent a\\nwider class of piece-wise continuous functions, thereby endowing the network\\nwith more expressive power than that of maxout networks. We test our\\nchannel-out networks on several well-known image classification benchmarks,\\nsetting new state-of-the-art performance on CIFAR-100 and STL-10, which\\nrepresent some of the \"harder\" image classification benchmarks.',\n",
              " 'Competitive Learning with Feedforward Supervisory Signal for Pre-trained\\n  Multilayered Networks\\nWe propose a novel learning method for multilayered neural networks which\\nuses feedforward supervisory signal and associates classification of a new\\ninput with that of pre-trained input. The proposed method effectively uses rich\\ninput information in the earlier layer for robust leaning and revising internal\\nrepresentation in a multilayer neural network.',\n",
              " 'Deeply-Supervised Nets\\nOur proposed deeply-supervised nets (DSN) method simultaneously minimizes\\nclassification error while making the learning process of hidden layers direct\\nand transparent. We make an attempt to boost the classification performance by\\nstudying a new formulation in deep networks. Three aspects in convolutional\\nneural networks (CNN) style architectures are being looked at: (1) transparency\\nof the intermediate layers to the overall classification; (2)\\ndiscriminativeness and robustness of learned features, especially in the early\\nlayers; (3) effectiveness in training due to the presence of the exploding and\\nvanishing gradients. We introduce \"companion objective\" to the individual\\nhidden layers, in addition to the overall objective at the output layer (a\\ndifferent strategy to layer-wise pre-training). We extend techniques from\\nstochastic gradient methods to analyze our algorithm. The advantage of our\\nmethod is evident and our experimental result on benchmark datasets shows\\nsignificant performance gain over existing methods (e.g. all state-of-the-art\\nresults on MNIST, CIFAR-10, CIFAR-100, and SVHN).',\n",
              " 'Path-SGD: Path-Normalized Optimization in Deep Neural Networks\\nWe revisit the choice of SGD for training deep neural networks by\\nreconsidering the appropriate geometry in which to optimize the weights. We\\nargue for a geometry invariant to rescaling of weights that does not affect the\\noutput of the network, and suggest Path-SGD, which is an approximate steepest\\ndescent method with respect to a path-wise regularizer related to max-norm\\nregularization. Path-SGD is easy and efficient to implement and leads to\\nempirical gains over SGD and AdaGrad.',\n",
              " \"Adapting Resilient Propagation for Deep Learning\\nThe Resilient Propagation (Rprop) algorithm has been very popular for\\nbackpropagation training of multilayer feed-forward neural networks in various\\napplications. The standard Rprop however encounters difficulties in the context\\nof deep neural networks as typically happens with gradient-based learning\\nalgorithms. In this paper, we propose a modification of the Rprop that combines\\nstandard Rprop steps with a special drop out technique. We apply the method for\\ntraining Deep Neural Networks as standalone components and in ensemble\\nformulations. Results on the MNIST dataset show that the proposed modification\\nalleviates standard Rprop's problems demonstrating improved learning speed and\\naccuracy.\",\n",
              " 'Convolutional Neural Network for Stereotypical Motor Movement Detection\\n  in Autism\\nAutism Spectrum Disorders (ASDs) are often associated with specific atypical\\npostural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have\\na specific visibility. While the identification and the quantification of SMM\\npatterns remain complex, its automation would provide support to accurate\\ntuning of the intervention in the therapy of autism. Therefore, it is essential\\nto develop automatic SMM detection systems in a real world setting, taking care\\nof strong inter-subject and intra-subject variability. Wireless accelerometer\\nsensing technology can provide a valid infrastructure for real-time SMM\\ndetection, however such variability remains a problem also for machine learning\\nmethods, in particular whenever handcrafted features extracted from\\naccelerometer signal are considered. Here, we propose to employ the deep\\nlearning paradigm in order to learn discriminating features from multi-sensor\\naccelerometer signals. Our results provide preliminary evidence that feature\\nlearning and transfer learning embedded in the deep architecture achieve higher\\naccurate SMM detectors in longitudinal scenarios.',\n",
              " 'Resnet in Resnet: Generalizing Residual Architectures\\nResidual networks (ResNets) have recently achieved state-of-the-art on\\nchallenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep\\ndual-stream architecture that generalizes ResNets and standard CNNs and is\\neasily implemented with no computational overhead. RiR consistently improves\\nperformance over ResNets, outperforms architectures with similar amounts of\\naugmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.',\n",
              " 'Evolutionary Synthesis of Deep Neural Networks via Synaptic\\n  Cluster-driven Genetic Encoding\\nThere has been significant recent interest towards achieving highly efficient\\ndeep neural network architectures. A promising paradigm for achieving this is\\nthe concept of evolutionary deep intelligence, which attempts to mimic\\nbiological evolution processes to synthesize highly-efficient deep neural\\nnetworks over successive generations. An important aspect of evolutionary deep\\nintelligence is the genetic encoding scheme used to mimic heredity, which can\\nhave a significant impact on the quality of offspring deep neural networks.\\nMotivated by the neurobiological phenomenon of synaptic clustering, we\\nintroduce a new genetic encoding scheme where synaptic probability is driven\\ntowards the formation of a highly sparse set of synaptic clusters. Experimental\\nresults for the task of image classification demonstrated that the synthesized\\noffspring networks using this synaptic cluster-driven genetic encoding scheme\\ncan achieve state-of-the-art performance while having network architectures\\nthat are not only significantly more efficient (with a ~125-fold decrease in\\nsynapses for MNIST) compared to the original ancestor network, but also\\ntailored for GPU-accelerated machine learning applications.',\n",
              " 'Neural Photo Editing with Introspective Adversarial Networks\\nThe increasingly photorealistic sample quality of generative image models\\nsuggests their feasibility in applications beyond image generation. We present\\nthe Neural Photo Editor, an interface that leverages the power of generative\\nneural networks to make large, semantically coherent changes to existing\\nimages. To tackle the challenge of achieving accurate reconstructions without\\nloss of feature quality, we introduce the Introspective Adversarial Network, a\\nnovel hybridization of the VAE and GAN. Our model efficiently captures\\nlong-range dependencies through use of a computational block based on\\nweight-shared dilated convolutions, and improves generalization performance\\nwith Orthogonal Regularization, a novel weight regularization method. We\\nvalidate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples\\nand reconstructions with high visual fidelity.',\n",
              " 'Adaptive Neural Networks for Efficient Inference\\nWe present an approach to adaptively utilize deep neural networks in order to\\nreduce the evaluation time on new examples without loss of accuracy. Rather\\nthan attempting to redesign or approximate existing networks, we propose two\\nschemes that adaptively utilize networks. We first pose an adaptive network\\nevaluation scheme, where we learn a system to adaptively choose the components\\nof a deep network to be evaluated for each example. By allowing examples\\ncorrectly classified using early layers of the system to exit, we avoid the\\ncomputational time associated with full evaluation of the network. We extend\\nthis to learn a network selection system that adaptively selects the network to\\nbe evaluated for each example. We show that computational time can be\\ndramatically reduced by exploiting the fact that many examples can be correctly\\nclassified using relatively efficient networks and that complex,\\ncomputationally costly networks are only necessary for a small fraction of\\nexamples. We pose a global objective for learning an adaptive early exit or\\nnetwork selection policy and solve it by reducing the policy learning problem\\nto a layer-by-layer weighted binary classification problem. Empirically, these\\napproaches yield dramatic reductions in computational cost, with up to a 2.8x\\nspeedup on state-of-the-art networks from the ImageNet image recognition\\nchallenge with minimal (<1%) loss of top5 accuracy.',\n",
              " 'Spatial Variational Auto-Encoding via Matrix-Variate Normal\\n  Distributions\\nThe key idea of variational auto-encoders (VAEs) resembles that of\\ntraditional auto-encoder models in which spatial information is supposed to be\\nexplicitly encoded in the latent space. However, the latent variables in VAEs\\nare vectors, which are commonly interpreted as multiple feature maps of size\\n1x1. Such representations can only convey spatial information implicitly when\\ncoupled with powerful decoders. In this work, we propose spatial VAEs that use\\nlatent variables as feature maps of larger size to explicitly capture spatial\\ninformation. This is achieved by allowing the latent variables to be sampled\\nfrom matrix-variate normal (MVN) distributions whose parameters are computed\\nfrom the encoder network. To increase dependencies among locations on latent\\nfeature maps and reduce the number of parameters, we further propose spatial\\nVAEs via low-rank MVN distributions. Experimental results show that the\\nproposed spatial VAEs outperform original VAEs in capturing rich structural and\\nspatial information.',\n",
              " 'Dense Transformer Networks\\nThe key idea of current deep learning methods for dense prediction is to\\napply a model on a regular patch centered on each pixel to make pixel-wise\\npredictions. These methods are limited in the sense that the patches are\\ndetermined by network architecture instead of learned from data. In this work,\\nwe propose the dense transformer networks, which can learn the shapes and sizes\\nof patches from data. The dense transformer networks employ an encoder-decoder\\narchitecture, and a pair of dense transformer modules are inserted into each of\\nthe encoder and decoder paths. The novelty of this work is that we provide\\ntechnical solutions for learning the shapes and sizes of patches from data and\\nefficiently restoring the spatial correspondence required for dense prediction.\\nThe proposed dense transformer modules are differentiable, thus the entire\\nnetwork can be trained. We apply the proposed networks on natural and\\nbiological image segmentation tasks and show superior performance is achieved\\nin comparison to baseline methods.',\n",
              " 'Progressive Learning for Systematic Design of Large Neural Networks\\nWe develop an algorithm for systematic design of a large artificial neural\\nnetwork using a progression property. We find that some non-linear functions,\\nsuch as the rectifier linear unit and its derivatives, hold the property. The\\nsystematic design addresses the choice of network size and regularization of\\nparameters. The number of nodes and layers in network increases in progression\\nwith the objective of consistently reducing an appropriate cost. Each layer is\\noptimized at a time, where appropriate parameters are learned using convex\\noptimization. Regularization parameters for convex optimization do not need a\\nsignificant manual effort for tuning. We also use random instances for some\\nweight matrices, and that helps to reduce the number of parameters we learn.\\nThe developed network is expected to show good generalization power due to\\nappropriate regularization and use of random weights in the layers. This\\nexpectation is verified by extensive experiments for classification and\\nregression problems, using standard databases.',\n",
              " 'A Classification-Based Perspective on GAN Distributions\\nA fundamental, and still largely unanswered, question in the context of\\nGenerative Adversarial Networks (GANs) is whether GANs are actually able to\\ncapture the key characteristics of the datasets they are trained on. The\\ncurrent approaches to examining this issue require significant human\\nsupervision, such as visual inspection of sampled images, and often offer only\\nfairly limited scalability. In this paper, we propose new techniques that\\nemploy a classification-based perspective to evaluate synthetic GAN\\ndistributions and their capability to accurately reflect the essential\\nproperties of the training data. These techniques require only minimal human\\nsupervision and can easily be scaled and adapted to evaluate a variety of\\nstate-of-the-art GANs on large, popular datasets. Our analysis indicates that\\nGANs have significant problems in reproducing the more distributional\\nproperties of the training dataset. In particular, when seen through the lens\\nof classification, the diversity of GAN data is orders of magnitude less than\\nthat of the original data.',\n",
              " 'Learning Visual Reasoning Without Strong Priors\\nAchieving artificial visual reasoning - the ability to answer image-related\\nquestions which require a multi-step, high-level process - is an important step\\ntowards artificial general intelligence. This multi-modal task requires\\nlearning a question-dependent, structured reasoning process over images from\\nlanguage. Standard deep learning approaches tend to exploit biases in the data\\nrather than learn this underlying structure, while leading methods learn to\\nvisually reason successfully but are hand-crafted for reasoning. We show that a\\ngeneral-purpose, Conditional Batch Normalization approach achieves\\nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%\\nerror rate. We outperform the next best end-to-end method (4.5%) and even\\nmethods that use extra supervision (3.1%). We probe our model to shed light on\\nhow it reasons, showing it has learned a question-dependent, multi-step\\nprocess. Previous work has operated under the assumption that visual reasoning\\ncalls for a specialized architecture, but we show that a general architecture\\nwith proper conditioning can learn to visually reason effectively.',\n",
              " 'Men Also Like Shopping: Reducing Gender Bias Amplification using\\n  Corpus-level Constraints\\nLanguage is increasingly being used to define rich visual recognition\\nproblems with supporting image collections sourced from the web. Structured\\nprediction models are used in these tasks to take advantage of correlations\\nbetween co-occurring labels and visual input but risk inadvertently encoding\\nsocial biases found in web corpora. In this work, we study data and models\\nassociated with multilabel object classification and visual semantic role\\nlabeling. We find that (a) datasets for these tasks contain significant gender\\nbias and (b) models trained on these datasets further amplify existing bias.\\nFor example, the activity cooking is over 33% more likely to involve females\\nthan males in a training set, and a trained model further amplifies the\\ndisparity to 68% at test time. We propose to inject corpus-level constraints\\nfor calibrating existing structured prediction models and design an algorithm\\nbased on Lagrangian relaxation for collective inference. Our method results in\\nalmost no performance loss for the underlying recognition task but decreases\\nthe magnitude of bias amplification by 47.5% and 40.5% for multilabel\\nclassification and visual semantic role labeling, respectively.',\n",
              " 'Acquiring Common Sense Spatial Knowledge through Implicit Spatial\\n  Templates\\nSpatial understanding is a fundamental problem with wide-reaching real-world\\napplications. The representation of spatial knowledge is often modeled with\\nspatial templates, i.e., regions of acceptability of two objects under an\\nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with\\nprior work that restricts spatial templates to explicit spatial prepositions\\n(e.g., \"glass on table\"), here we extend this concept to implicit spatial\\nlanguage, i.e., those relationships (generally actions) for which the spatial\\narrangement of the objects is only implicitly implied (e.g., \"man riding\\nhorse\"). In contrast with explicit relationships, predicting spatial\\narrangements from implicit spatial language requires significant common sense\\nspatial understanding. Here, we introduce the task of predicting spatial\\ntemplates for two objects under a relationship, which can be seen as a spatial\\nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t.\\na horse when the man is walking the horse?\"). We present two simple\\nneural-based models that leverage annotated images and structured text to learn\\nthis task. The good performance of these models reveals that spatial locations\\nare to a large extent predictable from implicit spatial language. Crucially,\\nthe models attain similar performance in a challenging generalized setting,\\nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have\\nnever been seen before. Next, we go one step further by presenting the models\\nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging\\nword embeddings enables the models to output accurate spatial predictions,\\nproving that the models acquire solid common sense spatial knowledge allowing\\nfor such generalization.',\n",
              " 'FiLM: Visual Reasoning with a General Conditioning Layer\\nWe introduce a general-purpose conditioning method for neural networks called\\nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network\\ncomputation via a simple, feature-wise affine transformation based on\\nconditioning information. We show that FiLM layers are highly effective for\\nvisual reasoning - answering image-related questions which require a\\nmulti-step, high-level process - a task which has proven difficult for standard\\ndeep learning methods that do not explicitly model reasoning. Specifically, we\\nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error\\nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are\\nrobust to ablations and architectural modifications, and 4) generalize well to\\nchallenging, new data from few examples or even zero-shot.',\n",
              " 'Unsupervised Induction of Semantic Roles within a Reconstruction-Error\\n  Minimization Framework\\nWe introduce a new approach to unsupervised estimation of feature-rich\\nsemantic role labeling models. Our model consists of two components: (1) an\\nencoding component: a semantic role labeling model which predicts roles given a\\nrich set of syntactic and lexical features; (2) a reconstruction component: a\\ntensor factorization model which relies on roles to predict argument fillers.\\nWhen the components are estimated jointly to minimize errors in argument\\nreconstruction, the induced roles largely correspond to roles defined in\\nannotated resources. Our method performs on par with most accurate role\\ninduction methods on English and German, even though, unlike these previous\\napproaches, we do not incorporate any prior linguistic knowledge about the\\nlanguages.',\n",
              " 'Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word\\n  Embeddings\\nThe blind application of machine learning runs the risk of amplifying biases\\npresent in data. Such a danger is facing us with word embedding, a popular\\nframework to represent text data as vectors which has been used in many machine\\nlearning and natural language processing tasks. We show that even word\\nembeddings trained on Google News articles exhibit female/male gender\\nstereotypes to a disturbing extent. This raises concerns because their\\nwidespread use, as we describe, often tends to amplify these biases.\\nGeometrically, gender bias is first shown to be captured by a direction in the\\nword embedding. Second, gender neutral words are shown to be linearly separable\\nfrom gender definition words in the word embedding. Using these properties, we\\nprovide a methodology for modifying an embedding to remove gender stereotypes,\\nsuch as the association between between the words receptionist and female,\\nwhile maintaining desired associations such as between the words queen and\\nfemale. We define metrics to quantify both direct and indirect gender biases in\\nembeddings, and develop algorithms to \"debias\" the embedding. Using\\ncrowd-worker evaluation as well as standard benchmarks, we empirically\\ndemonstrate that our algorithms significantly reduce gender bias in embeddings\\nwhile preserving the its useful properties such as the ability to cluster\\nrelated concepts and to solve analogy tasks. The resulting embeddings can be\\nused in applications without amplifying gender bias.',\n",
              " 'TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency\\nIn this paper, we propose TopicRNN, a recurrent neural network (RNN)-based\\nlanguage model designed to directly capture the global semantic meaning\\nrelating words in a document via latent topics. Because of their sequential\\nnature, RNNs are good at capturing the local structure of a word sequence -\\nboth semantic and syntactic - but might face difficulty remembering long-range\\ndependencies. Intuitively, these long-range dependencies are of semantic\\nnature. In contrast, latent topic models are able to capture the global\\nunderlying semantic structure of a document but do not account for word\\nordering. The proposed TopicRNN model integrates the merits of RNNs and latent\\ntopic models: it captures local (syntactic) dependencies using an RNN and\\nglobal (semantic) dependencies using latent topics. Unlike previous work on\\ncontextual RNN language modeling, our model is learned end-to-end. Empirical\\nresults on word prediction show that TopicRNN outperforms existing contextual\\nRNN baselines. In addition, TopicRNN can be used as an unsupervised feature\\nextractor for documents. We do this for sentiment analysis on the IMDB movie\\nreview dataset and report an error rate of $6.28\\\\%$. This is comparable to the\\nstate-of-the-art $5.91\\\\%$ resulting from a semi-supervised approach. Finally,\\nTopicRNN also yields sensible topics, making it a useful alternative to\\ndocument models such as latent Dirichlet allocation.',\n",
              " 'Gaussian Attention Model and Its Application to Knowledge Base Embedding\\n  and Question Answering\\nWe propose the Gaussian attention model for content-based neural memory\\naccess. With the proposed attention model, a neural network has the additional\\ndegree of freedom to control the focus of its attention from a laser sharp\\nattention to a broad attention. It is applicable whenever we can assume that\\nthe distance in the latent space reflects some notion of semantics. We use the\\nproposed attention model as a scoring function for the embedding of a knowledge\\nbase into a continuous vector space and then train a model that performs\\nquestion answering about the entities in the knowledge base. The proposed\\nattention model can handle both the propagation of uncertainty when following a\\nseries of relations and also the conjunction of conditions in a natural way. On\\na dataset of soccer players who participated in the FIFA World Cup 2014, we\\ndemonstrate that our model can handle both path queries and conjunctive queries\\nwell.',\n",
              " \"Variable Computation in Recurrent Neural Networks\\nRecurrent neural networks (RNNs) have been used extensively and with\\nincreasing success to model various types of sequential data. Much of this\\nprogress has been achieved through devising recurrent units and architectures\\nwith the flexibility to capture complex statistics in the data, such as long\\nrange dependency or localized attention phenomena. However, while many\\nsequential data (such as video, speech or language) can have highly variable\\ninformation flow, most recurrent models still consume input features at a\\nconstant rate and perform a constant number of computations per time step,\\nwhich can be detrimental to both speed and model capacity. In this paper, we\\nexplore a modification to existing recurrent units which allows them to learn\\nto vary the amount of computation they perform at each step, without prior\\nknowledge of the sequence's time structure. We show experimentally that not\\nonly do our models require fewer operations, they also lead to better\\nperformance overall on evaluation tasks.\",\n",
              " 'Learning to Learn from Weak Supervision by Full Supervision\\nIn this paper, we propose a method for training neural networks when we have\\na large set of data with weak labels and a small amount of data with true\\nlabels. In our proposed model, we train two neural networks: a target network,\\nthe learner and a confidence network, the meta-learner. The target network is\\noptimized to perform a given task and is trained using a large set of unlabeled\\ndata that are weakly annotated. We propose to control the magnitude of the\\ngradient updates to the target network using the scores provided by the second\\nconfidence network, which is trained on a small amount of supervised data. Thus\\nwe avoid that the weight updates computed from noisy labels harm the quality of\\nthe target network model.',\n",
              " 'SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for\\n  Predicting Chemical Properties\\nChemical databases store information in text representations, and the SMILES\\nformat is a universal standard used in many cheminformatics software. Encoded\\nin each SMILES string is structural information that can be used to predict\\ncomplex chemical properties. In this work, we develop SMILES2vec, a deep RNN\\nthat automatically learns features from SMILES to predict chemical properties,\\nwithout the need for additional explicit feature engineering. Using Bayesian\\noptimization methods to tune the network architecture, we show that an\\noptimized SMILES2vec model can serve as a general-purpose neural network for\\npredicting distinct chemical properties including toxicity, activity,\\nsolubility and solvation energy, while also outperforming contemporary MLP\\nneural networks that uses engineered features. Furthermore, we demonstrate\\nproof-of-concept of interpretability by developing an explanation mask that\\nlocalizes on the most important characters used in making a prediction. When\\ntested on the solubility dataset, it identified specific parts of a chemical\\nthat is consistent with established first-principles knowledge with an accuracy\\nof 88%. Our work demonstrates that neural networks can learn technically\\naccurate chemical concept and provide state-of-the-art accuracy, making\\ninterpretable deep neural networks a useful tool of relevance to the chemical\\nindustry.',\n",
              " 'Sample Efficient Deep Reinforcement Learning for Dialogue Systems with\\n  Large Action Spaces\\nIn spoken dialogue systems, we aim to deploy artificial intelligence to build\\nautomated dialogue agents that can converse with humans. A part of this effort\\nis the policy optimisation task, which attempts to find a policy describing how\\nto respond to humans, in the form of a function taking the current state of the\\ndialogue and returning the response of the system. In this paper, we\\ninvestigate deep reinforcement learning approaches to solve this problem.\\nParticular attention is given to actor-critic methods, off-policy reinforcement\\nlearning with experience replay, and various methods aimed at reducing the bias\\nand variance of estimators. When combined, these methods result in the\\npreviously proposed ACER algorithm that gave competitive results in gaming\\nenvironments. These environments however are fully observable and have a\\nrelatively small action set so in this paper we examine the application of ACER\\nto dialogue policy optimisation. We show that this method beats the current\\nstate-of-the-art in deep learning approaches for spoken dialogue systems. This\\nnot only leads to a more sample efficient algorithm that can train faster, but\\nalso allows us to apply the algorithm in more difficult environments than\\nbefore. We thus experiment with learning in a very large action space, which\\nhas two orders of magnitude more actions than previously considered. We find\\nthat ACER trains significantly faster than the current state-of-the-art.',\n",
              " 'High-Dimensional Vector Semantics\\nIn this paper we explore the \"vector semantics\" problem from the perspective\\nof \"almost orthogonal\" property of high-dimensional random vectors. We show\\nthat this intriguing property can be used to \"memorize\" random vectors by\\nsimply adding them, and we provide an efficient probabilistic solution to the\\nset membership problem. Also, we discuss several applications to word context\\nvector embeddings, document sentences similarity, and spam filtering.',\n",
              " 'Learning Semantic Script Knowledge with Event Embeddings\\nInduction of common sense knowledge about prototypical sequences of events\\nhas recently received much attention. Instead of inducing this knowledge in the\\nform of graphs, as in much of the previous work, in our method, distributed\\nrepresentations of event realizations are computed based on distributed\\nrepresentations of predicates and their arguments, and then these\\nrepresentations are used to predict prototypical event orderings. The\\nparameters of the compositional process for computing the event representations\\nand the ranking component of the model are jointly estimated from texts. We\\nshow that this approach results in a substantial boost in ordering performance\\nwith respect to previous methods.',\n",
              " 'Mathematical Language Processing: Automatic Grading and Feedback for\\n  Open Response Mathematical Questions\\nWhile computer and communication technologies have provided effective means\\nto scale up many aspects of education, the submission and grading of\\nassessments such as homework assignments and tests remains a weak link. In this\\npaper, we study the problem of automatically grading the kinds of open response\\nmathematical questions that figure prominently in STEM (science, technology,\\nengineering, and mathematics) courses. Our data-driven framework for\\nmathematical language processing (MLP) leverages solution data from a large\\nnumber of learners to evaluate the correctness of their solutions, assign\\npartial-credit scores, and provide feedback to each learner on the likely\\nlocations of any errors. MLP takes inspiration from the success of natural\\nlanguage processing for text data and comprises three main steps. First, we\\nconvert each solution to an open response mathematical question into a series\\nof numerical features. Second, we cluster the features from several solutions\\nto uncover the structures of correct, partially correct, and incorrect\\nsolutions. We develop two different clustering approaches, one that leverages\\ngeneric clustering algorithms and one based on Bayesian nonparametrics. Third,\\nwe automatically grade the remaining (potentially large number of) solutions\\nbased on their assigned cluster and one instructor-provided grade per cluster.\\nAs a bonus, we can track the cluster assignment of each step of a multistep\\nsolution and determine when it departs from a cluster of correct solutions,\\nwhich enables us to indicate the likely locations of errors to learners. We\\ntest and validate MLP on real-world MOOC data to demonstrate how it can\\nsubstantially reduce the human effort required in large-scale educational\\nplatforms.',\n",
              " 'Nonparametric Bayesian Double Articulation Analyzer for Direct Language\\n  Acquisition from Continuous Speech Signals\\nHuman infants can discover words directly from unsegmented speech signals\\nwithout any explicitly labeled data. In this paper, we develop a novel machine\\nlearning method called nonparametric Bayesian double articulation analyzer\\n(NPB-DAA) that can directly acquire language and acoustic models from observed\\ncontinuous speech signals. For this purpose, we propose an integrative\\ngenerative model that combines a language model and an acoustic model into a\\nsingle generative model called the \"hierarchical Dirichlet process hidden\\nlanguage model\" (HDP-HLM). The HDP-HLM is obtained by extending the\\nhierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by\\nJohnson et al. An inference procedure for the HDP-HLM is derived using the\\nblocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure\\nenables the simultaneous and direct inference of language and acoustic models\\nfrom continuous speech signals. Based on the HDP-HLM and its inference\\nprocedure, we developed a novel double articulation analyzer. By assuming\\nHDP-HLM as a generative model of observed time series data, and by inferring\\nlatent variables of the model, the method can analyze latent double\\narticulation structure, i.e., hierarchically organized latent words and\\nphonemes, of the data in an unsupervised manner. The novel unsupervised double\\narticulation analyzer is called NPB-DAA.\\n  The NPB-DAA can automatically estimate double articulation structure embedded\\nin speech signals. We also carried out two evaluation experiments using\\nsynthetic data and actual human continuous speech signals representing Japanese\\nvowel sequences. In the word acquisition and phoneme categorization tasks, the\\nNPB-DAA outperformed a conventional double articulation analyzer (DAA) and\\nbaseline automatic speech recognition system whose acoustic model was trained\\nin a supervised manner.',\n",
              " 'Harnessing Deep Neural Networks with Logic Rules\\nCombining deep neural networks with structured logic rules is desirable to\\nharness flexibility and reduce uninterpretability of the neural models. We\\npropose a general framework capable of enhancing various types of neural\\nnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.\\nSpecifically, we develop an iterative distillation method that transfers the\\nstructured information of logic rules into the weights of neural networks. We\\ndeploy the framework on a CNN for sentiment analysis, and an RNN for named\\nentity recognition. With a few highly intuitive rules, we obtain substantial\\nimprovements and achieve state-of-the-art or comparable results to previous\\nbest-performing systems.',\n",
              " 'Toward Controlled Generation of Text\\nGeneric generation and manipulation of text is challenging and has limited\\nsuccess compared to recent deep generative modeling in visual domain. This\\npaper aims at generating plausible natural language sentences, whose attributes\\nare dynamically controlled by learning disentangled latent representations with\\ndesignated semantics. We propose a new neural generative model which combines\\nvariational auto-encoders and holistic attribute discriminators for effective\\nimposition of semantic structures. With differentiable approximation to\\ndiscrete text samples, explicit constraints on independent attribute controls,\\nand efficient collaborative learning of generator and discriminators, our model\\nlearns highly interpretable representations from even only word annotations,\\nand produces realistic sentences with desired attributes. Quantitative\\nevaluation validates the accuracy of sentence and attribute generation.',\n",
              " 'Adversarial Connective-exploiting Networks for Implicit Discourse\\n  Relation Classification\\nImplicit discourse relation classification is of great challenge due to the\\nlack of connectives as strong linguistic cues, which motivates the use of\\nannotated implicit connectives to improve the recognition. We propose a feature\\nimitation framework in which an implicit relation network is driven to learn\\nfrom another neural network with access to connectives, and thus encouraged to\\nextract similarly salient features for accurate classification. We develop an\\nadversarial model to enable an adaptive imitation scheme through competition\\nbetween the implicit network and a rival feature discriminator. Our method\\neffectively transfers discriminability of connectives to the implicit features,\\nand achieves state-of-the-art performance on the PDTB benchmark.',\n",
              " 'Abstract Syntax Networks for Code Generation and Semantic Parsing\\nTasks like code generation and semantic parsing require mapping unstructured\\n(or partially structured) inputs to well-formed, executable outputs. We\\nintroduce abstract syntax networks, a modeling framework for these problems.\\nThe outputs are represented as abstract syntax trees (ASTs) and constructed by\\na decoder with a dynamically-determined modular structure paralleling the\\nstructure of the output tree. On the benchmark Hearthstone dataset for code\\ngeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,\\ncompared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we\\nperform competitively on the Atis, Jobs, and Geo semantic parsing datasets with\\nno task-specific engineering.',\n",
              " 'Multimodal Word Distributions\\nWord embeddings provide point representations of words containing useful\\nsemantic information. We introduce multimodal word distributions formed from\\nGaussian mixtures, for multiple word meanings, entailment, and rich uncertainty\\ninformation. To learn these distributions, we propose an energy-based\\nmax-margin objective. We show that the resulting approach captures uniquely\\nexpressive semantic information, and outperforms alternatives, such as word2vec\\nskip-grams, and Gaussian embeddings, on benchmark datasets such as word\\nsimilarity and entailment.',\n",
              " 'Guiding Reinforcement Learning Exploration Using Natural Language\\nIn this work we present a technique to use natural language to help\\nreinforcement learning generalize to unseen environments. This technique uses\\nneural machine translation, specifically the use of encoder-decoder networks,\\nto learn associations between natural language behavior descriptions and\\nstate-action information. We then use this learned model to guide agent\\nexploration using a modified version of policy shaping to make it more\\neffective at learning in unseen environments. We evaluate this technique using\\nthe popular arcade game, Frogger, under ideal and non-ideal conditions. This\\nevaluation shows that our modified policy shaping algorithm improves over a\\nQ-learning agent as well as a baseline version of policy shaping.',\n",
              " 'Robust Task Clustering for Deep Many-Task Learning\\nWe investigate task clustering for deep-learning based multi-task and\\nfew-shot learning in a many-task setting. We propose a new method to measure\\ntask similarities with cross-task transfer performance matrix for the deep\\nlearning scenario. Although this matrix provides us critical information\\nregarding similarity between tasks, its asymmetric property and unreliable\\nperformance scores can affect conventional clustering methods adversely.\\nAdditionally, the uncertain task-pairs, i.e., the ones with extremely\\nasymmetric transfer scores, may collectively mislead clustering algorithms to\\noutput an inaccurate task-partition. To overcome these limitations, we propose\\na novel task-clustering algorithm by using the matrix completion technique. The\\nproposed algorithm constructs a partially-observed similarity matrix based on\\nthe certainty of cluster membership of the task-pairs. We then use a matrix\\ncompletion algorithm to complete the similarity matrix. Our theoretical\\nanalysis shows that under mild constraints, the proposed algorithm will\\nperfectly recover the underlying \"true\" similarity matrix with a high\\nprobability. Our results show that the new task clustering method can discover\\ntask clusters for training flexible and superior neural network models in a\\nmulti-task learning setup for sentiment classification and dialog intent\\nclassification tasks. Our task clustering approach also extends metric-based\\nfew-shot learning methods to adapt multiple metrics, which demonstrates\\nempirical advantages when the tasks are diverse.',\n",
              " 'Natural Language Multitasking: Analyzing and Improving Syntactic\\n  Saliency of Hidden Representations\\nWe train multi-task autoencoders on linguistic tasks and analyze the learned\\nhidden sentence representations. The representations change significantly when\\ntranslation and part-of-speech decoders are added. The more decoders a model\\nemploys, the better it clusters sentences according to their syntactic\\nsimilarity, as the representation space becomes less entangled. We explore the\\nstructure of the representation space by interpolating between sentences, which\\nyields interesting pseudo-English sentences, many of which have recognizable\\nsyntactic structure. Lastly, we point out an interesting property of our\\nmodels: The difference-vector between two sentences can be added to change a\\nthird sentence with similar features in a meaningful way.',\n",
              " 'Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement\\n  Learning\\nWith the increasing popularity of video sharing websites such as YouTube and\\nFacebook, multimodal sentiment analysis has received increasing attention from\\nthe scientific community. Contrary to previous works in multimodal sentiment\\nanalysis which focus on holistic information in speech segments such as bag of\\nwords representations and average facial expression intensity, we develop a\\nnovel deep architecture for multimodal sentiment analysis that performs\\nmodality fusion at the word level. In this paper, we propose the Gated\\nMultimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is\\ncomposed of 2 modules. The Gated Multimodal Embedding alleviates the\\ndifficulties of fusion when there are noisy modalities. The LSTM with Temporal\\nAttention performs word level fusion at a finer fusion resolution between input\\nmodalities and attends to the most important time steps. As a result, the\\nGME-LSTM(A) is able to better model the multimodal structure of speech through\\ntime and perform better sentiment comprehension. We demonstrate the\\neffectiveness of this approach on the publicly-available Multimodal Corpus of\\nSentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving\\nstate-of-the-art sentiment classification and regression results. Qualitative\\nanalysis on our model emphasizes the importance of the Temporal Attention Layer\\nin sentiment prediction because the additional acoustic and visual modalities\\nare noisy. We also demonstrate the effectiveness of the Gated Multimodal\\nEmbedding in selectively filtering these noisy modalities out. Our results and\\nanalysis open new areas in the study of sentiment analysis in human\\ncommunication and provide new models for multimodal fusion.',\n",
              " 'A Supervised Approach to Extractive Summarisation of Scientific Papers\\nAutomatic summarisation is a popular approach to reduce a document to its\\nmain arguments. Recent research in the area has focused on neural approaches to\\nsummarisation, which can be very data-hungry. However, few large datasets exist\\nand none for the traditionally popular domain of scientific publications, which\\nopens up challenging research avenues centered on encoding large, complex\\ndocuments. In this paper, we introduce a new dataset for summarisation of\\ncomputer science publications by exploiting a large resource of author provided\\nsummaries and show straightforward ways of extending it further. We develop\\nmodels on the dataset making use of both neural sentence encoding and\\ntraditionally used summarisation features and show that models which encode\\nsentences as well as their local and global context perform best, significantly\\noutperforming well-established baseline methods.',\n",
              " 'Language Models for Image Captioning: The Quirks and What Works\\nTwo recent approaches have achieved state-of-the-art results in image\\ncaptioning. The first uses a pipelined process where a set of candidate words\\nis generated by a convolutional neural network (CNN) trained on images, and\\nthen a maximum entropy (ME) language model is used to arrange these words into\\na coherent sentence. The second uses the penultimate activation layer of the\\nCNN as input to a recurrent neural network (RNN) that then generates the\\ncaption sequence. In this paper, we compare the merits of these different\\nlanguage modeling approaches for the first time by using the same\\nstate-of-the-art CNN as input. We examine issues in the different approaches,\\nincluding linguistic irregularities, caption repetition, and data set overlap.\\nBy combining key aspects of the ME and RNN methods, we achieve a new record\\nperformance over previously published results on the benchmark COCO dataset.\\nHowever, the gains we see in BLEU do not translate to human judgments.',\n",
              " 'Exploring Models and Data for Image Question Answering\\nThis work aims to address the problem of image-based question-answering (QA)\\nwith new models and datasets. In our work, we propose to use neural networks\\nand visual semantic embeddings, without intermediate stages such as object\\ndetection and image segmentation, to predict answers to simple questions about\\nimages. Our model performs 1.8 times better than the only published results on\\nan existing image QA dataset. We also present a question generation algorithm\\nthat converts image descriptions, which are widely available, into QA form. We\\nused this algorithm to produce an order-of-magnitude larger dataset, with more\\nevenly distributed answers. A suite of baseline results on this new dataset are\\nalso presented.',\n",
              " 'Making the V in VQA Matter: Elevating the Role of Image Understanding in\\n  Visual Question Answering\\nProblems at the intersection of vision and language are of significant\\nimportance both as challenging research questions and for the rich set of\\napplications they enable. However, inherent structure in our world and bias in\\nour language tend to be a simpler signal for learning than visual modalities,\\nresulting in models that ignore visual information, leading to an inflated\\nsense of their capability.\\n  We propose to counter these language priors for the task of Visual Question\\nAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balance\\nthe popular VQA dataset by collecting complementary images such that every\\nquestion in our balanced dataset is associated with not just a single image,\\nbut rather a pair of similar images that result in two different answers to the\\nquestion. Our dataset is by construction more balanced than the original VQA\\ndataset and has approximately twice the number of image-question pairs. Our\\ncomplete balanced dataset is publicly available at www.visualqa.org as part of\\nthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA\\nv2.0).\\n  We further benchmark a number of state-of-art VQA models on our balanced\\ndataset. All models perform significantly worse on our balanced dataset,\\nsuggesting that these models have indeed learned to exploit language priors.\\nThis finding provides the first concrete empirical evidence for what seems to\\nbe a qualitative sense among practitioners.\\n  Finally, our data collection protocol for identifying complementary images\\nenables us to develop a novel interpretable model, which in addition to\\nproviding an answer to the given (image, question) pair, also provides a\\ncounter-example based explanation. Specifically, it identifies an image that is\\nsimilar to the original image, but it believes has a different answer to the\\nsame question. This can help in building trust for machines among their users.',\n",
              " 'A Multi-World Approach to Question Answering about Real-World Scenes\\n  based on Uncertain Input\\nWe propose a method for automatically answering questions about images by\\nbringing together recent advances from natural language processing and computer\\nvision. We combine discrete reasoning with uncertain predictions by a\\nmulti-world approach that represents uncertainty about the perceived world in a\\nbayesian framework. Our approach can handle human questions of high complexity\\nabout realistic scenes and replies with range of answer like counts, object\\nclasses, instances and lists of them. The system is directly trained from\\nquestion-answer pairs. We establish a first benchmark for this task that can be\\nseen as a modern attempt at a visual turing test.',\n",
              " 'Hard to Cheat: A Turing Test based on Answering Questions about Images\\nProgress in language and image understanding by machines has sparkled the\\ninterest of the research community in more open-ended, holistic tasks, and\\nrefueled an old AI dream of building intelligent machines. We discuss a few\\nprominent challenges that characterize such holistic tasks and argue for\\n\"question answering about images\" as a particular appealing instance of such a\\nholistic task. In particular, we point out that it is a version of a Turing\\nTest that is likely to be more robust to over-interpretations and contrast it\\nwith tasks like grounding and generation of descriptions. Finally, we discuss\\ntools to measure progress in this field.',\n",
              " 'Analyzing the Behavior of Visual Question Answering Models\\nRecently, a number of deep-learning based models have been proposed for the\\ntask of Visual Question Answering (VQA). The performance of most models is\\nclustered around 60-70%. In this paper we propose systematic methods to analyze\\nthe behavior of these models as a first step towards recognizing their\\nstrengths and weaknesses, and identifying the most fruitful directions for\\nprogress. We analyze two models, one each from two major classes of VQA models\\n-- with-attention and without-attention and show the similarities and\\ndifferences in the behavior of these models. We also analyze the winning entry\\nof the VQA Challenge 2016.\\n  Our behavior analysis reveals that despite recent progress, today\\'s VQA\\nmodels are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump\\nto conclusions\" (converge on a predicted answer after \\'listening\\' to just half\\nthe question), and are \"stubborn\" (do not change their answers across images).',\n",
              " 'Sort Story: Sorting Jumbled Images and Captions into Stories\\nTemporal common sense has applications in AI tasks such as QA, multi-document\\nsummarization, and human-AI communication. We propose the task of sequencing --\\ngiven a jumbled set of aligned image-caption pairs that belong to a story, the\\ntask is to sort them such that the output sequence forms a coherent story. We\\npresent multiple approaches, via unary (position) and pairwise (order)\\npredictions, and their ensemble-based combinations, achieving strong results on\\nthis task. We use both text-based and image-based features, which depict\\ncomplementary improvements. Using qualitative examples, we demonstrate that our\\nmodels have learnt interesting aspects of temporal common sense.',\n",
              " \"Mean Box Pooling: A Rich Image Representation and Output Embedding for\\n  the Visual Madlibs Task\\nWe present Mean Box Pooling, a novel visual representation that pools over\\nCNN representations of a large number, highly overlapping object proposals. We\\nshow that such representation together with nCCA, a successful multimodal\\nembedding technique, achieves state-of-the-art performance on the Visual\\nMadlibs task. Moreover, inspired by the nCCA's objective function, we extend\\nclassical CNN+LSTM approach to train the network by directly maximizing the\\nsimilarity between the internal representation of the deep learning\\narchitecture and candidate answers. Again, such approach achieves a significant\\nimprovement over the prior work that also uses CNN+LSTM approach on Visual\\nMadlibs.\",\n",
              " 'Learning to generalize to new compositions in image understanding\\nRecurrent neural networks have recently been used for learning to describe\\nimages using natural language. However, it has been observed that these models\\ngeneralize poorly to scenes that were not observed during training, possibly\\ndepending too strongly on the statistics of the text in the training data. Here\\nwe propose to describe images using short structured representations, aiming to\\ncapture the crux of a description. These structured representations allow us to\\ntease-out and evaluate separately two types of generalization: standard\\ngeneralization to new images with similar scenes, and generalization to new\\ncombinations of known entities. We compare two learning approaches on the\\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\\nAttend and Tell), and a simple structured prediction model on top of a deep\\nnetwork. We find that the structured model generalizes to new compositions\\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\\nstructured representations. By providing a concrete method to quantify\\ngeneralization for unseen combinations, we argue that structured\\nrepresentations and compositional splits are a useful benchmark for image\\ncaptioning, and advocate compositional models that capture linguistic and\\nvisual structure.',\n",
              " \"Measuring Machine Intelligence Through Visual Question Answering\\nAs machines have become more intelligent, there has been a renewed interest\\nin methods for measuring their intelligence. A common approach is to propose\\ntasks for which a human excels, but one which machines find difficult. However,\\nan ideal task should also be easy to evaluate and not be easily gameable. We\\nbegin with a case study exploring the recently popular task of image captioning\\nand its limitations as a task for measuring machine intelligence. An\\nalternative and more promising task is Visual Question Answering that tests a\\nmachine's ability to reason about language and vision. We describe a dataset\\nunprecedented in size created for the task that contains over 760,000 human\\ngenerated questions about images. Using around 10 million human generated\\nanswers, machines may be easily evaluated.\",\n",
              " 'Towards Transparent AI Systems: Interpreting Visual Question Answering\\n  Models\\nDeep neural networks have shown striking progress and obtained\\nstate-of-the-art results in many AI research fields in the recent years.\\nHowever, it is often unsatisfying to not know why they predict what they do. In\\nthis paper, we address the problem of interpreting Visual Question Answering\\n(VQA) models. Specifically, we are interested in finding what part of the input\\n(pixels in images or words in questions) the VQA model focuses on while\\nanswering the question. To tackle this problem, we use two visualization\\ntechniques -- guided backpropagation and occlusion -- to find important words\\nin the question and important regions in the image. We then present qualitative\\nand quantitative analyses of these importance maps. We found that even without\\nexplicit attention mechanisms, VQA models may sometimes be implicitly attending\\nto relevant regions in the image, and often to appropriate words in the\\nquestion.',\n",
              " \"Visual Dialog\\nWe introduce the task of Visual Dialog, which requires an AI agent to hold a\\nmeaningful dialog with humans in natural, conversational language about visual\\ncontent. Specifically, given an image, a dialog history, and a question about\\nthe image, the agent has to ground the question in image, infer context from\\nhistory, and answer the question accurately. Visual Dialog is disentangled\\nenough from a specific downstream task so as to serve as a general test of\\nmachine intelligence, while being grounded in vision enough to allow objective\\nevaluation of individual responses and benchmark progress. We develop a novel\\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\\nquestion-answer pairs.\\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\\nand 2 decoders (generative and discriminative), which outperform a number of\\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\\nquantify gap between machine and human performance on the Visual Dialog task\\nvia human studies. Putting it all together, we demonstrate the first 'visual\\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\\nhttps://visualdialog.org\",\n",
              " 'Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic\\n  Speech Recognition\\nMulti-task learning (MTL) involves the simultaneous training of two or more\\nrelated tasks over shared representations. In this work, we apply MTL to\\naudio-visual automatic speech recognition(AV-ASR). Our primary task is to learn\\na mapping between audio-visual fused features and frame labels obtained from\\nacoustic GMM/HMM model. This is combined with an auxiliary task which maps\\nvisual features to frame labels obtained from a separate visual GMM/HMM model.\\nThe MTL model is tested at various levels of babble noise and the results are\\ncompared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate\\nthat MTL is especially useful at higher level of noise. Compared to base-line,\\nupto 7\\\\% relative improvement in WER is reported at -3 SNR dB',\n",
              " \"Learning Cooperative Visual Dialog Agents with Deep Reinforcement\\n  Learning\\nWe introduce the first goal-driven training for visual question answering and\\ndialog agents. Specifically, we pose a cooperative 'image guessing' game\\nbetween two agents -- Qbot and Abot -- who communicate in natural language\\ndialog so that Qbot can select an unseen image from a lineup of images. We use\\ndeep reinforcement learning (RL) to learn the policies of these agents\\nend-to-end -- from pixels to multi-agent multi-round dialog to game reward.\\n  We demonstrate two experimental results.\\n  First, as a 'sanity check' demonstration of pure RL (from scratch), we show\\nresults on a synthetic world, where the agents communicate in ungrounded\\nvocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find\\nthat two bots invent their own communication protocol and start using certain\\nsymbols to ask/answer about certain visual attributes (shape/color/style).\\nThus, we demonstrate the emergence of grounded language and communication among\\n'visual' dialog agents with no human supervision.\\n  Second, we conduct large-scale real-image experiments on the VisDial dataset,\\nwhere we pretrain with supervised dialog data and show that the RL 'fine-tuned'\\nagents significantly outperform SL agents. Interestingly, the RL Qbot learns to\\nask questions that Abot is good at, ultimately resulting in more informative\\ndialog and a better team.\",\n",
              " 'Being Negative but Constructively: Lessons Learnt from Creating Better\\n  Visual Question Answering Datasets\\nVisual question answering (QA) has attracted a lot of attention lately, seen\\nessentially as a form of (visual) Turing test that artificial intelligence\\nshould strive to achieve. In this paper, we study a crucial component of this\\ntask: how can we design good datasets for the task? We focus on the design of\\nmultiple-choice based datasets where the learner has to select the right answer\\nfrom a set of candidate ones including the target (i.e. the correct one) and\\nthe decoys (i.e. the incorrect ones). Through careful analysis of the results\\nattained by state-of-the-art learning models and human annotators on existing\\ndatasets, we show the design of the decoy answers has a significant impact on\\nhow and what the learning models learn from the datasets. In particular, the\\nresulting learner can ignore the visual information, the question, or the both\\nwhile still doing well on the task. Inspired by this, we propose automatic\\nprocedures to remedy such design deficiencies. We apply the procedures to\\nre-construct decoy answers for two popular visual QA datasets as well as to\\ncreate a new visual QA dataset from the Visual Genome project, resulting in the\\nlargest dataset for this task. Extensive empirical studies show that the design\\ndeficiencies have been alleviated in the remedied datasets and the performance\\non them is likely a more faithful indicator of the difference among learning\\nmodels. The datasets are released and publicly available via\\nhttp://www.teds.usc.edu/website_vqa/.',\n",
              " 'C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0\\n  Dataset\\nVisual Question Answering (VQA) has received a lot of attention over the past\\ncouple of years. A number of deep learning models have been proposed for this\\ntask. However, it has been shown that these models are heavily driven by\\nsuperficial correlations in the training data and lack compositionality -- the\\nability to answer questions about unseen compositions of seen concepts. This\\ncompositionality is desirable and central to intelligence. In this paper, we\\npropose a new setting for Visual Question Answering where the test\\nquestion-answer pairs are compositionally novel compared to training\\nquestion-answer pairs. To facilitate developing models under this setting, we\\npresent a new compositional split of the VQA v1.0 dataset, which we call\\nCompositional VQA (C-VQA). We analyze the distribution of questions and answers\\nin the C-VQA splits. Finally, we evaluate several existing VQA models under\\nthis new setting and show that the performances of these models degrade by a\\nsignificant amount compared to the original VQA setting.',\n",
              " \"Deep learning evaluation using deep linguistic processing\\nWe discuss problems with the standard approaches to evaluation for tasks like\\nvisual question answering, and argue that artificial data can be used to\\naddress these as a complement to current practice. We demonstrate that with the\\nhelp of existing 'deep' linguistic processing technology we are able to create\\nchallenging abstract datasets, which enable us to investigate the language\\nunderstanding abilities of multimodal deep learning models in detail.\",\n",
              " 'meProp: Sparsified Back Propagation for Accelerated Deep Learning with\\n  Reduced Overfitting\\nWe propose a simple yet effective technique for neural network learning. The\\nforward propagation is computed as usual. In back propagation, only a small\\nsubset of the full gradient is computed to update the model parameters. The\\ngradient vectors are sparsified in such a way that only the top-$k$ elements\\n(in terms of magnitude) are kept. As a result, only $k$ rows or columns\\n(depending on the layout) of the weight matrix are modified, leading to a\\nlinear reduction ($k$ divided by the vector dimension) in the computational\\ncost. Surprisingly, experimental results demonstrate that we can update only\\n1--4\\\\% of the weights at each back propagation pass. This does not result in a\\nlarger number of training iterations. More interestingly, the accuracy of the\\nresulting models is actually improved rather than degraded, and a detailed\\nanalysis is given. The code is available at https://github.com/jklj077/meProp',\n",
              " 'Towards Crafting Text Adversarial Samples\\nAdversarial samples are strategically modified samples, which are crafted\\nwith the purpose of fooling a classifier at hand. An attacker introduces\\nspecially crafted adversarial samples to a deployed classifier, which are being\\nmis-classified by the classifier. However, the samples are perceived to be\\ndrawn from entirely different classes and thus it becomes hard to detect the\\nadversarial samples. Most of the prior works have been focused on synthesizing\\nadversarial samples in the image domain. In this paper, we propose a new method\\nof crafting adversarial text samples by modification of the original samples.\\nModifications of the original text samples are done by deleting or replacing\\nthe important or salient words in the text or by introducing new words in the\\ntext sample. Our algorithm works best for the datasets which have\\nsub-categories within each of the classes of examples. While crafting\\nadversarial samples, one of the key constraint is to generate meaningful\\nsentences which can at pass off as legitimate from language (English)\\nviewpoint. Experimental results on IMDB movie review dataset for sentiment\\nanalysis and Twitter dataset for gender detection show the efficiency of our\\nproposed method.',\n",
              " 'Reinforced Video Captioning with Entailment Rewards\\nSequence-to-sequence models have shown promising improvements on the temporal\\ntask of video captioning, but they optimize word-level cross-entropy loss\\nduring training. First, using policy gradient and mixed-loss methods for\\nreinforcement learning, we directly optimize sentence-level task-based metrics\\n(as rewards), achieving significant improvements over the baseline, based on\\nboth automatic metrics and human evaluation on multiple datasets. Next, we\\npropose a novel entailment-enhanced reward (CIDEnt) that corrects\\nphrase-matching based metrics (such as CIDEr) to only allow for\\nlogically-implied partial matches and avoid contradictions, achieving further\\nsignificant improvements over the CIDEr-reward model. Overall, our\\nCIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.',\n",
              " 'Hierarchically-Attentive RNN for Album Summarization and Storytelling\\nWe address the problem of end-to-end visual storytelling. Given a photo\\nalbum, our model first selects the most representative (summary) photos, and\\nthen composes a natural language story for the album. For this task, we make\\nuse of the Visual Storytelling dataset and a model composed of three\\nhierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album\\nphotos, select representative (summary) photos, and compose the story.\\nAutomatic and human evaluations show our model achieves better performance on\\nselection, generation, and retrieval than baselines.',\n",
              " 'Generating Natural Adversarial Examples\\nDue to their complex nature, it is hard to characterize the ways in which\\nmachine learning models can misbehave or be exploited when deployed. Recent\\nwork on adversarial examples, i.e. inputs with minor perturbations that result\\nin substantially different model predictions, is helpful in evaluating the\\nrobustness of these models by exposing the adversarial scenarios where they\\nfail. However, these malicious perturbations are often unnatural, not\\nsemantically meaningful, and not applicable to complicated domains such as\\nlanguage. In this paper, we propose a framework to generate natural and legible\\nadversarial examples that lie on the data manifold, by searching in semantic\\nspace of dense and continuous data representation, utilizing the recent\\nadvances in generative adversarial networks. We present generated adversaries\\nto demonstrate the potential of the proposed approach for black-box classifiers\\nfor a wide range of applications such as image classification, textual\\nentailment, and machine translation. We include experiments to show that the\\ngenerated adversaries are natural, legible to humans, and useful in evaluating\\nand analyzing black-box classifiers.',\n",
              " 'Training Simplification and Model Simplification for Deep Learning: A\\n  Minimal Effort Back Propagation Method\\nWe propose a simple yet effective technique to simplify the training and the\\nresulting model of neural networks. In back propagation, only a small subset of\\nthe full gradient is computed to update the model parameters. The gradient\\nvectors are sparsified in such a way that only the top-$k$ elements (in terms\\nof magnitude) are kept. As a result, only $k$ rows or columns (depending on the\\nlayout) of the weight matrix are modified, leading to a linear reduction in the\\ncomputational cost. Based on the sparsified gradients, we further simplify the\\nmodel by eliminating the rows or columns that are seldom updated, which will\\nreduce the computational cost both in the training and decoding, and\\npotentially accelerate decoding in real-world applications. Surprisingly,\\nexperimental results demonstrate that most of time we only need to update fewer\\nthan 5% of the weights at each back propagation pass. More interestingly, the\\naccuracy of the resulting models is actually improved rather than degraded, and\\na detailed analysis is given. The model simplification results show that we\\ncould adaptively simplify the model which could often be reduced by around 9x,\\nwithout any loss on accuracy or even with improved accuracy.',\n",
              " 'Embodied Question Answering\\nWe present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where\\nan agent is spawned at a random location in a 3D environment and asked a\\nquestion (\"What color is the car?\"). In order to answer, the agent must first\\nintelligently navigate to explore the environment, gather information through\\nfirst-person (egocentric) vision, and then answer the question (\"orange\").\\n  This challenging task requires a range of AI skills -- active perception,\\nlanguage understanding, goal-driven navigation, commonsense reasoning, and\\ngrounding of language into actions. In this work, we develop the environments,\\nend-to-end-trained reinforcement learning agents, and evaluation protocols for\\nEmbodiedQA.',\n",
              " \"Don't Just Assume; Look and Answer: Overcoming Priors for Visual\\n  Question Answering\\nA number of studies have found that today's Visual Question Answering (VQA)\\nmodels are heavily driven by superficial correlations in the training data and\\nlack sufficient image grounding. To encourage development of models geared\\ntowards the latter, we propose a new setting for VQA where for every question\\ntype, train and test sets have different prior distributions of answers.\\nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we\\ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2\\nrespectively). First, we evaluate several existing VQA models under this new\\nsetting and show that their performance degrades significantly compared to the\\noriginal VQA setting. Second, we propose a novel Grounded Visual Question\\nAnswering model (GVQA) that contains inductive biases and restrictions in the\\narchitecture specifically designed to prevent the model from 'cheating' by\\nprimarily relying on priors in the training data. Specifically, GVQA explicitly\\ndisentangles the recognition of visual concepts present in the image from the\\nidentification of plausible answer space for a given question, enabling the\\nmodel to more robustly generalize across different distributions of answers.\\nGVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).\\nOur experiments demonstrate that GVQA significantly outperforms SAN on both\\nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more\\npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in\\nseveral cases. GVQA offers strengths complementary to SAN when trained and\\nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more\\ntransparent and interpretable than existing VQA models.\",\n",
              " \"CoDraw: Visual Dialog for Collaborative Drawing\\nIn this work, we propose a goal-driven collaborative task that contains\\nvision, language, and action in a virtual environment as its core components.\\nSpecifically, we develop a collaborative `Image Drawing' game between two\\nagents, called CoDraw. Our game is grounded in a virtual world that contains\\nmovable clip art objects. Two players, Teller and Drawer, are involved. The\\nTeller sees an abstract scene containing multiple clip arts in a semantically\\nmeaningful configuration, while the Drawer tries to reconstruct the scene on an\\nempty canvas using available clip arts. The two players communicate via two-way\\ncommunication using natural language. We collect the CoDraw dataset of ~10K\\ndialogs consisting of 138K messages exchanged between a Teller and a Drawer\\nfrom Amazon Mechanical Turk (AMT). We analyze our dataset and present three\\nmodels to model the players' behaviors, including an attention model to\\ndescribe and draw multiple clip arts at each round. The attention models are\\nquantitatively compared to the other models to show how the conventional\\napproaches work for this new task. We also present qualitative visualizations.\",\n",
              " 'Answerer in Questioner\\'s Mind for Goal-Oriented Visual Dialogue\\nGoal-oriented dialogue has been paid attention for its numerous applications\\nin artificial intelligence. To solve this task, deep learning and reinforcement\\nlearning have recently been applied. However, these approaches struggle to find\\na competent recurrent neural questioner, owing to the complexity of learning a\\nseries of sentences. Motivated by theory of mind, we propose \"Answerer in\\nQuestioner\\'s Mind\" (AQM), a novel algorithm for goal-oriented dialogue. With\\nAQM, a questioner asks and infers based on an approximated probabilistic model\\nof the answerer. The questioner figures out the answerer\\'s intent via selecting\\na plausible question by explicitly calculating the information gain of the\\ncandidate intentions and possible answers to each question. We test our\\nframework on two goal-oriented visual dialogue tasks: \"MNIST Counting Dialog\"\\nand \"GuessWhat?!.\" In our experiments, AQM outperforms comparative algorithms\\nand makes human-like dialogue. We further use AQM as a tool for analyzing the\\nmechanism of deep reinforcement learning approach and discuss the future\\ndirection of practical goal-oriented neural dialogue systems.',\n",
              " 'Resource Constrained Structured Prediction\\nWe study the problem of structured prediction under test-time budget\\nconstraints. We propose a novel approach applicable to a wide range of\\nstructured prediction problems in computer vision and natural language\\nprocessing. Our approach seeks to adaptively generate computationally costly\\nfeatures during test-time in order to reduce the computational cost of\\nprediction while maintaining prediction performance. We show that training the\\nadaptive feature generation system can be reduced to a series of structured\\nlearning problems, resulting in efficient training using existing structured\\nlearning algorithms. This framework provides theoretical justification for\\nseveral existing heuristic approaches found in literature. We evaluate our\\nproposed adaptive system on two structured prediction tasks, optical character\\nrecognition (OCR) and dependency parsing and show strong performance in\\nreduction of the feature costs without degrading accuracy.',\n",
              " 'Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to\\n  Action Sequences\\nWe propose a neural sequence-to-sequence model for direction following, a\\ntask that is essential to realizing effective autonomous agents. Our\\nalignment-based encoder-decoder model with long short-term memory recurrent\\nneural networks (LSTM-RNN) translates natural language instructions to action\\nsequences based upon a representation of the observable world state. We\\nintroduce a multi-level aligner that empowers our model to focus on sentence\\n\"regions\" salient to the current world state by using multiple abstractions of\\nthe input sentence. In contrast to existing methods, our model uses no\\nspecialized linguistic resources (e.g., parsers) or task-specific annotations\\n(e.g., seed lexicons). It is therefore generalizable, yet still achieves the\\nbest results reported to-date on a benchmark single-sentence dataset and\\ncompetitive results for the limited-training multi-sentence setting. We analyze\\nour model through a series of ablations that elucidate the contributions of the\\nprimary components of our model.',\n",
              " \"Coupling Distributed and Symbolic Execution for Natural Language Queries\\nBuilding neural networks to query a knowledge base (a table) with natural\\nlanguage is an emerging research topic in deep learning. An executor for table\\nquerying typically requires multiple steps of execution because queries may\\nhave complicated structures. In previous studies, researchers have developed\\neither fully distributed executors or symbolic executors for table querying. A\\ndistributed executor can be trained in an end-to-end fashion, but is weak in\\nterms of execution efficiency and explicit interpretability. A symbolic\\nexecutor is efficient in execution, but is very difficult to train especially\\nat initial stages. In this paper, we propose to couple distributed and symbolic\\nexecution for natural language queries, where the symbolic executor is\\npretrained with the distributed executor's intermediate execution results in a\\nstep-by-step fashion. Experiments show that our approach significantly\\noutperforms both distributed and symbolic executors, exhibiting high accuracy,\\nhigh learning efficiency, high execution efficiency, and high interpretability.\",\n",
              " 'An agent-driven semantical identifier using radial basis neural networks\\n  and reinforcement learning\\nDue to the huge availability of documents in digital form, and the deception\\npossibility raise bound to the essence of digital documents and the way they\\nare spread, the authorship attribution problem has constantly increased its\\nrelevance. Nowadays, authorship attribution,for both information retrieval and\\nanalysis, has gained great importance in the context of security, trust and\\ncopyright preservation. This work proposes an innovative multi-agent driven\\nmachine learning technique that has been developed for authorship attribution.\\nBy means of a preprocessing for word-grouping and time-period related analysis\\nof the common lexicon, we determine a bias reference level for the recurrence\\nfrequency of the words within analysed texts, and then train a Radial Basis\\nNeural Networks (RBPNN)-based classifier to identify the correct author. The\\nmain advantage of the proposed approach lies in the generality of the semantic\\nanalysis, which can be applied to different contexts and lexical domains,\\nwithout requiring any modification. Moreover, the proposed system is able to\\nincorporate an external input, meant to tune the classifier, and then\\nself-adjust by means of continuous learning reinforcement.',\n",
              " 'Where is my forearm? Clustering of body parts from simultaneous tactile\\n  and linguistic input using sequential mapping\\nHumans and animals are constantly exposed to a continuous stream of sensory\\ninformation from different modalities. At the same time, they form more\\ncompressed representations like concepts or symbols. In species that use\\nlanguage, this process is further structured by this interaction, where a\\nmapping between the sensorimotor concepts and linguistic elements needs to be\\nestablished. There is evidence that children might be learning language by\\nsimply disambiguating potential meanings based on multiple exposures to\\nutterances in different contexts (cross-situational learning). In existing\\nmodels, the mapping between modalities is usually found in a single step by\\ndirectly using frequencies of referent and meaning co-occurrences. In this\\npaper, we present an extension of this one-step mapping and introduce a newly\\nproposed sequential mapping algorithm together with a publicly available Matlab\\nimplementation. For demonstration, we have chosen a less typical scenario:\\ninstead of learning to associate objects with their names, we focus on body\\nrepresentations. A humanoid robot is receiving tactile stimulations on its\\nbody, while at the same time listening to utterances of the body part names\\n(e.g., hand, forearm and torso). With the goal at arriving at the correct \"body\\ncategories\", we demonstrate how a sequential mapping algorithm outperforms\\none-step mapping. In addition, the effect of data set size and noise in the\\nlinguistic input are studied.',\n",
              " 'Improvements to deep convolutional neural networks for LVCSR\\nDeep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural\\nNetworks (DNN), as they are able to better reduce spectral variation in the\\ninput signal. This has also been confirmed experimentally, with CNNs showing\\nimprovements in word error rate (WER) between 4-12% relative compared to DNNs\\nacross a variety of LVCSR tasks. In this paper, we describe different methods\\nto further improve CNN performance. First, we conduct a deep analysis comparing\\nlimited weight sharing and full weight sharing with state-of-the-art features.\\nSecond, we apply various pooling strategies that have shown improvements in\\ncomputer vision to an LVCSR speech task. Third, we introduce a method to\\neffectively incorporate speaker adaptation, namely fMLLR, into log-mel\\nfeatures. Fourth, we introduce an effective strategy to use dropout during\\nHessian-free sequence training. We find that with these improvements,\\nparticularly with fMLLR and dropout, we are able to achieve an additional 2-3%\\nrelative improvement in WER on a 50-hour Broadcast News task over our previous\\nbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%\\nrelative improvement over our previous best CNN baseline.',\n",
              " 'Collaborative Deep Learning for Recommender Systems\\nCollaborative filtering (CF) is a successful approach commonly used by many\\nrecommender systems. Conventional CF-based methods use the ratings given to\\nitems by users as the sole source of information for learning to make\\nrecommendation. However, the ratings are often very sparse in many\\napplications, causing CF-based methods to degrade significantly in their\\nrecommendation performance. To address this sparsity problem, auxiliary\\ninformation such as item content information may be utilized. Collaborative\\ntopic regression (CTR) is an appealing recent method taking this approach which\\ntightly couples the two components that learn from two different sources of\\ninformation. Nevertheless, the latent representation learned by CTR may not be\\nvery effective when the auxiliary information is very sparse. To address this\\nproblem, we generalize recent advances in deep learning from i.i.d. input to\\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\\nmodel called collaborative deep learning (CDL), which jointly performs deep\\nrepresentation learning for the content information and collaborative filtering\\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\\ndatasets from different domains show that CDL can significantly advance the\\nstate of the art.',\n",
              " 'Explaining Predictions of Non-Linear Classifiers in NLP\\nLayer-wise relevance propagation (LRP) is a recently proposed technique for\\nexplaining predictions of complex non-linear classifiers in terms of input\\nvariables. In this paper, we apply LRP for the first time to natural language\\nprocessing (NLP). More precisely, we use it to explain the predictions of a\\nconvolutional neural network (CNN) trained on a topic categorization task. Our\\nanalysis highlights which words are relevant for a specific prediction of the\\nCNN. We compare our technique to standard sensitivity analysis, both\\nqualitatively and quantitatively, using a \"word deleting\" perturbation\\nexperiment, a PCA analysis, and various visualizations. All experiments\\nvalidate the suitability of LRP for explaining the CNN predictions, which is\\nalso in line with results reported in recent image classification studies.',\n",
              " 'Tensor network language model\\nWe propose a new statistical model suitable for machine learning of systems\\nwith long distance correlations such as natural languages. The model is based\\non directed acyclic graph decorated by multi-linear tensor maps in the vertices\\nand vector spaces in the edges, called tensor network. Such tensor networks\\nhave been previously employed for effective numerical computation of the\\nrenormalization group flow on the space of effective quantum field theories and\\nlattice models of statistical mechanics. We provide explicit algebro-geometric\\nanalysis of the parameter moduli space for tree graphs, discuss model\\nproperties and applications such as statistical translation.',\n",
              " 'Language as a matrix product state\\nWe propose a statistical model for natural language that begins by\\nconsidering language as a monoid, then representing it in complex matrices with\\na compatible translation invariant probability measure. We interpret the\\nprobability measure as arising via the Born rule from a translation invariant\\nmatrix product state.',\n",
              " 'Accelerating Hessian-free optimization for deep neural networks by\\n  implicit preconditioning and sampling\\nHessian-free training has become a popular parallel second or- der\\noptimization technique for Deep Neural Network training. This study aims at\\nspeeding up Hessian-free training, both by means of decreasing the amount of\\ndata used for training, as well as through reduction of the number of Krylov\\nsubspace solver iterations used for implicit estimation of the Hessian. In this\\npaper, we develop an L-BFGS based preconditioning scheme that avoids the need\\nto access the Hessian explicitly. Since L-BFGS cannot be regarded as a\\nfixed-point iteration, we further propose the employment of flexible Krylov\\nsubspace solvers that retain the desired theoretical convergence guarantees of\\ntheir conventional counterparts. Second, we propose a new sampling algorithm,\\nwhich geometrically increases the amount of data utilized for gradient and\\nKrylov subspace iteration calculations. On a 50-hr English Broadcast News task,\\nwe find that these methodologies provide roughly a 1.5x speed-up, whereas, on a\\n300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no\\nloss in WER. These results suggest that even further speed-up is expected, as\\nproblems scale and complexity grows.',\n",
              " 'Is a Picture Worth Ten Thousand Words in a Review Dataset?\\nWhile textual reviews have become prominent in many recommendation-based\\nsystems, automated frameworks to provide relevant visual cues against text\\nreviews where pictures are not available is a new form of task confronted by\\ndata mining and machine learning researchers. Suggestions of pictures that are\\nrelevant to the content of a review could significantly benefit the users by\\nincreasing the effectiveness of a review. We propose a deep learning-based\\nframework to automatically: (1) tag the images available in a review dataset,\\n(2) generate a caption for each image that does not have one, and (3) enhance\\neach review by recommending relevant images that might not be uploaded by the\\ncorresponding reviewer. We evaluate the proposed framework using the Yelp\\nChallenge Dataset. While a subset of the images in this particular dataset are\\ncorrectly captioned, the majority of the pictures do not have any associated\\ntext. Moreover, there is no mapping between reviews and images. Each image has\\na corresponding business-tag where the picture was taken, though. The overall\\ndata setting and unavailability of crucial pieces required for a mapping make\\nthe problem of recommending images for reviews a major challenge. Qualitative\\nand quantitative evaluations indicate that our proposed framework provides high\\nquality enhancements through automatic captioning, tagging, and recommendation\\nfor mapping reviews and images.',\n",
              " 'Validation of nonlinear PCA\\nLinear principal component analysis (PCA) can be extended to a nonlinear PCA\\nby using artificial neural networks. But the benefit of curved components\\nrequires a careful control of the model complexity. Moreover, standard\\ntechniques for model selection, including cross-validation and more generally\\nthe use of an independent test set, fail when applied to nonlinear PCA because\\nof its inherent unsupervised characteristics. This paper presents a new\\napproach for validating the complexity of nonlinear PCA models by using the\\nerror in missing data estimation as a criterion for model selection. It is\\nmotivated by the idea that only the model of optimal complexity is able to\\npredict missing values with the highest accuracy. While standard test set\\nvalidation usually favours over-fitted nonlinear PCA models, the proposed model\\nvalidation approach correctly selects the optimal model complexity.',\n",
              " 'Graph Approximation and Clustering on a Budget\\nWe consider the problem of learning from a similarity matrix (such as\\nspectral clustering and lowd imensional embedding), when computing pairwise\\nsimilarities are costly, and only a limited number of entries can be observed.\\nWe provide a theoretical analysis using standard notions of graph\\napproximation, significantly generalizing previous results (which focused on\\nspectral clustering with two clusters). We also propose a new algorithmic\\napproach based on adaptive sampling, which experimentally matches or improves\\non previous methods, while being considerably more general and computationally\\ncheaper.',\n",
              " 'ShareBoost: Efficient Multiclass Learning with Feature Sharing\\nMulticlass prediction is the problem of classifying an object into a relevant\\ntarget class. We consider the problem of learning a multiclass predictor that\\nuses only few features, and in particular, the number of used features should\\nincrease sub-linearly with the number of possible classes. This implies that\\nfeatures should be shared by several classes. We describe and analyze the\\nShareBoost algorithm for learning a multiclass predictor that uses few shared\\nfeatures. We prove that ShareBoost efficiently finds a predictor that uses few\\nshared features (if such a predictor exists) and that it has a small\\ngeneralization error. We also describe how to use ShareBoost for learning a\\nnon-linear predictor that has a fast evaluation time. In a series of\\nexperiments with natural data sets we demonstrate the benefits of ShareBoost\\nand evaluate its success relatively to other state-of-the-art approaches.',\n",
              " 'Functional Principal Component Analysis and Randomized Sparse Clustering\\n  Algorithm for Medical Image Analysis\\nDue to advances in sensors, growing large and complex medical image data have\\nthe ability to visualize the pathological change in the cellular or even the\\nmolecular level or anatomical changes in tissues and organs. As a consequence,\\nthe medical images have the potential to enhance diagnosis of disease,\\nprediction of clinical outcomes, characterization of disease progression,\\nmanagement of health care and development of treatments, but also pose great\\nmethodological and computational challenges for representation and selection of\\nfeatures in image cluster analysis. To address these challenges, we first\\nextend one dimensional functional principal component analysis to the two\\ndimensional functional principle component analyses (2DFPCA) to fully capture\\nspace variation of image signals. Image signals contain a large number of\\nredundant and irrelevant features which provide no additional or no useful\\ninformation for cluster analysis. Widely used methods for removing redundant\\nand irrelevant features are sparse clustering algorithms using a lasso-type\\npenalty to select the features. However, the accuracy of clustering using a\\nlasso-type penalty depends on how to select penalty parameters and a threshold\\nfor selecting features. In practice, they are difficult to determine. Recently,\\nrandomized algorithms have received a great deal of attention in big data\\nanalysis. This paper presents a randomized algorithm for accurate feature\\nselection in image cluster analysis. The proposed method is applied to ovarian\\nand kidney cancer histology image data from the TCGA database. The results\\ndemonstrate that the randomized feature selection method coupled with\\nfunctional principal component analysis substantially outperforms the current\\nsparse clustering algorithms in image cluster analysis.',\n",
              " 'Jointly Learning Multiple Measures of Similarities from Triplet\\n  Comparisons\\nSimilarity between objects is multi-faceted and it can be easier for human\\nannotators to measure it when the focus is on a specific aspect. We consider\\nthe problem of mapping objects into view-specific embeddings where the distance\\nbetween them is consistent with the similarity comparisons of the form \"from\\nthe t-th view, object A is more similar to B than to C\". Our framework jointly\\nlearns view-specific embeddings exploiting correlations between views.\\nExperiments on a number of datasets, including one of multi-view crowdsourced\\ncomparison on bird images, show the proposed method achieves lower triplet\\ngeneralization error when compared to both learning embeddings independently\\nfor each view and all views pooled into one view. Our method can also be used\\nto learn multiple measures of similarity over input features taking class\\nlabels into account and compares favorably to existing approaches for\\nmulti-task metric learning on the ISOLET dataset.',\n",
              " 'Variational Inference for Uncertainty on the Inputs of Gaussian Process\\n  Models\\nThe Gaussian process latent variable model (GP-LVM) provides a flexible\\napproach for non-linear dimensionality reduction that has been widely applied.\\nHowever, the current approach for training GP-LVMs is based on maximum\\nlikelihood, where the latent projection variables are maximized over rather\\nthan integrated out. In this paper we present a Bayesian method for training\\nGP-LVMs by introducing a non-standard variational inference framework that\\nallows to approximately integrate out the latent variables and subsequently\\ntrain a GP-LVM by maximizing an analytic lower bound on the exact marginal\\nlikelihood. We apply this method for learning a GP-LVM from iid observations\\nand for learning non-linear dynamical systems where the observations are\\ntemporally correlated. We show that a benefit of the variational Bayesian\\nprocedure is its robustness to overfitting and its ability to automatically\\nselect the dimensionality of the nonlinear latent space. The resulting\\nframework is generic, flexible and easy to extend for other purposes, such as\\nGaussian process regression with uncertain inputs and semi-supervised Gaussian\\nprocesses. We demonstrate our method on synthetic data and standard machine\\nlearning benchmarks, as well as challenging real world datasets, including high\\nresolution video data.',\n",
              " 'Conditional Generative Adversarial Nets\\nGenerative Adversarial Nets [8] were recently introduced as a novel way to\\ntrain generative models. In this work we introduce the conditional version of\\ngenerative adversarial nets, which can be constructed by simply feeding the\\ndata, y, we wish to condition on to both the generator and discriminator. We\\nshow that this model can generate MNIST digits conditioned on class labels. We\\nalso illustrate how this model could be used to learn a multi-modal model, and\\nprovide preliminary examples of an application to image tagging in which we\\ndemonstrate how this approach can generate descriptive tags which are not part\\nof training labels.',\n",
              " 'Visual Causal Feature Learning\\nWe provide a rigorous definition of the visual cause of a behavior that is\\nbroadly applicable to the visually driven behavior in humans, animals, neurons,\\nrobots and other perceiving systems. Our framework generalizes standard\\naccounts of causal learning to settings in which the causal variables need to\\nbe constructed from micro-variables. We prove the Causal Coarsening Theorem,\\nwhich allows us to gain causal knowledge from observational data with minimal\\nexperimental effort. The theorem provides a connection to standard inference\\ntechniques in machine learning that identify features of an image that\\ncorrelate with, but may not cause, the target behavior. Finally, we propose an\\nactive learning scheme to learn a manipulator function that performs optimal\\nmanipulations on the image to automatically identify the visual cause of a\\ntarget behavior. We illustrate our inference and learning algorithms in\\nexperiments based on both synthetic and real data.',\n",
              " 'In Search of the Real Inductive Bias: On the Role of Implicit\\n  Regularization in Deep Learning\\nWe present experiments demonstrating that some other form of capacity\\ncontrol, different from network size, plays a central role in learning\\nmultilayer feed-forward networks. We argue, partially through analogy to matrix\\nfactorization, that this is an inductive bias that can help shed light on deep\\nlearning.',\n",
              " 'Domain Generalization for Object Recognition with Multi-task\\n  Autoencoders\\nThe problem of domain generalization is to take knowledge acquired from a\\nnumber of related domains where training data is available, and to then\\nsuccessfully apply it to previously unseen domains. We propose a new feature\\nlearning algorithm, Multi-Task Autoencoder (MTAE), that provides good\\ngeneralization performance for cross-domain object recognition.\\n  Our algorithm extends the standard denoising autoencoder framework by\\nsubstituting artificially induced corruption with naturally occurring\\ninter-domain variability in the appearance of objects. Instead of\\nreconstructing images from noisy versions, MTAE learns to transform the\\noriginal image into analogs in multiple related domains. It thereby learns\\nfeatures that are robust to variations across domains. The learnt features are\\nthen used as inputs to a classifier.\\n  We evaluated the performance of the algorithm on benchmark image recognition\\ndatasets, where the task is to learn features from multiple datasets and to\\nthen predict the image label from unseen datasets. We found that (denoising)\\nMTAE outperforms alternative autoencoder-based models as well as the current\\nstate-of-the-art algorithms for domain generalization.',\n",
              " 'Data-Efficient Learning of Feedback Policies from Image Pixels using\\n  Deep Dynamical Models\\nData-efficient reinforcement learning (RL) in continuous state-action spaces\\nusing very high-dimensional observations remains a key challenge in developing\\nfully autonomous systems. We consider a particularly important instance of this\\nchallenge, the pixels-to-torques problem, where an RL agent learns a\\nclosed-loop control policy (\"torques\") from pixel information only. We\\nintroduce a data-efficient, model-based reinforcement learning algorithm that\\nlearns such a closed-loop policy directly from pixel information. The key\\ningredient is a deep dynamical model for learning a low-dimensional feature\\nembedding of images jointly with a predictive model in this low-dimensional\\nfeature space. Joint learning is crucial for long-term predictions, which lie\\nat the core of the adaptive nonlinear model predictive control strategy that we\\nuse for closed-loop control. Compared to state-of-the-art RL methods for\\ncontinuous states and actions, our approach learns quickly, scales to\\nhigh-dimensional state spaces, is lightweight and an important step toward\\nfully autonomous end-to-end learning from pixels to torques.',\n",
              " 'Scatter Component Analysis: A Unified Framework for Domain Adaptation\\n  and Domain Generalization\\nThis paper addresses classification tasks on a particular target domain in\\nwhich labeled training data are only available from source domains different\\nfrom (but related to) the target. Two closely related frameworks, domain\\nadaptation and domain generalization, are concerned with such tasks, where the\\nonly difference between those frameworks is the availability of the unlabeled\\ntarget data: domain adaptation can leverage unlabeled target information, while\\ndomain generalization cannot. We propose Scatter Component Analyis (SCA), a\\nfast representation learning algorithm that can be applied to both domain\\nadaptation and domain generalization. SCA is based on a simple geometrical\\nmeasure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA\\nfinds a representation that trades between maximizing the separability of\\nclasses, minimizing the mismatch between domains, and maximizing the\\nseparability of data; each of which is quantified through scatter. The\\noptimization problem of SCA can be reduced to a generalized eigenvalue problem,\\nwhich results in a fast and exact solution. Comprehensive experiments on\\nbenchmark cross-domain object recognition datasets verify that SCA performs\\nmuch faster than several state-of-the-art algorithms and also provides\\nstate-of-the-art classification accuracy in both domain adaptation and domain\\ngeneralization. We also show that scatter can be used to establish a\\ntheoretical generalization bound in the case of domain adaptation.',\n",
              " 'Robust Subspace Clustering via Tighter Rank Approximation\\nMatrix rank minimization problem is in general NP-hard. The nuclear norm is\\nused to substitute the rank function in many recent studies. Nevertheless, the\\nnuclear norm approximation adds all singular values together and the\\napproximation error may depend heavily on the magnitudes of singular values.\\nThis might restrict its capability in dealing with many practical problems. In\\nthis paper, an arctangent function is used as a tighter approximation to the\\nrank function. We use it on the challenging subspace clustering problem. For\\nthis nonconvex minimization problem, we develop an effective optimization\\nprocedure based on a type of augmented Lagrange multipliers (ALM) method.\\nExtensive experiments on face clustering and motion segmentation show that the\\nproposed method is effective for rank approximation.',\n",
              " 'Recognizing Semantic Features in Faces using Deep Learning\\nThe human face constantly conveys information, both consciously and\\nsubconsciously. However, as basic as it is for humans to visually interpret\\nthis information, it is quite a big challenge for machines. Conventional\\nsemantic facial feature recognition and analysis techniques are already in use\\nand are based on physiological heuristics, but they suffer from lack of\\nrobustness and high computation time. This thesis aims to explore ways for\\nmachines to learn to interpret semantic information available in faces in an\\nautomated manner without requiring manual design of feature detectors, using\\nthe approach of Deep Learning. This thesis provides a study of the effects of\\nvarious factors and hyper-parameters of deep neural networks in the process of\\ndetermining an optimal network configuration for the task of semantic facial\\nfeature recognition. This thesis explores the effectiveness of the system to\\nrecognize the various semantic features (like emotions, age, gender, ethnicity\\netc.) present in faces. Furthermore, the relation between the effect of\\nhigh-level concepts on low level features is explored through an analysis of\\nthe similarities in low-level descriptors of different semantic features. This\\nthesis also demonstrates a novel idea of using a deep network to generate 3-D\\nActive Appearance Models of faces from real-world 2-D images.\\n  For a more detailed report on this work, please see [arXiv:1512.00743v1].',\n",
              " \"Deep Reconstruction-Classification Networks for Unsupervised Domain\\n  Adaptation\\nIn this paper, we propose a novel unsupervised domain adaptation algorithm\\nbased on deep learning for visual object recognition. Specifically, we design a\\nnew model called Deep Reconstruction-Classification Network (DRCN), which\\njointly learns a shared encoding representation for two tasks: i) supervised\\nclassification of labeled source data, and ii) unsupervised reconstruction of\\nunlabeled target data.In this way, the learnt representation not only preserves\\ndiscriminability, but also encodes useful information from the target domain.\\nOur new DRCN model can be optimized by using backpropagation similarly as the\\nstandard neural networks.\\n  We evaluate the performance of DRCN on a series of cross-domain object\\nrecognition tasks, where DRCN provides a considerable improvement (up to ~8% in\\naccuracy) over the prior state-of-the-art algorithms. Interestingly, we also\\nobserve that the reconstruction pipeline of DRCN transforms images from the\\nsource domain into images whose appearance resembles the target dataset. This\\nsuggests that DRCN's performance is due to constructing a single composite\\nrepresentation that encodes information about both the structure of target\\nimages and the classification of source images. Finally, we provide a formal\\nanalysis to justify the algorithm's objective in domain adaptation context.\",\n",
              " 'A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation\\nFinding the most effective way to aggregate multi-subject fMRI data is a\\nlong-standing and challenging problem. It is of increasing interest in\\ncontemporary fMRI studies of human cognition due to the scarcity of data per\\nsubject and the variability of brain anatomy and functional response across\\nsubjects. Recent work on latent factor models shows promising results in this\\ntask but this approach does not preserve spatial locality in the brain. We\\nexamine two ways to combine the ideas of a factor model and a searchlight based\\nanalysis to aggregate multi-subject fMRI data while preserving spatial\\nlocality. We first do this directly by combining a recent factor method known\\nas a shared response model with searchlight analysis. Then we design a\\nmulti-view convolutional autoencoder for the same task. Both approaches\\npreserve spatial locality and have competitive or better performance compared\\nwith standard searchlight analysis and the shared response model applied across\\nthe whole brain. We also report a system design to handle the computational\\nchallenge of training the convolutional autoencoder.',\n",
              " 'Feedback-Controlled Sequential Lasso Screening\\nOne way to solve lasso problems when the dictionary does not fit into\\navailable memory is to first screen the dictionary to remove unneeded features.\\nPrior research has shown that sequential screening methods offer the greatest\\npromise in this endeavor. Most existing work on sequential screening targets\\nthe context of tuning parameter selection, where one screens and solves a\\nsequence of $N$ lasso problems with a fixed grid of geometrically spaced\\nregularization parameters. In contrast, we focus on the scenario where a target\\nregularization parameter has already been chosen via cross-validated model\\nselection, and we then need to solve many lasso instances using this fixed\\nvalue. In this context, we propose and explore a feedback controlled sequential\\nscreening scheme. Feedback is used at each iteration to select the next problem\\nto be solved. This allows the sequence of problems to be adapted to the\\ninstance presented and the number of intermediate problems to be automatically\\nselected. We demonstrate our feedback scheme using several datasets including a\\ndictionary of approximate size 100,000 by 300,000.',\n",
              " \"The Symmetry of a Simple Optimization Problem in Lasso Screening\\nRecently dictionary screening has been proposed as an effective way to\\nimprove the computational efficiency of solving the lasso problem, which is one\\nof the most commonly used method for learning sparse representations. To\\naddress today's ever increasing large dataset, effective screening relies on a\\ntight region bound on the solution to the dual lasso. Typical region bounds are\\nin the form of an intersection of a sphere and multiple half spaces. One way to\\ntighten the region bound is using more half spaces, which however, adds to the\\noverhead of solving the high dimensional optimization problem in lasso\\nscreening. This paper reveals the interesting property that the optimization\\nproblem only depends on the projection of features onto the subspace spanned by\\nthe normals of the half spaces. This property converts an optimization problem\\nin high dimension to much lower dimension, and thus sheds light on reducing the\\ncomputation overhead of lasso screening based on tighter region bounds.\",\n",
              " 'Hard Negative Mining for Metric Learning Based Zero-Shot Classification\\nZero-Shot learning has been shown to be an efficient strategy for domain\\nadaptation. In this context, this paper builds on the recent work of Bucher et\\nal. [1], which proposed an approach to solve Zero-Shot classification problems\\n(ZSC) by introducing a novel metric learning based objective function. This\\nobjective function allows to learn an optimal embedding of the attributes\\njointly with a measure of similarity between images and attributes. This paper\\nextends their approach by proposing several schemes to control the generation\\nof the negative pairs, resulting in a significant improvement of the\\nperformance and giving above state-of-the-art results on three challenging ZSC\\ndatasets.',\n",
              " 'Pose-Selective Max Pooling for Measuring Similarity\\nIn this paper, we deal with two challenges for measuring the similarity of\\nthe subject identities in practical video-based face recognition - the\\nvariation of the head pose in uncontrolled environments and the computational\\nexpense of processing videos. Since the frame-wise feature mean is unable to\\ncharacterize the pose diversity among frames, we define and preserve the\\noverall pose diversity and closeness in a video. Then, identity will be the\\nonly source of variation across videos since the pose varies even within a\\nsingle video. Instead of simply using all the frames, we select those faces\\nwhose pose point is closest to the centroid of the K-means cluster containing\\nthat pose point. Then, we represent a video as a bag of frame-wise deep face\\nfeatures while the number of features has been reduced from hundreds to K.\\nSince the video representation can well represent the identity, now we measure\\nthe subject similarity between two videos as the max correlation among all\\npossible pairs in the two bags of features. On the official 5,000 video-pairs\\nof the YouTube Face dataset for face verification, our algorithm achieves a\\ncomparable performance with VGG-face that averages over deep features of all\\nframes. Other vision tasks can also benefit from the generic idea of employing\\ngeometric cues to improve the descriptiveness of deep features.',\n",
              " 'Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble\\n  of Autoencoders\\nA fall is an abnormal activity that occurs rarely, so it is hard to collect\\nreal data for falls. It is, therefore, difficult to use supervised learning\\nmethods to automatically detect falls. Another challenge in using machine\\nlearning methods to automatically detect falls is the choice of engineered\\nfeatures. In this paper, we propose to use an ensemble of autoencoders to\\nextract features from different channels of wearable sensor data trained only\\non normal activities. We show that the traditional approach of choosing a\\nthreshold as the maximum of the reconstruction error on the training normal\\ndata is not the right way to identify unseen falls. We propose two methods for\\nautomatic tightening of reconstruction error from only the normal activities\\nfor better identification of unseen falls. We present our results on two\\nactivity recognition datasets and show the efficacy of our proposed method\\nagainst traditional autoencoder models and two standard one-class\\nclassification methods.',\n",
              " 'Generalization Error of Invariant Classifiers\\nThis paper studies the generalization error of invariant classifiers. In\\nparticular, we consider the common scenario where the classification task is\\ninvariant to certain transformations of the input, and that the classifier is\\nconstructed (or learned) to be invariant to these transformations. Our approach\\nrelies on factoring the input space into a product of a base space and a set of\\ntransformations. We show that whereas the generalization error of a\\nnon-invariant classifier is proportional to the complexity of the input space,\\nthe generalization error of an invariant classifier is proportional to the\\ncomplexity of the base space. We also derive a set of sufficient conditions on\\nthe geometry of the base space and the set of transformations that ensure that\\nthe complexity of the base space is much smaller than the complexity of the\\ninput space. Our analysis applies to general classifiers such as convolutional\\nneural networks. We demonstrate the implications of the developed theory for\\nsuch classifiers with experiments on the MNIST and CIFAR-10 datasets.',\n",
              " 'Universal adversarial perturbations\\nGiven a state-of-the-art deep neural network classifier, we show the\\nexistence of a universal (image-agnostic) and very small perturbation vector\\nthat causes natural images to be misclassified with high probability. We\\npropose a systematic algorithm for computing universal perturbations, and show\\nthat state-of-the-art deep neural networks are highly vulnerable to such\\nperturbations, albeit being quasi-imperceptible to the human eye. We further\\nempirically analyze these universal perturbations and show, in particular, that\\nthey generalize very well across neural networks. The surprising existence of\\nuniversal perturbations reveals important geometric correlations among the\\nhigh-dimensional decision boundary of classifiers. It further outlines\\npotential security breaches with the existence of single directions in the\\ninput space that adversaries can possibly exploit to break a classifier on most\\nnatural images.',\n",
              " 'Linear Disentangled Representation Learning for Facial Actions\\nLimited annotated data available for the recognition of facial expression and\\naction units embarrasses the training of deep networks, which can learn\\ndisentangled invariant features. However, a linear model with just several\\nparameters normally is not demanding in terms of training data. In this paper,\\nwe propose an elegant linear model to untangle confounding factors in\\nchallenging realistic multichannel signals such as 2D face videos. The simple\\nyet powerful model does not rely on huge training data and is natural for\\nrecognizing facial actions without explicitly disentangling the identity. Base\\non well-understood intuitive linear models such as Sparse Representation based\\nClassification (SRC), previous attempts require a prepossessing of explicit\\ndecoupling which is practically inexact. Instead, we exploit the low-rank\\nproperty across frames to subtract the underlying neutral faces which are\\nmodeled jointly with sparse representation on the action components with group\\nsparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot\\nautomatic method on raw face videos performs as competitive as SRC applied on\\nmanually prepared action components and performs even better than SRC in terms\\nof true positive rate. We apply the model to the even more challenging task of\\nfacial action unit recognition, verified on the MPI Face Video Database\\n(MPI-VDB) achieving a decent performance. All the programs and data have been\\nmade publicly available.',\n",
              " 'On Detecting Adversarial Perturbations\\nMachine learning and deep learning in particular has advanced tremendously on\\nperceptual tasks in recent years. However, it remains vulnerable against\\nadversarial perturbations of the input that have been crafted specifically to\\nfool the system while being quasi-imperceptible to a human. In this work, we\\npropose to augment deep neural networks with a small \"detector\" subnetwork\\nwhich is trained on the binary classification task of distinguishing genuine\\ndata from data containing adversarial perturbations. Our method is orthogonal\\nto prior work on addressing adversarial perturbations, which has mostly focused\\non making the classification network itself more robust. We show empirically\\nthat adversarial perturbations can be detected surprisingly well even though\\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\\ntrained to detect only a specific adversary, they generalize to similar and\\nweaker adversaries. In addition, we propose an adversarial attack that fools\\nboth the classifier and the detector and a novel training procedure for the\\ndetector that counteracts this attack.',\n",
              " \"Activation Maximization Generative Adversarial Nets\\nClass labels have been empirically shown useful in improving the sample\\nquality of generative adversarial nets (GANs). In this paper, we mathematically\\nstudy the properties of the current variants of GANs that make use of class\\nlabel information. With class aware gradient and cross-entropy decomposition,\\nwe reveal how class labels and associated losses influence GAN's training.\\nBased on that, we propose Activation Maximization Generative Adversarial\\nNetworks (AM-GAN) as an advanced solution. Comprehensive experiments have been\\nconducted to validate our analysis and evaluate the effectiveness of our\\nsolution, where AM-GAN outperforms other strong baselines and achieves\\nstate-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we\\ndemonstrate that, with the Inception ImageNet classifier, Inception Score\\nmainly tracks the diversity of the generator, and there is, however, no\\nreliable evidence that it can reflect the true sample quality. We thus propose\\na new metric, called AM Score, to provide more accurate estimation on the\\nsample quality. Our proposed model also outperforms the baseline methods in the\\nnew metric.\",\n",
              " 'Interpretable Explanations of Black Boxes by Meaningful Perturbation\\nAs machine learning algorithms are increasingly applied to high impact yet\\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\\ncritical that researchers can explain how such algorithms arrived at their\\npredictions. In recent years, a number of image saliency methods have been\\ndeveloped to summarize where highly complex neural networks \"look\" in an image\\nfor evidence for their predictions. However, these techniques are limited by\\ntheir heuristic nature and architectural constraints. In this paper, we make\\ntwo main contributions: First, we propose a general framework for learning\\ndifferent kinds of explanations for any black box algorithm. Second, we\\nspecialise the framework to find the part of an image most responsible for a\\nclassifier decision. Unlike previous works, our method is model-agnostic and\\ntestable because it is grounded in explicit and interpretable image\\nperturbations.',\n",
              " 'A General Theory for Training Learning Machine\\nThough the deep learning is pushing the machine learning to a new stage,\\nbasic theories of machine learning are still limited. The principle of\\nlearning, the role of the a prior knowledge, the role of neuron bias, and the\\nbasis for choosing neural transfer function and cost function, etc., are still\\nfar from clear. In this paper, we present a general theoretical framework for\\nmachine learning. We classify the prior knowledge into common and\\nproblem-dependent parts, and consider that the aim of learning is to maximally\\nincorporate them. The principle we suggested for maximizing the former is the\\ndesign risk minimization principle, while the neural transfer function, the\\ncost function, as well as pretreatment of samples, are endowed with the role\\nfor maximizing the latter. The role of the neuron bias is explained from a\\ndifferent angle. We develop a Monte Carlo algorithm to establish the\\ninput-output responses, and we control the input-output sensitivity of a\\nlearning machine by controlling that of individual neurons. Applications of\\nfunction approaching and smoothing, pattern recognition and classification, are\\nprovided to illustrate how to train general learning machines based on our\\ntheory and algorithm. Our method may in addition induce new applications, such\\nas the transductive inference.',\n",
              " 'A Generalization of Convolutional Neural Networks to Graph-Structured\\n  Data\\nThis paper introduces a generalization of Convolutional Neural Networks\\n(CNNs) from low-dimensional grid data, such as images, to graph-structured\\ndata. We propose a novel spatial convolution utilizing a random walk to uncover\\nthe relations within the input, analogous to the way the standard convolution\\nuses the spatial neighborhood of a pixel on the grid. The convolution has an\\nintuitive interpretation, is efficient and scalable and can also be used on\\ndata with varying graph structure. Furthermore, this generalization can be\\napplied to many standard regression or classification problems, by learning the\\nthe underlying graph. We empirically demonstrate the performance of the\\nproposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular\\nactivity data set.',\n",
              " 'Formal Guarantees on the Robustness of a Classifier against Adversarial\\n  Manipulation\\nRecent work has shown that state-of-the-art classifiers are quite brittle, in\\nthe sense that a small adversarial change of an originally with high confidence\\ncorrectly classified input leads to a wrong classification again with high\\nconfidence. This raises concerns that such classifiers are vulnerable to\\nattacks and calls into question their usage in safety-critical systems. We show\\nin this paper for the first time formal guarantees on the robustness of a\\nclassifier by giving instance-specific lower bounds on the norm of the input\\nmanipulation required to change the classifier decision. Based on this analysis\\nwe propose the Cross-Lipschitz regularization functional. We show that using\\nthis form of regularization in kernel methods resp. neural networks improves\\nthe robustness of the classifier without any loss in prediction performance.',\n",
              " 'Classification regions of deep neural networks\\nThe goal of this paper is to analyze the geometric properties of deep neural\\nnetwork classifiers in the input space. We specifically study the topology of\\nclassification regions created by deep networks, as well as their associated\\ndecision boundary. Through a systematic empirical investigation, we show that\\nstate-of-the-art deep nets learn connected classification regions, and that the\\ndecision boundary in the vicinity of datapoints is flat along most directions.\\nWe further draw an essential connection between two seemingly unrelated\\nproperties of deep networks: their sensitivity to additive perturbations in the\\ninputs, and the curvature of their decision boundary. The directions where the\\ndecision boundary is curved in fact remarkably characterize the directions to\\nwhich the classifier is the most vulnerable. We finally leverage a fundamental\\nasymmetry in the curvature of the decision boundary of deep nets, and propose a\\nmethod to discriminate between original images, and images perturbed with small\\nadversarial examples. We show the effectiveness of this purely geometric\\napproach for detecting small adversarial perturbations in images, and for\\nrecovering the labels of perturbed images.',\n",
              " 'Analysis of universal adversarial perturbations\\nDeep networks have recently been shown to be vulnerable to universal\\nperturbations: there exist very small image-agnostic perturbations that cause\\nmost natural images to be misclassified by such classifiers. In this paper, we\\npropose the first quantitative analysis of the robustness of classifiers to\\nuniversal perturbations, and draw a formal link between the robustness to\\nuniversal perturbations, and the geometry of the decision boundary.\\nSpecifically, we establish theoretical bounds on the robustness of classifiers\\nunder two decision boundary models (flat and curved models). We show in\\nparticular that the robustness of deep networks to universal perturbations is\\ndriven by a key property of their curvature: there exists shared directions\\nalong which the decision boundary of deep networks is systematically positively\\ncurved. Under such conditions, we prove the existence of small universal\\nperturbations. Our analysis further provides a novel geometric method for\\ncomputing universal perturbations, in addition to explaining their properties.',\n",
              " 'Bayesian GAN\\nGenerative adversarial networks (GANs) can implicitly learn rich\\ndistributions over images, audio, and data which are hard to model with an\\nexplicit likelihood. We present a practical Bayesian formulation for\\nunsupervised and semi-supervised learning with GANs. Within this framework, we\\nuse stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of\\nthe generator and discriminator networks. The resulting approach is\\nstraightforward and obtains good performance without any standard interventions\\nsuch as feature matching, or mini-batch discrimination. By exploring an\\nexpressive posterior over the parameters of the generator, the Bayesian GAN\\navoids mode-collapse, produces interpretable and diverse candidate samples, and\\nprovides state-of-the-art quantitative results for semi-supervised learning on\\nbenchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,\\nWasserstein GANs, and DCGAN ensembles.',\n",
              " 'Unsupervised Learning of Disentangled Representations from Video\\nWe present a new model DrNET that learns disentangled image representations\\nfrom video. Our approach leverages the temporal coherence of video and a novel\\nadversarial loss to learn a representation that factorizes each frame into a\\nstationary part and a temporally varying component. The disentangled\\nrepresentation can be used for a range of tasks. For example, applying a\\nstandard LSTM to the time-vary components enables prediction of future frames.\\nWe evaluate our approach on a range of synthetic and real videos, demonstrating\\nthe ability to coherently generate hundreds of steps into the future.',\n",
              " \"Dualing GANs\\nGenerative adversarial nets (GANs) are a promising technique for modeling a\\ndistribution from samples. It is however well known that GAN training suffers\\nfrom instability due to the nature of its maximin formulation. In this paper,\\nwe explore ways to tackle the instability problem by dualizing the\\ndiscriminator. We start from linear discriminators in which case conjugate\\nduality provides a mechanism to reformulate the saddle point objective into a\\nmaximization problem, such that both the generator and the discriminator of\\nthis 'dualing GAN' act in concert. We then demonstrate how to extend this\\nintuition to non-linear formulations. For GANs with linear discriminators our\\napproach is able to remove the instability in training, while for GANs with\\nnonlinear discriminators our approach provides an alternative to the commonly\\nused GAN training algorithm.\",\n",
              " 'Wavelet Residual Network for Low-Dose CT via Deep Convolutional\\n  Framelets\\nModel based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT\\nare computationally expensive. To address this problem, we recently proposed\\nthe world-first deep convolutional neural network (CNN) for low-dose X-ray CT\\nand won the second place in 2016 AAPM Low-Dose CT Grand Challenge. However,\\nsome of the texture were not fully recovered. To cope with this problem, here\\nwe propose a deep residual learning approach in directional wavelet domain. The\\nproposed method is motivated by an observation that a deep convolutional neural\\nnetwork can be interpreted as a multilayer convolutional framelets expansion\\nusing non-local basis convolved with data-driven local basis. We further extend\\nthe idea to derive a deep convolutional framelet expansion by combining global\\nredundant transforms and signal boosting from multiple signal representations.\\nExtensive experimental results confirm that the proposed network has\\nsignificantly improved performance and preserves the detail texture of the\\noriginal images',\n",
              " '3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks\\nThe success of various applications including robotics, digital content\\ncreation, and visualization demand a structured and abstract representation of\\nthe 3D world from limited sensor data. Inspired by the nature of human\\nperception of 3D shapes as a collection of simple parts, we explore such an\\nabstract shape representation based on primitives. Given a single depth image\\nof an object, we present 3D-PRNN, a generative recurrent neural network that\\nsynthesizes multiple plausible shapes composed of a set of primitives. Our\\ngenerative model encodes symmetry characteristics of common man-made objects,\\npreserves long-range structural coherence, and describes objects of varying\\ncomplexity with a compact representation. We also propose a method based on\\nGaussian Fields to generate a large scale dataset of primitive-based shape\\nrepresentations to train our network. We evaluate our approach on a wide range\\nof examples and show that it outperforms nearest-neighbor based shape retrieval\\nmethods and is on-par with voxel-based generative models while using a\\nsignificantly reduced parameter space.',\n",
              " 'Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x))\\n  Alternative\\nIn this article, we mathematically study several GAN related topics,\\nincluding Inception score, label smoothing, gradient vanishing and the\\n-log(D(x)) alternative.\\n  --- An advanced version is included in arXiv:1703.02000 \"Activation\\nMaximization Generative Adversarial Nets\". Please refer Section 6 in 1703.02000\\nfor detailed analysis on Inception Score, and refer its appendix for the\\ndiscussions on Label Smoothing, Gradient Vanishing and -log(D(x)) Alternative.\\n---',\n",
              " 'A Brief Survey of Deep Reinforcement Learning\\nDeep reinforcement learning is poised to revolutionise the field of AI and\\nrepresents a step towards building autonomous systems with a higher level\\nunderstanding of the visual world. Currently, deep learning is enabling\\nreinforcement learning to scale to problems that were previously intractable,\\nsuch as learning to play video games directly from pixels. Deep reinforcement\\nlearning algorithms are also applied to robotics, allowing control policies for\\nrobots to be learned directly from camera inputs in the real world. In this\\nsurvey, we begin with an introduction to the general field of reinforcement\\nlearning, then progress to the main streams of value-based and policy-based\\nmethods. Our survey will cover central algorithms in deep reinforcement\\nlearning, including the deep $Q$-network, trust region policy optimisation, and\\nasynchronous advantage actor-critic. In parallel, we highlight the unique\\nadvantages of deep neural networks, focusing on visual understanding via\\nreinforcement learning. To conclude, we describe several current areas of\\nresearch within the field.',\n",
              " 'CirCNN: Accelerating and Compressing Deep Neural Networks Using\\n  Block-CirculantWeight Matrices\\nLarge-scale deep neural networks (DNNs) are both compute and memory\\nintensive. As the size of DNNs continues to grow, it is critical to improve the\\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\\nmodel size is an important factor affecting performance, scalability and energy\\nefficiency. Weight pruning achieves good compression ratios but suffers from\\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\\nincreased training complexity; and 3) the lack of rigorous guarantee of\\ncompression ratio and inference accuracy. To overcome these limitations, this\\npaper proposes CirCNN, a principled approach to represent weights and process\\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\\ncomputational complexity (both in inference and training) from O(n2) to\\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\\nmathematical rigor: it can converge to the same effectiveness as DNNs without\\ncompression. The CirCNN architecture, a universal DNN inference engine that can\\nbe implemented on various hardware/software platforms with configurable network\\narchitecture. To demonstrate the performance and energy efficiency, we test\\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\\narchitecture achieves very high energy efficiency and performance with a small\\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\\nthe best state-of-the-art results.',\n",
              " 'XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual\\n  Classification\\nWe propose two multimodal deep learning architectures that allow for\\ncross-modal dataflow (XFlow) between the feature extractors, thereby extracting\\nmore interpretable features and obtaining a better representation than through\\nunimodal learning, for the same amount of training data. These models can\\nusefully exploit correlations between audio and visual data, which have a\\ndifferent dimensionality and are therefore nontrivially exchangeable. Our work\\nimproves on existing multimodal deep learning metholodogies in two essential\\nways: (1) it presents a novel method for performing cross-modality (before\\nfeatures are learned from individual modalities) and (2) extends the previously\\nproposed cross-connections, which only transfer information between streams\\nthat process compatible data. Both cross-modal architectures outperformed their\\nbaselines (by up to 7.5%) when evaluated on the AVletters dataset.',\n",
              " 'Context Embedding Networks\\nLow dimensional embeddings that capture the main variations of interest in\\ncollections of data are important for many applications. One way to construct\\nthese embeddings is to acquire estimates of similarity from the crowd. However,\\nsimilarity is a multi-dimensional concept that varies from individual to\\nindividual. Existing models for learning embeddings from the crowd typically\\nmake simplifying assumptions such as all individuals estimate similarity using\\nthe same criteria, the list of criteria is known in advance, or that the crowd\\nworkers are not influenced by the data that they see. To overcome these\\nlimitations we introduce Context Embedding Networks (CENs). In addition to\\nlearning interpretable embeddings from images, CENs also model worker biases\\nfor different attributes along with the visual context i.e. the visual\\nattributes highlighted by a set of images. Experiments on two noisy crowd\\nannotated datasets show that modeling both worker bias and visual context\\nresults in more interpretable embeddings compared to existing approaches.',\n",
              " \"How Much Chemistry Does a Deep Neural Network Need to Know to Make\\n  Accurate Predictions?\\nThe meteoric rise of deep learning models in computer vision research, having\\nachieved human-level accuracy in image recognition tasks is firm evidence of\\nthe impact of representation learning of deep neural networks. In the chemistry\\ndomain, recent advances have also led to the development of similar CNN models,\\nsuch as Chemception, that is trained to predict chemical properties using\\nimages of molecular drawings. In this work, we investigate the effects of\\nsystematically removing and adding localized domain-specific information to the\\nimage channels of the training data. By augmenting images with only 3\\nadditional basic information, and without introducing any architectural\\nchanges, we demonstrate that an augmented Chemception (AugChemception)\\noutperforms the original model in the prediction of toxicity, activity, and\\nsolvation free energy. Then, by altering the information content in the images,\\nand examining the resulting model's performance, we also identify two distinct\\nlearning patterns in predicting toxicity/activity as compared to solvation free\\nenergy. These patterns suggest that Chemception is learning about its tasks in\\nthe manner that is consistent with established knowledge. Thus, our work\\ndemonstrates that advanced chemical knowledge is not a pre-requisite for deep\\nlearning models to accurately predict complex chemical properties.\",\n",
              " 'Variational Inference of Disentangled Latent Concepts from Unlabeled\\n  Observations\\nDisentangled representations, where the higher level data generative factors\\nare reflected in disjoint latent dimensions, offer several benefits such as\\nease of deriving invariant representations, transferability to other tasks,\\ninterpretability, etc. We consider the problem of unsupervised learning of\\ndisentangled representations from large pool of unlabeled observations, and\\npropose a variational inference based approach to infer disentangled latent\\nfactors. We introduce a regularizer on the expectation of the approximate\\nposterior over observed data that encourages the disentanglement. We evaluate\\nthe proposed approach using several quantitative metrics and empirically\\nobserve significant gains over existing methods in terms of both\\ndisentanglement and data likelihood (reconstruction quality).',\n",
              " 'Three Factors Influencing Minima in SGD\\nWe study the properties of the endpoint of stochastic gradient descent (SGD).\\nBy approximating SGD as a stochastic differential equation (SDE) we consider\\nthe Boltzmann-Gibbs equilibrium distribution of that SDE under the assumption\\nof isotropic variance in loss gradients. Through this analysis, we find that\\nthree factors - learning rate, batch size and the variance of the loss\\ngradients - control the trade-off between the depth and width of the minima\\nfound by SGD, with wider minima favoured by a higher ratio of learning rate to\\nbatch size. We have direct control over the learning rate and batch size, while\\nthe variance is determined by the choice of model architecture, model\\nparameterization and dataset. In the equilibrium distribution only the ratio of\\nlearning rate to batch size appears, implying that the equilibrium distribution\\nis invariant under a simultaneous rescaling of learning rate and batch size by\\nthe same amount. We then explore experimentally how learning rate and batch\\nsize affect SGD from two perspectives: the endpoint of SGD and the dynamics\\nthat lead up to it. For the endpoint, the experiments suggest the endpoint of\\nSGD is invariant under simultaneous rescaling of batch size and learning rate,\\nand also that a higher ratio leads to flatter minima, both findings are\\nconsistent with our theoretical analysis. We note experimentally that the\\ndynamics also seem to be invariant under the same rescaling of learning rate\\nand batch size, which we explore showing that one can exchange batch size and\\nlearning rate for cyclical learning rate schedule. Next, we illustrate how\\nnoise affects memorization, showing that high noise levels lead to better\\ngeneralization. Finally, we find experimentally that the invariance under\\nsimultaneous rescaling of learning rate and batch size breaks down if the\\nlearning rate gets too large or the batch size gets too small.',\n",
              " \"Learning to Play Othello with Deep Neural Networks\\nAchieving superhuman playing level by AlphaGo corroborated the capabilities\\nof convolutional neural architectures (CNNs) for capturing complex spatial\\npatterns. This result was to a great extent due to several analogies between Go\\nboard states and 2D images CNNs have been designed for, in particular\\ntranslational invariance and a relatively large board. In this paper, we verify\\nwhether CNN-based move predictors prove effective for Othello, a game with\\nsignificantly different characteristics, including a much smaller board size\\nand complete lack of translational invariance. We compare several CNN\\narchitectures and board encodings, augment them with state-of-the-art\\nextensions, train on an extensive database of experts' moves, and examine them\\nwith respect to move prediction accuracy and playing strength. The empirical\\nevaluation confirms high capabilities of neural move predictors and suggests a\\nstrong correlation between prediction accuracy and playing strength. The best\\nCNNs not only surpass all other 1-ply Othello players proposed to date but\\ndefeat (2-ply) Edax, the best open-source Othello player.\",\n",
              " 'Deep Learning Can Reverse Photon Migration for Diffuse Optical\\n  Tomography\\nCan artificial intelligence (AI) learn complicated non-linear physics? Here\\nwe propose a novel deep learning approach that learns non-linear photon\\nscattering physics and obtains accurate 3D distribution of optical anomalies.\\nIn contrast to the traditional black-box deep learning approaches to inverse\\nproblems, our deep network learns to invert the Lippmann-Schwinger integral\\nequation which describes the essential physics of photon migration of diffuse\\nnear-infrared (NIR) photons in turbid media. As an example for clinical\\nrelevance, we applied the method to our prototype diffuse optical tomography\\n(DOT). We show that our deep neural network, trained with only simulation data,\\ncan accurately recover the location of anomalies within biomimetic phantoms and\\nlive animals without the use of an exogenous contrast agent.',\n",
              " \"Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for\\n  Transferable Chemical Property Prediction\\nWith access to large datasets, deep neural networks (DNN) have achieved\\nhuman-level accuracy in image and speech recognition tasks. However, in\\nchemistry, data is inherently small and fragmented. In this work, we develop an\\napproach of using rule-based knowledge for training ChemNet, a transferable and\\ngeneralizable deep neural network for chemical property prediction that learns\\nin a weak-supervised manner from large unlabeled chemical databases. When\\ncoupled with transfer learning approaches to predict other smaller datasets for\\nchemical properties that it was not originally trained on, we show that\\nChemNet's accuracy outperforms contemporary DNN models that were trained using\\nconventional supervised learning. Furthermore, we demonstrate that the ChemNet\\npre-training approach is equally effective on both CNN (Chemception) and RNN\\n(SMILES2vec) models, indicating that this approach is network architecture\\nagnostic and is effective across multiple data modalities. Our results indicate\\na pre-trained ChemNet that incorporates chemistry domain knowledge, enables the\\ndevelopment of generalizable neural networks for more accurate prediction of\\nnovel chemical properties.\",\n",
              " 'Deep Learning in RF Sub-sampled B-mode Ultrasound Imaging\\nIn portable, three dimensional, and ultra-fast ultrasound (US) imaging\\nsystems, there is an increasing need to reconstruct high quality images from a\\nlimited number of RF data from receiver (Rx) or scan-line (SC) sub-sampling.\\nHowever, due to the severe side lobe artifacts from RF sub-sampling, the\\nstandard beam-former often produces blurry images with less contrast that are\\nnot suitable for diagnostic purpose. To address this problem, some researchers\\nhave studied compressed sensing (CS) to exploit the sparsity of the image or RF\\ndata in some domains. However, the existing CS approaches require either\\nhardware changes or computationally expensive algorithms. To overcome these\\nlimitations, here we propose a novel deep learning approach that directly\\ninterpolates the missing RF data by utilizing redundancy in the Rx-SC plane. In\\nparticular, the network design principle derives from a novel interpretation of\\nthe deep neural network as a cascaded convolution framelets that learns the\\ndata-driven bases for Hankel matrix decomposition. Our extensive experimental\\nresults from sub-sampled RF data from a real US system confirmed that the\\nproposed method can effectively reduce the data rate without sacrificing the\\nimage quality.',\n",
              " 'Deep Learning Interior Tomography for Region-of-Interest Reconstruction\\nInterior tomography for the region-of-interest (ROI) imaging has advantages\\nof using a small detector and reducing X-ray radiation dose. However, standard\\nanalytic reconstruction suffers from severe cupping artifacts due to existence\\nof null space in the truncated Radon transform. Existing penalized\\nreconstruction methods may address this problem but they require extensive\\ncomputations due to the iterative reconstruction. Inspired by the recent deep\\nlearning approaches to low-dose and sparse view CT, here we propose a deep\\nlearning architecture that removes null space signals from the FBP\\nreconstruction. Experimental results have shown that the proposed method\\nprovides near-perfect reconstruction with about 7-10 dB improvement in PSNR\\nover existing methods in spite of significantly reduced run-time complexity.',\n",
              " 'Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner\\nFor homeland and transportation security applications, 2D X-ray explosive\\ndetection system (EDS) have been widely used, but they have limitations in\\nrecognizing 3D shape of the hidden objects. Among various types of 3D computed\\ntomography (CT) systems to address this issue, this paper is interested in a\\nstationary CT using fixed X-ray sources and detectors. However, due to the\\nlimited number of projection views, analytic reconstruction algorithms produce\\nsevere streaking artifacts. Inspired by recent success of deep learning\\napproach for sparse view CT reconstruction, here we propose a novel image and\\nsinogram domain deep learning architecture for 3D reconstruction from very\\nsparse view measurement. The algorithm has been tested with the real data from\\na prototype 9-view dual energy stationary CT EDS carry-on baggage scanner\\ndeveloped by GEMSS Medical Systems, Korea, which confirms the superior\\nreconstruction performance over the existing approaches.',\n",
              " 'Effective Building Block Design for Deep Convolutional Neural Networks\\n  using Search\\nDeep learning has shown promising results on many machine learning tasks but\\nDL models are often complex networks with large number of neurons and layers,\\nand recently, complex layer structures known as building blocks. Finding the\\nbest deep model requires a combination of finding both the right architecture\\nand the correct set of parameters appropriate for that architecture. In\\naddition, this complexity (in terms of layer types, number of neurons, and\\nnumber of layers) also present problems with generalization since larger\\nnetworks are easier to overfit to the data. In this paper, we propose a search\\nframework for finding effective architectural building blocks for convolutional\\nneural networks (CNN). Our approach is much faster at finding models that are\\nclose to state-of-the-art in performance. In addition, the models discovered by\\nour approach are also smaller than models discovered by similar techniques. We\\nachieve these twin advantages by designing our search space in such a way that\\nit searches over a reduced set of state-of-the-art building blocks for CNNs\\nincluding residual block, inception block, inception-residual block, ResNeXt\\nblock and many others. We apply this technique to generate models for multiple\\nimage datasets and show that these models achieve performance comparable to\\nstate-of-the-art (and even surpassing the state-of-the-art in one case). We\\nalso show that learned models are transferable between datasets.',\n",
              " 'TVAE: Triplet-Based Variational Autoencoder using Metric Learning\\nDeep metric learning has been demonstrated to be highly effective in learning\\nsemantic representation and encoding information that can be used to measure\\ndata similarity, by relying on the embedding learned from metric learning. At\\nthe same time, variational autoencoder (VAE) has widely been used to\\napproximate inference and proved to have a good performance for directed\\nprobabilistic models. However, for traditional VAE, the data label or feature\\ninformation are intractable. Similarly, traditional representation learning\\napproaches fail to represent many salient aspects of the data. In this project,\\nwe propose a novel integrated framework to learn latent embedding in VAE by\\nincorporating deep metric learning. The features are learned by optimizing a\\ntriplet loss on the mean vectors of VAE in conjunction with standard evidence\\nlower bound (ELBO) of VAE. This approach, which we call Triplet based\\nVariational Autoencoder (TVAE), allows us to capture more fine-grained\\ninformation in the latent embedding. Our model is tested on MNIST data set and\\nachieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma &\\nWelling, 2013) achieves triplet accuracy of 75.08%.',\n",
              " 'Learning to Play with Intrinsically-Motivated Self-Aware Agents\\nInfants are experts at playing, with an amazing ability to generate novel\\nstructured behaviors in unstructured environments that lack clear extrinsic\\nreward signals. We seek to mathematically formalize these abilities using a\\nneural network that implements curiosity-driven intrinsic motivation. Using a\\nsimple but ecologically naturalistic simulated environment in which an agent\\ncan move and interact with objects it sees, we propose a \"world-model\" network\\nthat learns to predict the dynamic consequences of the agent\\'s actions.\\nSimultaneously, we train a separate explicit \"self-model\" that allows the agent\\nto track the error map of its own world-model, and then uses the self-model to\\nadversarially challenge the developing world-model. We demonstrate that this\\npolicy causes the agent to explore novel and informative interactions with its\\nenvironment, leading to the generation of a spectrum of complex behaviors,\\nincluding ego-motion prediction, object attention, and object gathering.\\nMoreover, the world-model that the agent learns supports improved performance\\non object dynamics prediction, detection, localization and recognition tasks.\\nTaken together, our results are initial steps toward creating flexible\\nautonomous agents that self-supervise in complex novel physical environments.',\n",
              " 'Emergence of Structured Behaviors from Curiosity-Based Intrinsic\\n  Motivation\\nInfants are experts at playing, with an amazing ability to generate novel\\nstructured behaviors in unstructured environments that lack clear extrinsic\\nreward signals. We seek to replicate some of these abilities with a neural\\nnetwork that implements curiosity-driven intrinsic motivation. Using a simple\\nbut ecologically naturalistic simulated environment in which the agent can move\\nand interact with objects it sees, the agent learns a world model predicting\\nthe dynamic consequences of its actions. Simultaneously, the agent learns to\\ntake actions that adversarially challenge the developing world model, pushing\\nthe agent to explore novel and informative interactions with its environment.\\nWe demonstrate that this policy leads to the self-supervised emergence of a\\nspectrum of complex behaviors, including ego motion prediction, object\\nattention, and object gathering. Moreover, the world model that the agent\\nlearns supports improved performance on object dynamics prediction and\\nlocalization tasks. Our results are a proof-of-principle that computational\\nmodels of intrinsic motivation might account for key features of developmental\\nvisuomotor learning in infants.',\n",
              " 'Stochastic Video Generation with a Learned Prior\\nGenerating video frames that accurately predict future world states is\\nchallenging. Existing approaches either fail to capture the full distribution\\nof outcomes, or yield blurry generations, or both. In this paper we introduce\\nan unsupervised video generation model that learns a prior model of uncertainty\\nin a given environment. Video frames are generated by drawing samples from this\\nprior and combining them with a deterministic estimate of the future frame. The\\napproach is simple and easily trained end-to-end on a variety of datasets.\\nSample generations are both varied and sharp, even many frames into the future,\\nand compare favorably to those from existing approaches.',\n",
              " 'Multi-Evidence Filtering and Fusion for Multi-Label Classification,\\n  Object Detection and Semantic Segmentation Based on Weakly Supervised\\n  Learning\\nSupervised object detection and semantic segmentation require object or even\\npixel level annotations. When there exist image level labels only, it is\\nchallenging for weakly supervised algorithms to achieve accurate predictions.\\nThe accuracy achieved by top weakly supervised algorithms is still\\nsignificantly lower than their fully supervised counterparts. In this paper, we\\npropose a novel weakly supervised curriculum learning pipeline for multi-label\\nobject recognition, detection and semantic segmentation. In this pipeline, we\\nfirst obtain intermediate object localization and pixel labeling results for\\nthe training images, and then use such results to train task-specific deep\\nnetworks in a fully supervised manner. The entire process consists of four\\nstages, including object localization in the training images, filtering and\\nfusing object instances, pixel labeling for the training images, and\\ntask-specific network training. To obtain clean object instances in the\\ntraining images, we propose a novel algorithm for filtering, fusing and\\nclassifying object instances collected from multiple solution mechanisms. In\\nthis algorithm, we incorporate both metric learning and density-based\\nclustering to filter detected object instances. Experiments show that our\\nweakly supervised pipeline achieves state-of-the-art results in multi-label\\nimage classification as well as weakly supervised object detection and very\\ncompetitive results in weakly supervised semantic segmentation on MS-COCO,\\nPASCAL VOC 2007 and PASCAL VOC 2012.',\n",
              " 'Neural Networks Should Be Wide Enough to Learn Disconnected Decision\\n  Regions\\nIn the recent literature the important role of depth in deep learning has\\nbeen emphasized. In this paper we argue that sufficient width of a feedforward\\nnetwork is equally important by answering the simple question under which\\nconditions the decision regions of a neural network are connected. It turns out\\nthat for a class of activation functions including leaky ReLU, neural networks\\nhaving a pyramidal structure, that is no layer has more hidden units than the\\ninput dimension, produce necessarily connected decision regions. This implies\\nthat a sufficiently wide layer is necessary to produce disconnected decision\\nregions. We discuss the implications of this result for the construction of\\nneural networks, in particular the relation to the problem of adversarial\\nmanipulation of classifiers.',\n",
              " \"Visual Explanations From Deep 3D Convolutional Neural Networks for\\n  Alzheimer's Disease Classification\\nWe develop three efficient approaches for generating visual explanations from\\n3D convolutional neural networks (3D-CNNs) for Alzheimer's disease\\nclassification. One approach conducts sensitivity analysis on hierarchical 3D\\nimage segmentation, and the other two visualize network activations on a\\nspatial map. Visual checks and a quantitative localization benchmark indicate\\nthat all approaches identify important brain parts for Alzheimer's disease\\ndiagnosis. Comparative analysis show that the sensitivity analysis based\\napproach has difficulty handling loosely distributed cerebral cortex, and\\napproaches based on visualization of activations are constrained by the\\nresolution of the convolutional layer. The complementarity of these methods\\nimproves the understanding of 3D-CNNs in Alzheimer's disease classification\\nfrom different perspectives.\",\n",
              " 'Averaging Weights Leads to Wider Optima and Better Generalization\\nDeep neural networks are typically trained by optimizing a loss function with\\nan SGD variant, in conjunction with a decaying learning rate, until\\nconvergence. We show that simple averaging of multiple points along the\\ntrajectory of SGD, with a cyclical or constant learning rate, leads to better\\ngeneralization than conventional training. We also show that this Stochastic\\nWeight Averaging (SWA) procedure finds much broader optima than SGD, and\\napproximates the recent Fast Geometric Ensembling (FGE) approach with a single\\nmodel. Using SWA we achieve notable improvement in test accuracy over\\nconventional SGD training on a range of state-of-the-art residual networks,\\nPyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and\\nImageNet. In short, SWA is extremely easy to implement, improves\\ngeneralization, and has almost no computational overhead.',\n",
              " 'SENNS: Sparse Extraction Neural NetworkS for Feature Extraction\\nBy drawing on ideas from optimisation theory, artificial neural networks\\n(ANN), graph embeddings and sparse representations, I develop a novel\\ntechnique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at\\naddressing the feature extraction problem. The proposed method uses (preferably\\ndeep) ANNs for projecting input attribute vectors to an output space wherein\\npairwise distances are maximized for vectors belonging to different classes,\\nbut minimized for those belonging to the same class, while simultaneously\\nenforcing sparsity on the ANN outputs. The vectors that result from the\\nprojection can then be used as features in any classifier of choice.\\nMathematically, I formulate the proposed method as the minimisation of an\\nobjective function which can be interpreted, in the ANN output space, as a\\nnegative factor of the sum of the squares of the pair-wise distances between\\noutput vectors belonging to different classes, added to a positive factor of\\nthe sum of squares of the pair-wise distances between output vectors belonging\\nto the same classes, plus sparsity and weight decay terms. To derive an\\nalgorithm for minimizing the objective function via gradient descent, I use the\\nmulti-variate version of the chain rule to obtain the partial derivatives of\\nthe function with respect to ANN weights and biases, and find that each of the\\nrequired partial derivatives can be expressed as a sum of six terms. As it\\nturns out, four of those six terms can be computed using the standard back\\npropagation algorithm; the fifth can be computed via a slight modification of\\nthe standard backpropagation algorithm; while the sixth one can be computed via\\nsimple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora\\nof digits and letters, the CMU PIE database of faces, the MNIST digits\\ndatabase, and other standard machine learning databases.',\n",
              " \"Generative Models and Model Criticism via Optimized Maximum Mean\\n  Discrepancy\\nWe propose a method to optimize the representation and distinguishability of\\nsamples from two probability distributions, by maximizing the estimated power\\nof a statistical test based on the maximum mean discrepancy (MMD). This\\noptimized MMD is applied to the setting of unsupervised learning by generative\\nadversarial networks (GAN), in which a model attempts to generate realistic\\nsamples, and a discriminator attempts to tell these apart from data samples. In\\nthis context, the MMD may be used in two roles: first, as a discriminator,\\neither directly on the samples, or on features of the samples. Second, the MMD\\ncan be used to evaluate the performance of a generative model, by testing the\\nmodel's samples against a reference data set. In the latter role, the optimized\\nMMD is particularly helpful, as it gives an interpretable indication of how the\\nmodel and data distributions differ, even in cases where individual model\\nsamples are not easily distinguished either by eye or by classifier.\",\n",
              " 'Deep Learning Approximation for Stochastic Control Problems\\nMany real world stochastic control problems suffer from the \"curse of\\ndimensionality\". To overcome this difficulty, we develop a deep learning\\napproach that directly solves high-dimensional stochastic control problems\\nbased on Monte-Carlo sampling. We approximate the time-dependent controls as\\nfeedforward neural networks and stack these networks together through model\\ndynamics. The objective function for the control problem plays the role of the\\nloss function for the deep neural network. We test this approach using examples\\nfrom the areas of optimal trading and energy storage. Our results suggest that\\nthe algorithm presented here achieves satisfactory accuracy and at the same\\ntime, can handle rather high dimensional problems.',\n",
              " 'Generating Focussed Molecule Libraries for Drug Discovery with Recurrent\\n  Neural Networks\\nIn de novo drug design, computational strategies are used to generate novel\\nmolecules with good affinity to the desired biological target. In this work, we\\nshow that recurrent neural networks can be trained as generative models for\\nmolecular structures, similar to statistical language models in natural\\nlanguage processing. We demonstrate that the properties of the generated\\nmolecules correlate very well with the properties of the molecules used to\\ntrain the model. In order to enrich libraries with molecules active towards a\\ngiven biological target, we propose to fine-tune the model with small sets of\\nmolecules, which are known to be active against that target.\\n  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test\\nmolecules that medicinal chemists designed, whereas against Plasmodium\\nfalciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled\\nwith a scoring function, our model can perform the complete de novo drug design\\ncycle to generate large sets of novel molecules for drug discovery.',\n",
              " \"Parameter Space Noise for Exploration\\nDeep reinforcement learning (RL) methods generally engage in exploratory\\nbehavior through noise injection in the action space. An alternative is to add\\nnoise directly to the agent's parameters, which can lead to more consistent\\nexploration and a richer set of behaviors. Methods such as evolutionary\\nstrategies use parameter perturbations, but discard all temporal structure in\\nthe process and require significantly more samples. Combining parameter noise\\nwith traditional RL methods allows to combine the best of both worlds. We\\ndemonstrate that both off- and on-policy methods benefit from this approach\\nthrough experimental comparison of DQN, DDPG, and TRPO on high-dimensional\\ndiscrete action environments as well as continuous control tasks. Our results\\nshow that RL with parameter noise learns more efficiently than traditional RL\\nwith action space noise and evolutionary strategies individually.\",\n",
              " 'On The Robustness of a Neural Network\\nWith the development of neural networks based machine learning and their\\nusage in mission critical applications, voices are rising against the\\n\\\\textit{black box} aspect of neural networks as it becomes crucial to\\nunderstand their limits and capabilities. With the rise of neuromorphic\\nhardware, it is even more critical to understand how a neural network, as a\\ndistributed system, tolerates the failures of its computing nodes, neurons, and\\nits communication channels, synapses. Experimentally assessing the robustness\\nof neural networks involves the quixotic venture of testing all the possible\\nfailures, on all the possible inputs, which ultimately hits a combinatorial\\nexplosion for the first, and the impossibility to gather all the possible\\ninputs for the second.\\n  In this paper, we prove an upper bound on the expected error of the output\\nwhen a subset of neurons crashes. This bound involves dependencies on the\\nnetwork parameters that can be seen as being too pessimistic in the average\\ncase. It involves a polynomial dependency on the Lipschitz coefficient of the\\nneurons activation function, and an exponential dependency on the depth of the\\nlayer where a failure occurs. We back up our theoretical results with\\nexperiments illustrating the extent to which our prediction matches the\\ndependencies between the network parameters and robustness. Our results show\\nthat the robustness of neural networks to the average crash can be estimated\\nwithout the need to neither test the network on all failure configurations, nor\\naccess the training set used to train the network, both of which are\\npractically impossible requirements.',\n",
              " 'ZhuSuan: A Library for Bayesian Deep Learning\\nIn this paper we introduce ZhuSuan, a python probabilistic programming\\nlibrary for Bayesian deep learning, which conjoins the complimentary advantages\\nof Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike\\nexisting deep learning libraries, which are mainly designed for deterministic\\nneural networks and supervised tasks, ZhuSuan is featured for its deep root\\ninto Bayesian inference, thus supporting various kinds of probabilistic models,\\nincluding both the traditional hierarchical Bayesian models and recent deep\\ngenerative models. We use running examples to illustrate the probabilistic\\nprogramming on ZhuSuan, including Bayesian logistic regression, variational\\nauto-encoders, deep sigmoid belief networks and Bayesian recurrent neural\\nnetworks.',\n",
              " 'Using Parameterized Black-Box Priors to Scale Up Model-Based Policy\\n  Search for Robotics\\nThe most data-efficient algorithms for reinforcement learning in robotics are\\nmodel-based policy search algorithms, which alternate between learning a\\ndynamical model of the robot and optimizing a policy to maximize the expected\\nreturn given the model and its uncertainties. Among the few proposed\\napproaches, the recently introduced Black-DROPS algorithm exploits a black-box\\noptimization algorithm to achieve both high data-efficiency and good\\ncomputation times when several cores are used; nevertheless, like all\\nmodel-based policy search approaches, Black-DROPS does not scale to high\\ndimensional state/action spaces. In this paper, we introduce a new model\\nlearning procedure in Black-DROPS that leverages parameterized black-box priors\\nto (1) scale up to high-dimensional systems, and (2) be robust to large\\ninaccuracies of the prior information. We demonstrate the effectiveness of our\\napproach with the \"pendubot\" swing-up task in simulation and with a physical\\nhexapod robot (48D state space, 18D action space) that has to walk forward as\\nfast as possible. The results show that our new algorithm is more\\ndata-efficient than previous model-based policy search algorithms (with and\\nwithout priors) and that it can allow a physical 6-legged robot to learn new\\ngaits in only 16 to 30 seconds of interaction time.',\n",
              " 'Bayesian Optimization with Automatic Prior Selection for Data-Efficient\\n  Direct Policy Search\\nOne of the most interesting features of Bayesian optimization for direct\\npolicy search is that it can leverage priors (e.g., from simulation or from\\nprevious tasks) to accelerate learning on a robot. In this paper, we are\\ninterested in situations for which several priors exist but we do not know in\\nadvance which one fits best the current situation. We tackle this problem by\\nintroducing a novel acquisition function, called Most Likely Expected\\nImprovement (MLEI), that combines the likelihood of the priors and the expected\\nimprovement. We evaluate this new acquisition function on a transfer learning\\ntask for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has\\nto learn to walk on flat ground and on stairs, with priors corresponding to\\ndifferent stairs and different kinds of damages. Our results show that MLEI\\neffectively identifies and exploits the priors, even when there is no obvious\\nmatch between the current situations and the priors.',\n",
              " 'Bounding and Counting Linear Regions of Deep Neural Networks\\nIn this paper, we study the representational power of deep neural networks\\n(DNN) that belong to the family of piecewise-linear (PWL) functions, based on\\nPWL activation units such as rectifier or maxout. We investigate the complexity\\nof such networks by studying the number of linear regions of the PWL function.\\nTypically, a PWL function from a DNN can be seen as a large family of linear\\nfunctions acting on millions of such regions. We directly build upon the work\\nof Montufar et al. (2014), Montufar (2017) and Raghu et al. (2017) by refining\\nthe upper and lower bounds on the number of linear regions for rectified and\\nmaxout networks. In addition to achieving tighter bounds, we also develop a\\nnovel method to perform exact enumeration or counting of the number of linear\\nregions with a mixed-integer linear formulation that maps the input space to\\noutput. We use this new capability to visualize how the number of linear\\nregions change while training DNNs.',\n",
              " 'Deep Rewiring: Training very sparse deep networks\\nNeuromorphic hardware tends to pose limits on the connectivity of deep\\nnetworks that one can run on them. But also generic hardware and software\\nimplementations of deep learning run more efficiently for sparse networks.\\nSeveral methods exist for pruning connections of a neural network after it was\\ntrained without connectivity constraints. We present an algorithm, DEEP R, that\\nenables us to train directly a sparsely connected neural network. DEEP R\\nautomatically rewires the network during supervised training so that\\nconnections are there where they are most needed for the task, while its total\\nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used\\nto train very sparse feedforward and recurrent neural networks on standard\\nbenchmark tasks with just a minor loss in performance. DEEP R is based on a\\nrigorous theoretical foundation that views rewiring as stochastic sampling of\\nnetwork configurations from a posterior.',\n",
              " 'Comparing heterogeneous entities using artificial neural networks of\\n  trainable weighted structural components and machine-learned activation\\n  functions\\nTo compare entities of differing types and structural components, the\\nartificial neural network paradigm was used to cross-compare structural\\ncomponents between heterogeneous documents. Trainable weighted structural\\ncomponents were input into machine-learned activation functions of the neurons.\\nThe model was used for matching news articles and videos, where the inputs and\\nactivation functions respectively consisted of term vectors and cosine\\nsimilarity measures between the weighted structural components. The model was\\ntested with different weights, achieving as high as 59.2% accuracy for matching\\nvideos to news articles. A mobile application user interface for recommending\\nrelated videos for news articles was developed to demonstrate consumer value,\\nincluding its potential usefulness for cross-selling products from unrelated\\ncategories.',\n",
              " 'Active Learning of Inverse Models with Intrinsically Motivated Goal\\n  Exploration in Robots\\nWe introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive\\nCuriosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal\\nexploration mechanism which allows active learning of inverse models in\\nhigh-dimensional redundant robots. This allows a robot to efficiently and\\nactively learn distributions of parameterized motor skills/policies that solve\\na corresponding distribution of parameterized tasks/goals. The architecture\\nmakes the robot sample actively novel parameterized tasks in the task space,\\nbased on a measure of competence progress, each of which triggers low-level\\ngoal-directed learning of the motor policy pa- rameters that allow to solve it.\\nFor both learning and generalization, the system leverages regression\\ntechniques which allow to infer the motor policy parameters corresponding to a\\ngiven novel parameterized task, and based on the previously learnt\\ncorrespondences between policy and task parameters. We present experiments with\\nhigh-dimensional continuous sensorimotor spaces in three different robotic\\nsetups: 1) learning the inverse kinematics in a highly-redundant robotic arm,\\n2) learning omnidirectional locomotion with motor primitives in a quadruped\\nrobot, 3) an arm learning to control a fishing rod with a flexible wire. We\\nshow that 1) exploration in the task space can be a lot faster than exploration\\nin the actuator space for learning inverse models in redundant robots; 2)\\nselecting goals maximizing competence progress creates developmental\\ntrajectories driving the robot to progressively focus on tasks of increasing\\ncomplexity and is statistically significantly more efficient than selecting\\ntasks randomly, as well as more efficient than different standard active motor\\nbabbling methods; 3) this architecture allows the robot to actively discover\\nwhich parts of its task space it can learn to reach and which part it cannot.',\n",
              " \"End-to-End Tracking and Semantic Segmentation Using Recurrent Neural\\n  Networks\\nIn this work we present a novel end-to-end framework for tracking and\\nclassifying a robot's surroundings in complex, dynamic and only partially\\nobservable real-world environments. The approach deploys a recurrent neural\\nnetwork to filter an input stream of raw laser measurements in order to\\ndirectly infer object locations, along with their identity in both visible and\\noccluded areas. To achieve this we first train the network using unsupervised\\nDeep Tracking, a recently proposed theoretical framework for end-to-end space\\noccupancy prediction. We show that by learning to track on a large amount of\\nunsupervised data, the network creates a rich internal representation of its\\nenvironment which we in turn exploit through the principle of inductive\\ntransfer of knowledge to perform the task of it's semantic classification. As a\\nresult, we show that only a small amount of labelled data suffices to steer the\\nnetwork towards mastering this additional task. Furthermore we propose a novel\\nrecurrent neural network architecture specifically tailored to tracking and\\nsemantic classification in real-world robotics applications. We demonstrate the\\ntracking and classification performance of the method on real-world data\\ncollected at a busy road junction. Our evaluation shows that the proposed\\nend-to-end framework compares favourably to a state-of-the-art, model-free\\ntracking solution and that it outperforms a conventional one-shot training\\nscheme for semantic classification.\",\n",
              " 'Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks\\nThis paper presents to the best of our knowledge the first end-to-end object\\ntracking approach which directly maps from raw sensor input to object tracks in\\nsensor space without requiring any feature engineering or system identification\\nin the form of plant or sensor models. Specifically, our system accepts a\\nstream of raw sensor data at one end and, in real-time, produces an estimate of\\nthe entire environment state at the output including even occluded objects. We\\nachieve this by framing the problem as a deep learning task and exploit\\nsequence models in the form of recurrent neural networks to learn a mapping\\nfrom sensor measurements to object tracks. In particular, we propose a learning\\nmethod based on a form of input dropout which allows learning in an\\nunsupervised manner, only based on raw, occluded sensor data without access to\\nground-truth annotations. We demonstrate our approach using a synthetic dataset\\ndesigned to mimic the task of tracking objects in 2D laser data -- as commonly\\nencountered in robotics applications -- and show that it learns to track many\\ndynamic objects despite occlusions and the presence of sensor noise.',\n",
              " 'Deep Predictive Coding Networks for Video Prediction and Unsupervised\\n  Learning\\nWhile great strides have been made in using deep learning algorithms to solve\\nsupervised learning tasks, the problem of unsupervised learning - leveraging\\nunlabeled examples to learn about the structure of a domain - remains a\\ndifficult unsolved challenge. Here, we explore prediction of future frames in a\\nvideo sequence as an unsupervised learning rule for learning about the\\nstructure of the visual world. We describe a predictive neural network\\n(\"PredNet\") architecture that is inspired by the concept of \"predictive coding\"\\nfrom the neuroscience literature. These networks learn to predict future frames\\nin a video sequence, with each layer in the network making local predictions\\nand only forwarding deviations from those predictions to subsequent network\\nlayers. We show that these networks are able to robustly learn to predict the\\nmovement of synthetic (rendered) objects, and that in doing so, the networks\\nlearn internal representations that are useful for decoding latent object\\nparameters (e.g. pose) that support object recognition with fewer training\\nviews. We also show that these networks can scale to complex natural image\\nstreams (car-mounted camera videos), capturing key aspects of both egocentric\\nmovement and the movement of objects in the visual scene, and the\\nrepresentation learned in this setting is useful for estimating the steering\\nangle. Altogether, these results suggest that prediction represents a powerful\\nframework for unsupervised learning, allowing for implicit learning of object\\nand scene structure.',\n",
              " 'Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient\\n  Convolutional Neural Networks\\nThis paper proposes a computationally efficient approach to detecting objects\\nnatively in 3D point clouds using convolutional neural networks (CNNs). In\\nparticular, this is achieved by leveraging a feature-centric voting scheme to\\nimplement novel convolutional layers which explicitly exploit the sparsity\\nencountered in the input. To this end, we examine the trade-off between\\naccuracy and speed for different architectures and additionally propose to use\\nan L1 penalty on the filter activations to further encourage sparsity in the\\nintermediate representations. To the best of our knowledge, this is the first\\nwork to propose sparse convolutional layers and L1 regularisation for efficient\\nlarge-scale processing of 3D data. We demonstrate the efficacy of our approach\\non the KITTI object detection benchmark and show that Vote3Deep models with as\\nfew as three layers outperform the previous state of the art in both laser and\\nlaser-vision based approaches by margins of up to 40% while remaining highly\\ncompetitive in terms of processing time.',\n",
              " 'On Convergence and Stability of GANs\\nWe propose studying GAN training dynamics as regret minimization, which is in\\ncontrast to the popular view that there is consistent minimization of a\\ndivergence between real and generated distributions. We analyze the convergence\\nof GAN training from this new point of view to understand why mode collapse\\nhappens. We hypothesize the existence of undesirable local equilibria in this\\nnon-convex game to be responsible for mode collapse. We observe that these\\nlocal equilibria often exhibit sharp gradients of the discriminator function\\naround some real data points. We demonstrate that these degenerate local\\nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show\\nthat DRAGAN enables faster training, achieves improved stability with fewer\\nmode collapses, and leads to generator networks with better modeling\\nperformance across a variety of architectures and objective functions.',\n",
              " 'Imitation from Observation: Learning to Imitate Behaviors from Raw Video\\n  via Context Translation\\nImitation learning is an effective approach for autonomous systems to acquire\\ncontrol policies when an explicit reward function is unavailable, using\\nsupervision provided as demonstrations from an expert, typically a human\\noperator. However, standard imitation learning methods assume that the agent\\nreceives examples of observation-action tuples that could be provided, for\\ninstance, to a supervised learning algorithm. This stands in contrast to how\\nhumans and animals imitate: we observe another person performing some behavior\\nand then figure out which actions will realize that behavior, compensating for\\nchanges in viewpoint, surroundings, and embodiment. We term this kind of\\nimitation learning as imitation-from-observation and propose an imitation\\nlearning method based on video prediction with context translation and deep\\nreinforcement learning. This lifts the assumption in imitation learning that\\nthe demonstration should consist of observations and actions in the same\\nenvironment, and enables a variety of interesting applications, including\\nlearning robotic skills that involve tool use simply by observing videos of\\nhuman tool use. Our experimental results show that our approach can perform\\nimitation-from-observation for a variety of real-world robotic tasks modeled on\\ncommon household chores, acquiring skills such as sweeping from videos of a\\nhuman demonstrator. Videos can be found at\\nhttps://sites.google.com/site/imitationfromobservation',\n",
              " 'Convergence rates for pretraining and dropout: Guiding learning\\n  parameters using network structure\\nUnsupervised pretraining and dropout have been well studied, especially with\\nrespect to regularization and output consistency. However, our understanding\\nabout the explicit convergence rates of the parameter estimates, and their\\ndependence on the learning (like denoising and dropout rate) and structural\\n(like depth and layer lengths) aspects of the network is less mature. An\\ninteresting question in this context is to ask if the network structure could\\n\"guide\" the choices of such learning parameters. In this work, we explore these\\ngaps between network structure, the learning mechanisms and their interaction\\nwith parameter convergence rates. We present a way to address these issues\\nbased on the backpropagation convergence rates for general nonconvex objectives\\nusing first-order information. We then incorporate two learning mechanisms into\\nthis general framework -- denoising autoencoder and dropout, and subsequently\\nderive the convergence rates of deep networks. Building upon these bounds, we\\nprovide insights into the choices of learning parameters and network sizes that\\nachieve certain levels of convergence accuracy. The results derived here\\nsupport existing empirical observations, and we also conduct a set of\\nexperiments to evaluate them.',\n",
              " 'Learning Discriminative Features via Label Consistent Neural Network\\nDeep Convolutional Neural Networks (CNN) enforces supervised information only\\nat the output layer, and hidden layers are trained by back propagating the\\nprediction error from the output layer without explicit supervision. We propose\\na supervised feature learning approach, Label Consistent Neural Network, which\\nenforces direct supervision in late hidden layers. We associate each neuron in\\na hidden layer with a particular class label and encourage it to be activated\\nfor input signals from the same class. More specifically, we introduce a label\\nconsistency regularization called \"discriminative representation error\" loss\\nfor late hidden layers and combine it with classification error loss to build\\nour overall objective function. This label consistency constraint alleviates\\nthe common problem of gradient vanishing and tends to faster convergence; it\\nalso makes the features derived from late hidden layers discriminative enough\\nfor classification even using a simple $k$-NN classifier, since input signals\\nfrom the same class will have very similar representations. Experimental\\nresults demonstrate that our approach achieves state-of-the-art performances on\\nseveral public benchmarks for action and object category recognition.',\n",
              " 'Out-of-Sample Extension for Dimensionality Reduction of Noisy Time\\n  Series\\nThis paper proposes an out-of-sample extension framework for a global\\nmanifold learning algorithm (Isomap) that uses temporal information in\\nout-of-sample points in order to make the embedding more robust to noise and\\nartifacts. Given a set of noise-free training data and its embedding, the\\nproposed framework extends the embedding for a noisy time series. This is\\nachieved by adding a spatio-temporal compactness term to the optimization\\nobjective of the embedding. To the best of our knowledge, this is the first\\nmethod for out-of-sample extension of manifold embeddings that leverages timing\\ninformation available for the extension set. Experimental results demonstrate\\nthat our out-of-sample extension algorithm renders a more robust and accurate\\nembedding of sequentially ordered image data in the presence of various noise\\nand artifacts when compared to other timing-aware embeddings. Additionally, we\\nshow that an out-of-sample extension framework based on the proposed algorithm\\noutperforms the state of the art in eye-gaze estimation.',\n",
              " 'Adversarial Examples for Semantic Image Segmentation\\nMachine learning methods in general and Deep Neural Networks in particular\\nhave shown to be vulnerable to adversarial perturbations. So far this\\nphenomenon has mainly been studied in the context of whole-image\\nclassification. In this contribution, we analyse how adversarial perturbations\\ncan affect the task of semantic segmentation. We show how existing adversarial\\nattackers can be transferred to this task and that it is possible to create\\nimperceptible adversarial perturbations that lead a deep network to misclassify\\nalmost all pixels of a chosen class while leaving network prediction nearly\\nunchanged outside this class.',\n",
              " 'Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box\\n  Machine Learning Models\\nMany machine learning algorithms are vulnerable to almost imperceptible\\nperturbations of their inputs. So far it was unclear how much risk adversarial\\nperturbations carry for the safety of real-world machine learning applications\\nbecause most methods used to generate such perturbations rely either on\\ndetailed model information (gradient-based attacks) or on confidence scores\\nsuch as class probabilities (score-based attacks), neither of which are\\navailable in most real-world scenarios. In many such cases one currently needs\\nto retreat to transfer-based attacks which rely on cumbersome substitute\\nmodels, need access to the training data and can be defended against. Here we\\nemphasise the importance of attacks which solely rely on the final model\\ndecision. Such decision-based attacks are (1) applicable to real-world\\nblack-box models such as autonomous cars, (2) need less knowledge and are\\neasier to apply than transfer-based attacks and (3) are more robust to simple\\ndefences than gradient- or score-based attacks. Previous attacks in this\\ncategory were limited to simple models or simple datasets. Here we introduce\\nthe Boundary Attack, a decision-based attack that starts from a large\\nadversarial perturbation and then seeks to reduce the perturbation while\\nstaying adversarial. The attack is conceptually simple, requires close to no\\nhyperparameter tuning, does not rely on substitute models and is competitive\\nwith the best gradient-based attacks in standard computer vision tasks like\\nImageNet. We apply the attack on two black-box algorithms from Clarifai.com.\\nThe Boundary Attack in particular and the class of decision-based attacks in\\ngeneral open new avenues to study the robustness of machine learning models and\\nraise new questions regarding the safety of deployed machine learning systems.\\nAn implementation of the attack is available as part of Foolbox at\\nhttps://github.com/bethgelab/foolbox .',\n",
              " 'Towards Building an Intelligent Anti-Malware System: A Deep Learning\\n  Approach using Support Vector Machine (SVM) for Malware Classification\\nEffective and efficient mitigation of malware is a long-time endeavor in the\\ninformation security community. The development of an anti-malware system that\\ncan counteract an unknown malware is a prolific activity that may benefit\\nseveral sectors. We envision an intelligent anti-malware system that utilizes\\nthe power of deep learning (DL) models. Using such models would enable the\\ndetection of newly-released malware through mathematical generalization. That\\nis, finding the relationship between a given malware $x$ and its corresponding\\nmalware family $y$, $f: x \\\\mapsto y$. To accomplish this feat, we used the\\nMalimg dataset (Nataraj et al., 2011) which consists of malware images that\\nwere processed from malware binaries, and then we trained the following DL\\nmodels 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM\\n(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM\\nstands out among the DL models with a predictive accuracy of ~84.92%. This\\nstands to reason for the mentioned model had the relatively most sophisticated\\narchitecture design among the presented models. The exploration of an even more\\noptimal DL-SVM model is the next stage towards the engineering of an\\nintelligent anti-malware system.',\n",
              " 'Feature extraction using Latent Dirichlet Allocation and Neural\\n  Networks: A case study on movie synopses\\nFeature extraction has gained increasing attention in the field of machine\\nlearning, as in order to detect patterns, extract information, or predict\\nfuture observations from big data, the urge of informative features is crucial.\\nThe process of extracting features is highly linked to dimensionality reduction\\nas it implies the transformation of the data from a sparse high-dimensional\\nspace, to higher level meaningful abstractions. This dissertation employs\\nNeural Networks for distributed paragraph representations, and Latent Dirichlet\\nAllocation to capture higher level features of paragraph vectors. Although\\nNeural Networks for distributed paragraph representations are considered the\\nstate of the art for extracting paragraph vectors, we show that a quick topic\\nanalysis model such as Latent Dirichlet Allocation can provide meaningful\\nfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, a\\ncollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,\\nfor both approaches, we use K-Nearest Neighbors to discover similar movies, and\\nplot the projected representations using T-Distributed Stochastic Neighbor\\nEmbedding to depict the context similarities. These similarities, expressed as\\nmovie distances, can be used for movies recommendation. The recommended movies\\nof this approach are compared with the recommended movies from IMDB, which use\\na collaborative filtering recommendation approach, to show that our two models\\ncould constitute either an alternative or a supplementary recommendation\\napproach.',\n",
              " 'A Survey of Available Corpora for Building Data-Driven Dialogue Systems\\nDuring the past decade, several areas of speech and language understanding\\nhave witnessed substantial breakthroughs from the use of data-driven models. In\\nthe area of dialogue systems, the trend is less obvious, and most practical\\nsystems are still built through significant engineering and expert knowledge.\\nNevertheless, several recent results suggest that data-driven approaches are\\nfeasible and quite promising. To facilitate research in this area, we have\\ncarried out a wide survey of publicly available datasets suitable for\\ndata-driven learning of dialogue systems. We discuss important characteristics\\nof these datasets, how they can be used to learn diverse dialogue strategies,\\nand their other potential uses. We also examine methods for transfer learning\\nbetween datasets and the use of external knowledge. Finally, we discuss\\nappropriate choice of evaluation metrics for the learning objective.',\n",
              " 'Generative Topic Embedding: a Continuous Representation of Documents\\n  (Extended Version with Proofs)\\nWord embedding maps words into a low-dimensional continuous embedding space\\nby exploiting the local word collocation patterns in a small context window. On\\nthe other hand, topic modeling maps documents onto a low-dimensional topic\\nspace, by utilizing the global word collocation patterns in the same document.\\nThese two types of patterns are complementary. In this paper, we propose a\\ngenerative topic embedding model to combine the two types of patterns. In our\\nmodel, topics are represented by embedding vectors, and are shared across\\ndocuments. The probability of each word is influenced by both its local context\\nand its topic. A variational inference method yields the topic embeddings as\\nwell as the topic mixing proportions for each document. Jointly they represent\\nthe document in a low-dimensional continuous space. In two document\\nclassification tasks, our method performs better than eight existing methods,\\nwith fewer features. In addition, we illustrate with an example that our method\\ncan generate coherent topics even based on only one document.',\n",
              " 'Fine-Grained Entity Typing with High-Multiplicity Assignments\\nAs entity type systems become richer and more fine-grained, we expect the\\nnumber of types assigned to a given entity to increase. However, most\\nfine-grained typing work has focused on datasets that exhibit a low degree of\\ntype multiplicity. In this paper, we consider the high-multiplicity regime\\ninherent in data sources such as Wikipedia that have semi-open type systems. We\\nintroduce a set-prediction approach to this problem and show that our model\\noutperforms unstructured baselines on a new Wikipedia-based fine-grained typing\\ncorpus.',\n",
              " \"Towards a Visual Turing Challenge\\nAs language and visual understanding by machines progresses rapidly, we are\\nobserving an increasing interest in holistic architectures that tightly\\ninterlink both modalities in a joint learning and inference process. This trend\\nhas allowed the community to progress towards more challenging and open tasks\\nand refueled the hope at achieving the old AI dream of building machines that\\ncould pass a turing test in open domains. In order to steadily make progress\\ntowards this goal, we realize that quantifying performance becomes increasingly\\ndifficult. Therefore we ask how we can precisely define such challenges and how\\nwe can evaluate different algorithms on this open tasks? In this paper, we\\nsummarize and discuss such challenges as well as try to give answers where\\nappropriate options are available in the literature. We exemplify some of the\\nsolutions on a recently presented dataset of question-answering task based on\\nreal-world indoor images that establishes a visual turing challenge. Finally,\\nwe argue despite the success of unique ground-truth annotation, we likely have\\nto step away from carefully curated dataset and rather rely on 'social\\nconsensus' as the main driving force to create suitable benchmarks. Providing\\ncoverage in this inherently ambiguous output space is an emerging challenge\\nthat we face in order to make quantifiable progress in this area.\",\n",
              " \"Interactive Robot Learning of Gestures, Language and Affordances\\nA growing field in robotics and Artificial Intelligence (AI) research is\\nhuman-robot collaboration, whose target is to enable effective teamwork between\\nhumans and robots. However, in many situations human teams are still superior\\nto human-robot teams, primarily because human teams can easily agree on a\\ncommon goal with language, and the individual members observe each other\\neffectively, leveraging their shared motor repertoire and sensorimotor\\nresources. This paper shows that for cognitive robots it is possible, and\\nindeed fruitful, to combine knowledge acquired from interacting with elements\\nof the environment (affordance exploration) with the probabilistic observation\\nof another agent's actions.\\n  We propose a model that unites (i) learning robot affordances and word\\ndescriptions with (ii) statistical recognition of human gestures with vision\\nsensors. We discuss theoretical motivations, possible implementations, and we\\nshow initial results which highlight that, after having acquired knowledge of\\nits surrounding environment, a humanoid robot can generalize this knowledge to\\nthe case when it observes another agent (human partner) performing the same\\nmotor actions previously executed during training.\",\n",
              " 'Visual Features for Context-Aware Speech Recognition\\nAutomatic transcriptions of consumer-generated multi-media content such as\\n\"Youtube\" videos still exhibit high word error rates. Such data typically\\noccupies a very broad domain, has been recorded in challenging conditions, with\\ncheap hardware and a focus on the visual modality, and may have been\\npost-processed or edited. In this paper, we extend our earlier work on adapting\\nthe acoustic model of a DNN-based speech recognition system to an RNN language\\nmodel and show how both can be adapted to the objects and scenes that can be\\nautomatically detected in the video. We are working on a corpus of \"how-to\"\\nvideos from the web, and the idea is that an object that can be seen (\"car\"),\\nor a scene that is being detected (\"kitchen\") can be used to condition both\\nmodels on the \"context\" of the recording, thereby reducing perplexity and\\nimproving transcription. We achieve good improvements in both cases and compare\\nand analyze the respective reductions in word error rate. We expect that our\\nresults can be used for any type of speech processing in which \"context\"\\ninformation is available, for example in robotics, man-machine interaction, or\\nwhen indexing large audio-visual archives, and should ultimately help to bring\\ntogether the \"video-to-text\" and \"speech-to-text\" communities.',\n",
              " 'Examining Cooperation in Visual Dialog Models\\nIn this work we propose a blackbox intervention method for visual dialog\\nmodels, with the aim of assessing the contribution of individual linguistic or\\nvisual components. Concretely, we conduct structured or randomized\\ninterventions that aim to impair an individual component of the model, and\\nobserve changes in task performance. We reproduce a state-of-the-art visual\\ndialog model and demonstrate that our methodology yields surprising insights,\\nnamely that both dialog and image information have minimal contributions to\\ntask performance. The intervention method presented here can be applied as a\\nsanity check for the strength and robustness of each component in visual dialog\\nsystems.',\n",
              " 'Video Highlight Prediction Using Audience Chat Reactions\\nSports channel video portals offer an exciting domain for research on\\nmultimodal, multilingual analysis. We present methods addressing the problem of\\nautomatic video highlight prediction based on joint visual features and textual\\nanalysis of the real-world audience discourse with complex slang, in both\\nEnglish and traditional Chinese. We present a novel dataset based on League of\\nLegends championships recorded from North American and Taiwanese Twitch.tv\\nchannels (will be released for further research), and demonstrate strong\\nresults on these using multimodal, character-level CNN-RNN model architectures.',\n",
              " 'Invariant Representations for Noisy Speech Recognition\\nModern automatic speech recognition (ASR) systems need to be robust under\\nacoustic variability arising from environmental, speaker, channel, and\\nrecording conditions. Ensuring such robustness to variability is a challenge in\\nmodern day neural network-based ASR systems, especially when all types of\\nvariability are not seen during training. We attempt to address this problem by\\nencouraging the neural network acoustic model to learn invariant feature\\nrepresentations. We use ideas from recent research on image generation using\\nGenerative Adversarial Networks and domain adaptation ideas extending\\nadversarial gradient-based training. A recent work from Ganin et al. proposes\\nto use adversarial training for image domain adaptation by using an\\nintermediate representation from the main target classification network to\\ndeteriorate the domain classifier performance through a separate neural\\nnetwork. Our work focuses on investigating neural architectures which produce\\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\\nshow that our method generalizes better than the standard multi-condition\\ntraining especially when only a few noise categories are seen during training.',\n",
              " 'Self-Supervised Vision-Based Detection of the Active Speaker as a\\n  Prerequisite for Socially-Aware Language Acquisition\\nThis paper presents a self-supervised method for detecting the active speaker\\nin a multi-person spoken interaction scenario. We argue that this capability is\\na fundamental prerequisite for any artificial cognitive system attempting to\\nacquire language in social settings. Our methods are able to detect an\\narbitrary number of possibly overlapping active speakers based exclusively on\\nvisual information about their face. Our methods do not rely on external\\nannotations, thus complying with cognitive development. Instead, they use\\ninformation from the auditory modality to support learning in the visual\\ndomain. The methods have been extensively evaluated on a large multi-person\\nface-to-face interaction dataset. The results reach an accuracy of 80% on a\\nmulti-speaker setting. We believe this system represents an essential component\\nof any artificial cognitive system or robotic platform engaging in social\\ninteraction.',\n",
              " \"Product Characterisation towards Personalisation: Learning Attributes\\n  from Unstructured Data to Recommend Fashion Products\\nIn this paper, we describe a solution to tackle a common set of challenges in\\ne-commerce, which arise from the fact that new products are continually being\\nadded to the catalogue. The challenges involve properly personalising the\\ncustomer experience, forecasting demand and planning the product range. We\\nargue that the foundational piece to solve all of these problems is having\\nconsistent and detailed information about each product, information that is\\nrarely available or consistent given the multitude of suppliers and types of\\nproducts. We describe in detail the architecture and methodology implemented at\\nASOS, one of the world's largest fashion e-commerce retailers, to tackle this\\nproblem. We then show how this quantitative understanding of the products can\\nbe leveraged to improve recommendations in a hybrid recommender system\\napproach.\",\n",
              " 'The Self-Organization of Speech Sounds\\nThe speech code is a vehicle of language: it defines a set of forms used by a\\ncommunity to carry information. Such a code is necessary to support the\\nlinguistic interactions that allow humans to communicate. How then may a speech\\ncode be formed prior to the existence of linguistic interactions? Moreover, the\\nhuman speech code is discrete and compositional, shared by all the individuals\\nof a community but different across communities, and phoneme inventories are\\ncharacterized by statistical regularities. How can a speech code with these\\nproperties form? We try to approach these questions in the paper, using the\\n\"methodology of the artificial\". We build a society of artificial agents, and\\ndetail a mechanism that shows the formation of a discrete speech code without\\npre-supposing the existence of linguistic capacities or of coordinated\\ninteractions. The mechanism is based on a low-level model of sensory-motor\\ninteractions. We show that the integration of certain very simple and non\\nlanguage-specific neural devices leads to the formation of a speech code that\\nhas properties similar to the human speech code. This result relies on the\\nself-organizing properties of a generic coupling between perception and\\nproduction within agents, and on the interactions between agents. The\\nartificial system helps us to develop better intuitions on how speech might\\nhave appeared, by showing how self-organization might have helped natural\\nselection to find speech.',\n",
              " \"What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes\\nThe F-measure or F-score is one of the most commonly used single number\\nmeasures in Information Retrieval, Natural Language Processing and Machine\\nLearning, but it is based on a mistake, and the flawed assumptions render it\\nunsuitable for use in most contexts! Fortunately, there are better\\nalternatives.\",\n",
              " 'A Machine Learning Perspective on Predictive Coding with PAQ\\nPAQ8 is an open source lossless data compression algorithm that currently\\nachieves the best compression rates on many benchmarks. This report presents a\\ndetailed description of PAQ8 from a statistical machine learning perspective.\\nIt shows that it is possible to understand some of the modules of PAQ8 and use\\nthis understanding to improve the method. However, intuitive statistical\\nexplanations of the behavior of other modules remain elusive. We hope the\\ndescription in this report will be a starting point for discussions that will\\nincrease our understanding, lead to improvements to PAQ8, and facilitate a\\ntransfer of knowledge from PAQ8 to other machine learning methods, such a\\nrecurrent neural networks and stochastic memoizers. Finally, the report\\npresents a broad range of new applications of PAQ to machine learning tasks\\nincluding language modeling and adaptive text prediction, adaptive game\\nplaying, classification, and compression using features from the field of deep\\nlearning.',\n",
              " 'A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale\\n  SVM Training\\nRecently, there has been a renewed interest in the machine learning community\\nfor variants of a sparse greedy approximation procedure for concave\\noptimization known as {the Frank-Wolfe (FW) method}. In particular, this\\nprocedure has been successfully applied to train large-scale instances of\\nnon-linear Support Vector Machines (SVMs). Specializing FW to SVM training has\\nallowed to obtain efficient algorithms but also important theoretical results,\\nincluding convergence analysis of training algorithms and new characterizations\\nof model sparsity.\\n  In this paper, we present and analyze a novel variant of the FW method based\\non a new way to perform away steps, a classic strategy used to accelerate the\\nconvergence of the basic FW procedure. Our formulation and analysis is focused\\non a general concave maximization problem on the simplex. However, the\\nspecialization of our algorithm to quadratic forms is strongly related to some\\nclassic methods in computational geometry, namely the Gilbert and MDM\\nalgorithms.\\n  On the theoretical side, we demonstrate that the method matches the\\nguarantees in terms of convergence rate and number of iterations obtained by\\nusing classic away steps. In particular, the method enjoys a linear rate of\\nconvergence, a result that has been recently proved for MDM on quadratic forms.\\n  On the practical side, we provide experiments on several classification\\ndatasets, and evaluate the results using statistical tests. Experiments show\\nthat our method is faster than the FW method with classic away steps, and works\\nwell even in the cases in which classic away steps slow down the algorithm.\\nFurthermore, these improvements are obtained without sacrificing the predictive\\naccuracy of the obtained SVM model.',\n",
              " 'Semi-supervised Vocabulary-informed Learning\\nDespite significant progress in object categorization, in recent years, a\\nnumber of important challenges remain, mainly, ability to learn from limited\\nlabeled data and ability to recognize object classes within large, potentially\\nopen, set of labels. Zero-shot learning is one way of addressing these\\nchallenges, but it has only been shown to work with limited sized class\\nvocabularies and typically requires separation between supervised and\\nunsupervised classes, allowing former to inform the latter but not vice versa.\\nWe propose the notion of semi-supervised vocabulary-informed learning to\\nalleviate the above mentioned challenges and address problems of supervised,\\nzero-shot and open set recognition using a unified framework. Specifically, we\\npropose a maximum margin framework for semantic manifold-based recognition that\\nincorporates distance constraints from (both supervised and unsupervised)\\nvocabulary atoms, ensuring that labeled samples are projected closest to their\\ncorrect prototypes, in the embedding space, than to others. We show that\\nresulting model shows improvements in supervised, zero-shot, and large open set\\nrecognition, with up to 310K class vocabulary on AwA and ImageNet datasets.',\n",
              " 'Submodular meets Structured: Finding Diverse Subsets in\\n  Exponentially-Large Structured Item Sets\\nTo cope with the high level of ambiguity faced in domains such as Computer\\nVision or Natural Language processing, robust prediction methods often search\\nfor a diverse set of high-quality candidate solutions or proposals. In\\nstructured prediction problems, this becomes a daunting task, as the solution\\nspace (image labelings, sentence parses, etc.) is exponentially large. We study\\ngreedy algorithms for finding a diverse subset of solutions in\\nstructured-output spaces by drawing new connections between submodular\\nfunctions over combinatorial item sets and High-Order Potentials (HOPs) studied\\nfor graphical models. Specifically, we show via examples that when marginal\\ngains of submodular diversity functions allow structured representations, this\\nenables efficient (sub-linear time) approximate maximization by reducing the\\ngreedy augmentation step to inference in a factor graph with appropriately\\nconstructed HOPs. We discuss benefits, tradeoffs, and show that our\\nconstructions lead to significantly better proposals.',\n",
              " \"ZM-Net: Real-time Zero-shot Image Manipulation Network\\nMany problems in image processing and computer vision (e.g. colorization,\\nstyle transfer) can be posed as 'manipulating' an input image into a\\ncorresponding output image given a user-specified guiding signal. A holy-grail\\nsolution towards generic image manipulation should be able to efficiently alter\\nan input image with any personalized signals (even signals unseen during\\ntraining), such as diverse paintings and arbitrary descriptive attributes.\\nHowever, existing methods are either inefficient to simultaneously process\\nmultiple signals (let alone generalize to unseen signals), or unable to handle\\nsignals from other modalities. In this paper, we make the first attempt to\\naddress the zero-shot image manipulation task. We cast this problem as\\nmanipulating an input image according to a parametric model whose key\\nparameters can be conditionally generated from any guiding signal (even unseen\\nones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a\\nfully-differentiable architecture that jointly optimizes an\\nimage-transformation network (TNet) and a parameter network (PNet). The PNet\\nlearns to generate key transformation parameters for the TNet given any guiding\\nsignal while the TNet performs fast zero-shot image manipulation according to\\nboth signal-dependent parameters from the PNet and signal-invariant parameters\\nfrom the TNet itself. Extensive experiments show that our ZM-Net can perform\\nhigh-quality image manipulation conditioned on different forms of guiding\\nsignals (e.g. style images and attributes) in real-time (tens of milliseconds\\nper image) even for unseen signals. Moreover, a large-scale style dataset with\\nover 20,000 style images is also constructed to promote further research.\",\n",
              " \"Multi-Agent Diverse Generative Adversarial Networks\\nWe propose an intuitive generalization to the Generative Adversarial Networks\\n(GANs) and its conditional variants to address the well known mode collapse\\nproblem. Firstly, we propose a multi-agent GAN architecture incorporating\\nmultiple generators and one discriminator. Secondly, to enforce different\\ngenerators to capture diverse high probability modes, we modify discriminator's\\nobjective function where along with finding the real and fake samples, the\\ndiscriminator has to identify the generator that generated the fake sample.\\nIntuitively, to succeed in this task, the discriminator must learn to push\\ndifferent generators towards different identifiable modes. Our framework\\n(MAD-GAN) is generalizable in the sense that it can be easily combined with\\nother existing variants of GANs to produce diverse samples. We perform\\nextensive experiments on synthetic and real datasets and compare MAD-GAN with\\ndifferent variants of GAN. We show high quality diverse sample generations for\\nthe challenging tasks such as image-to-image translation (known to learn delta\\ndistribution) and face generation. In addition, we show that MAD-GAN is able to\\ndisentangle different modalities even when trained using highly challenging\\nmulti-view dataset (mixture of forests, icebergs, bedrooms etc). In the end, we\\nalso show its efficacy for the unsupervised feature representation task. In the\\nappendix we introduce a similarity based competing objective which encourages\\nthe different generators to generate varied samples judged by a user defined\\nsimilarity metric. We show extensive evaluations on a 1-D setting of mixture of\\ngaussians for non parametric density estimation. The theoretical proofs back\\nthe efficacy of the framework and explains why various generators are pushed\\ntowards distinct clusters of modes.\",\n",
              " 'Geometric GAN\\nGenerative Adversarial Nets (GANs) represent an important milestone for\\neffective generative models, which has inspired numerous variants seemingly\\ndifferent from each other. One of the main contributions of this paper is to\\nreveal a unified geometric structure in GAN and its variants. Specifically, we\\nshow that the adversarial generative model training can be decomposed into\\nthree geometric steps: separating hyperplane search, discriminator parameter\\nupdate away from the separating hyperplane, and the generator update along the\\nnormal vector direction of the separating hyperplane. This geometric intuition\\nreveals the limitations of the existing approaches and leads us to propose a\\nnew formulation called geometric GAN using SVM separating hyperplane that\\nmaximizes the margin. Our theoretical analysis shows that the geometric GAN\\nconverges to a Nash equilibrium between the discriminator and generator. In\\naddition, extensive numerical results show that the superior performance of\\ngeometric GAN.',\n",
              " 'A Data and Model-Parallel, Distributed and Scalable Framework for\\n  Training of Deep Networks in Apache Spark\\nTraining deep networks is expensive and time-consuming with the training\\nperiod increasing with data size and growth in model parameters. In this paper,\\nwe provide a framework for distributed training of deep networks over a cluster\\nof CPUs in Apache Spark. The framework implements both Data Parallelism and\\nModel Parallelism making it suitable to use for deep networks which require\\nhuge training data and model parameters which are too big to fit into the\\nmemory of a single machine. It can be scaled easily over a cluster of cheap\\ncommodity hardware to attain significant speedup and obtain better results\\nmaking it quite economical as compared to farm of GPUs and supercomputers. We\\nhave proposed a new algorithm for training of deep networks for the case when\\nthe network is partitioned across the machines (Model Parallelism) along with\\ndetailed cost analysis and proof of convergence of the same. We have developed\\nimplementations for Fully-Connected Feedforward Networks, Convolutional Neural\\nNetworks, Recurrent Neural Networks and Long Short-Term Memory architectures.\\nWe present the results of extensive simulations demonstrating the speedup and\\naccuracy obtained by our framework for different sizes of the data and model\\nparameters with variation in the number of worker cores/partitions; thereby\\nshowing that our proposed framework can achieve significant speedup (upto 11X\\nfor CNN) and is also quite scalable.',\n",
              " \"Understanding and Comparing Deep Neural Networks for Age and Gender\\n  Classification\\nRecently, deep neural networks have demonstrated excellent performances in\\nrecognizing the age and gender on human face images. However, these models were\\napplied in a black-box manner with no information provided about which facial\\nfeatures are actually used for prediction and how these features depend on\\nimage preprocessing, model initialization and architecture choice. We present a\\nstudy investigating these different effects.\\n  In detail, our work compares four popular neural network architectures,\\nstudies the effect of pretraining, evaluates the robustness of the considered\\nalignment preprocessings via cross-method test set swapping and intuitively\\nvisualizes the model's prediction strategies in given preprocessing conditions\\nusing the recent Layer-wise Relevance Propagation (LRP) algorithm. Our\\nevaluations on the challenging Adience benchmark show that suitable parameter\\ninitialization leads to a holistic perception of the input, compensating\\nartefactual data representations. With a combination of simple preprocessing\\nsteps, we reach state of the art performance in gender recognition.\",\n",
              " 'When is a Convolutional Filter Easy To Learn?\\nWe analyze the convergence of (stochastic) gradient descent algorithm for\\nlearning a convolutional filter with Rectified Linear Unit (ReLU) activation\\nfunction. Our analysis does not rely on any specific form of the input\\ndistribution and our proofs only use the definition of ReLU, in contrast with\\nprevious works that are restricted to standard Gaussian input. We show that\\n(stochastic) gradient descent with random initialization can learn the\\nconvolutional filter in polynomial time and the convergence rate depends on the\\nsmoothness of the input distribution and the closeness of patches. To the best\\nof our knowledge, this is the first recovery guarantee of gradient-based\\nalgorithms for convolutional filter on non-Gaussian input distributions. Our\\ntheory also justifies the two-stage learning rate strategy in deep neural\\nnetworks. While our focus is theoretical, we also present experiments that\\nillustrate our theoretical findings.',\n",
              " 'Learning Sparse Visual Representations with Leaky Capped Norm\\n  Regularizers\\nSparsity inducing regularization is an important part for learning\\nover-complete visual representations. Despite the popularity of $\\\\ell_1$\\nregularization, in this paper, we investigate the usage of non-convex\\nregularizations in this problem. Our contribution consists of three parts.\\nFirst, we propose the leaky capped norm regularization (LCNR), which allows\\nmodel weights below a certain threshold to be regularized more strongly as\\nopposed to those above, therefore imposes strong sparsity and only introduces\\ncontrollable estimation bias. We propose a majorization-minimization algorithm\\nto optimize the joint objective function. Second, our study over monocular 3D\\nshape recovery and neural networks with LCNR outperforms $\\\\ell_1$ and other\\nnon-convex regularizations, achieving state-of-the-art performance and faster\\nconvergence. Third, we prove a theoretical global convergence speed on the 3D\\nrecovery problem. To the best of our knowledge, this is the first convergence\\nanalysis of the 3D recovery problem.',\n",
              " \"ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection,\\n  Adversarial Examples and Model Criticism\\nConvNets and Imagenet have driven the recent success of deep learning for\\nimage classification. However, the marked slowdown in performance improvement,\\nthe recent studies on the lack of robustness of neural networks to adversarial\\nexamples and their tendency to exhibit undesirable biases (e.g racial biases)\\nquestioned the reliability and the sustained development of these methods. This\\nwork investigates these questions from the perspective of the end-user by using\\nhuman subject studies and explanations. We experimentally demonstrate that the\\naccuracy and robustness of ConvNets measured on Imagenet are underestimated. We\\nshow that explanations can mitigate the impact of misclassified adversarial\\nexamples from the perspective of the end-user and we introduce a novel tool for\\nuncovering the undesirable biases learned by a model. These contributions also\\nshow that explanations are a promising tool for improving our understanding of\\nConvNets' predictions and for designing more reliable models\",\n",
              " \"Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of\\n  Spurious Local Minima\\nWe consider the problem of learning a one-hidden-layer neural network with\\nnon-overlapping convolutional layer and ReLU activation function, i.e.,\\n$f(\\\\mathbf{Z}; \\\\mathbf{w}, \\\\mathbf{a}) = \\\\sum_j\\na_j\\\\sigma(\\\\mathbf{w}^\\\\top\\\\mathbf{Z}_j)$, in which both the convolutional\\nweights $\\\\mathbf{w}$ and the output weights $\\\\mathbf{a}$ are parameters to be\\nlearned. We prove that with Gaussian input $\\\\mathbf{Z}$, there is a spurious\\nlocal minimum that is not a global mininum. Surprisingly, in the presence of\\nlocal minimum, starting from randomly initialized weights, gradient descent\\nwith weight normalization can still be proven to recover the true parameters\\nwith constant probability (which can be boosted to arbitrarily high accuracy\\nwith multiple restarts). We also show that with constant probability, the same\\nprocedure could also converge to the spurious local minimum, showing that the\\nlocal minimum plays a non-trivial role in the dynamics of gradient descent.\\nFurthermore, a quantitative analysis shows that the gradient descent dynamics\\nhas two phases: it starts off slow, but converges much faster after several\\niterations.\",\n",
              " \"Curiosity-driven Exploration by Self-supervised Prediction\\nIn many real-world scenarios, rewards extrinsic to the agent are extremely\\nsparse, or absent altogether. In such cases, curiosity can serve as an\\nintrinsic reward signal to enable the agent to explore its environment and\\nlearn skills that might be useful later in its life. We formulate curiosity as\\nthe error in an agent's ability to predict the consequence of its own actions\\nin a visual feature space learned by a self-supervised inverse dynamics model.\\nOur formulation scales to high-dimensional continuous state spaces like images,\\nbypasses the difficulties of directly predicting pixels, and, critically,\\nignores the aspects of the environment that cannot affect the agent. The\\nproposed approach is evaluated in two environments: VizDoom and Super Mario\\nBros. Three broad settings are investigated: 1) sparse extrinsic reward, where\\ncuriosity allows for far fewer interactions with the environment to reach the\\ngoal; 2) exploration with no extrinsic reward, where curiosity pushes the agent\\nto explore more efficiently; and 3) generalization to unseen scenarios (e.g.\\nnew levels of the same game) where the knowledge gained from earlier experience\\nhelps the agent explore new places much faster than starting from scratch. Demo\\nvideo and code available at https://pathak22.github.io/noreward-rl/\",\n",
              " 'Houdini: Fooling Deep Structured Prediction Models\\nGenerating adversarial examples is a critical step for evaluating and\\nimproving the robustness of learning machines. So far, most existing methods\\nonly work for classification and are not designed to alter the true performance\\nmeasure of the problem at hand. We introduce a novel flexible approach named\\nHoudini for generating adversarial examples specifically tailored for the final\\nperformance measure of the task considered, be it combinatorial and\\nnon-decomposable. We successfully apply Houdini to a range of applications such\\nas speech recognition, pose estimation and semantic segmentation. In all cases,\\nthe attacks based on Houdini achieve higher success rate than those based on\\nthe traditional surrogates used to train the models while using a less\\nperceptible adversarial perturbation.',\n",
              " 'Recent Advances in Zero-shot Recognition\\nWith the recent renaissance of deep convolution neural networks, encouraging\\nbreakthroughs have been achieved on the supervised recognition tasks, where\\neach class has sufficient training data and fully annotated training data.\\nHowever, to scale the recognition to a large number of classes with few or now\\ntraining samples for each class remains an unsolved problem. One approach to\\nscaling up the recognition is to develop models capable of recognizing unseen\\ncategories without any training instances, or zero-shot recognition/ learning.\\nThis article provides a comprehensive review of existing zero-shot recognition\\ntechniques covering various aspects ranging from representations of models, and\\nfrom datasets and evaluation settings. We also overview related recognition\\ntasks including one-shot and open set recognition which can be used as natural\\nextensions of zero-shot recognition when limited number of class samples become\\navailable or when zero-shot recognition is implemented in a real-world setting.\\nImportantly, we highlight the limitations of existing approaches and point out\\nfuture research directions in this existing new research area.',\n",
              " 'The loss surface and expressivity of deep convolutional neural networks\\nWe analyze the expressiveness and loss surface of practical deep\\nconvolutional neural networks (CNNs) with shared weights and max pooling\\nlayers. We show that such CNNs produce linearly independent features at a\\n\"wide\" layer which has more neurons than the number of training samples. This\\ncondition holds e.g. for the VGG network. Furthermore, we provide for such wide\\nCNNs necessary and sufficient conditions for global minima with zero training\\nerror. For the case where the wide layer is followed by a fully connected\\nlayer, we show that almost every critical point of the empirical loss is a\\nglobal minimum with zero training error. Our analysis suggests that both depth\\nand width are very important in deep learning. While depth brings more\\nrepresentational power and allows the network to learn high level features,\\nwidth smoothes the optimization landscape of the loss function in the sense\\nthat a sufficiently wide network has a well-behaved loss surface with\\npotentially no bad local minima.',\n",
              " 'Physics-guided Neural Networks (PGNN): An Application in Lake\\n  Temperature Modeling\\nThis paper introduces a novel framework for combining scientific knowledge of\\nphysics-based models with neural networks to advance scientific discovery. This\\nframework, termed as physics-guided neural network (PGNN), leverages the output\\nof physics-based model simulations along with observational features to\\ngenerate predictions using a neural network architecture. Further, this paper\\npresents a novel framework for using physics-based loss functions in the\\nlearning objective of neural networks, to ensure that the model predictions not\\nonly show lower errors on the training set but are also scientifically\\nconsistent with the known physics on the unlabeled set. We illustrate the\\neffectiveness of PGNN for the problem of lake temperature modeling, where\\nphysical relationships between the temperature, density, and depth of water are\\nused to design a physics-based loss function. By using scientific knowledge to\\nguide the construction and learning of neural networks, we are able to show\\nthat the proposed framework ensures better generalizability as well as\\nscientific consistency of results.',\n",
              " 'Unified Spectral Clustering with Optimal Graph\\nSpectral clustering has found extensive use in many areas. Most traditional\\nspectral clustering algorithms work in three separate steps: similarity graph\\nconstruction; continuous labels learning; discretizing the learned labels by\\nk-means clustering. Such common practice has two potential flaws, which may\\nlead to severe information loss and performance degradation. First, predefined\\nsimilarity graph might not be optimal for subsequent clustering. It is\\nwell-accepted that similarity graph highly affects the clustering results. To\\nthis end, we propose to automatically learn similarity information from data\\nand simultaneously consider the constraint that the similarity matrix has exact\\nc connected components if there are c clusters. Second, the discrete solution\\nmay deviate from the spectral solution since k-means method is well-known as\\nsensitive to the initialization of cluster centers. In this work, we transform\\nthe candidate solution into a new one that better approximates the discrete\\none. Finally, those three subtasks are integrated into a unified framework,\\nwith each subtask iteratively boosted by using the results of the others\\ntowards an overall optimal solution. It is known that the performance of a\\nkernel method is largely determined by the choice of kernels. To tackle this\\npractical problem of how to select the most suitable kernel for a particular\\ndata set, we further extend our model to incorporate multiple kernel learning\\nability. Extensive experiments demonstrate the superiority of our proposed\\nmethod as compared to existing clustering approaches.',\n",
              " 'On the Inductive Bias of Dropout\\nDropout is a simple but effective technique for learning in neural networks\\nand other settings. A sound theoretical understanding of dropout is needed to\\ndetermine when dropout should be applied and how to use it most effectively. In\\nthis paper we continue the exploration of dropout as a regularizer pioneered by\\nWager, et.al. We focus on linear classification where a convex proxy to the\\nmisclassification loss (i.e. the logistic loss used in logistic regression) is\\nminimized. We show: (a) when the dropout-regularized criterion has a unique\\nminimizer, (b) when the dropout-regularization penalty goes to infinity with\\nthe weights, and when it remains bounded, (c) that the dropout regularization\\ncan be non-monotonic as individual weights increase from 0, and (d) that the\\ndropout regularization penalty may not be convex. This last point is\\nparticularly surprising because the combination of dropout regularization with\\nany convex loss proxy is always a convex function.\\n  In order to contrast dropout regularization with $L_2$ regularization, we\\nformalize the notion of when different sources are more compatible with\\ndifferent regularizers. We then exhibit distributions that are provably more\\ncompatible with dropout regularization than $L_2$ regularization, and vice\\nversa. These sources provide additional insight into how the inductive biases\\nof dropout and $L_2$ regularization differ. We provide some similar results for\\n$L_1$ regularization.',\n",
              " 'Surprising properties of dropout in deep networks\\nWe analyze dropout in deep networks with rectified linear units and the\\nquadratic loss. Our results expose surprising differences between the behavior\\nof dropout and more traditional regularizers like weight decay. For example, on\\nsome simple data sets dropout training produces negative weights even though\\nthe output is the sum of the inputs. This provides a counterpoint to the\\nsuggestion that dropout discourages co-adaptation of weights. We also show that\\nthe dropout penalty can grow exponentially in the depth of the network while\\nthe weight-decay penalty remains essentially linear, and that dropout is\\ninsensitive to various re-scalings of the input features, outputs, and network\\nweights. This last insensitivity implies that there are no isolated local\\nminima of the dropout training criterion. Our work uncovers new properties of\\ndropout, extends our understanding of why dropout succeeds, and lays the\\nfoundation for further progress.',\n",
              " 'Training Probabilistic Spiking Neural Networks with First-to-spike\\n  Decoding\\nThird-generation neural networks, or Spiking Neural Networks (SNNs), aim at\\nharnessing the energy efficiency of spike-domain processing by building on\\ncomputing elements that operate on, and exchange, spikes. In this paper, the\\nproblem of training a two-layer SNN is studied for the purpose of\\nclassification, under a Generalized Linear Model (GLM) probabilistic neural\\nmodel that was previously considered within the computational neuroscience\\nliterature. Conventional classification rules for SNNs operate offline based on\\nthe number of output spikes at each output neuron. In contrast, a novel\\ntraining method is proposed here for a first-to-spike decoding rule, whereby\\nthe SNN can perform an early classification decision once spike firing is\\ndetected at an output neuron. Numerical results bring insights into the optimal\\nparameter selection for the GLM neuron and on the accuracy-complexity trade-off\\nperformance of conventional and first-to-spike decoding.',\n",
              " \"A Novel Clustering Algorithm Based on Quantum Games\\nEnormous successes have been made by quantum algorithms during the last\\ndecade. In this paper, we combine the quantum game with the problem of data\\nclustering, and then develop a quantum-game-based clustering algorithm, in\\nwhich data points in a dataset are considered as players who can make decisions\\nand implement quantum strategies in quantum games. After each round of a\\nquantum game, each player's expected payoff is calculated. Later, he uses a\\nlink-removing-and-rewiring (LRR) function to change his neighbors and adjust\\nthe strength of links connecting to them in order to maximize his payoff.\\nFurther, algorithms are discussed and analyzed in two cases of strategies, two\\npayoff matrixes and two LRR functions. Consequently, the simulation results\\nhave demonstrated that data points in datasets are clustered reasonably and\\nefficiently, and the clustering algorithms have fast rates of convergence.\\nMoreover, the comparison with other algorithms also provides an indication of\\nthe effectiveness of the proposed approach.\",\n",
              " 'Exact solutions to the nonlinear dynamics of learning in deep linear\\n  neural networks\\nDespite the widespread practical success of deep learning methods, our\\ntheoretical understanding of the dynamics of learning in deep neural networks\\nremains quite sparse. We attempt to bridge the gap between the theory and\\npractice of deep learning by systematically analyzing learning dynamics for the\\nrestricted case of deep linear neural networks. Despite the linearity of their\\ninput-output map, such networks have nonlinear gradient descent dynamics on\\nweights that change with the addition of each new hidden layer. We show that\\ndeep linear networks exhibit nonlinear learning phenomena similar to those seen\\nin simulations of nonlinear networks, including long plateaus followed by rapid\\ntransitions to lower error solutions, and faster convergence from greedy\\nunsupervised pretraining initial conditions than from random initial\\nconditions. We provide an analytical description of these phenomena by finding\\nnew exact solutions to the nonlinear dynamics of deep learning. Our theoretical\\nanalysis also reveals the surprising finding that as the depth of a network\\napproaches infinity, learning speed can nevertheless remain finite: for a\\nspecial class of initial conditions on the weights, very deep networks incur\\nonly a finite, depth independent, delay in learning speed relative to shallow\\nnetworks. We show that, under certain conditions on the training data,\\nunsupervised pretraining can find this special class of initial conditions,\\nwhile scaled random Gaussian initializations cannot. We further exhibit a new\\nclass of random orthogonal initial conditions on weights that, like\\nunsupervised pre-training, enjoys depth independent learning times. We further\\nshow that these initial conditions also lead to faithful propagation of\\ngradients even in deep nonlinear networks, as long as they operate in a special\\nregime known as the edge of chaos.',\n",
              " 'Entropy of Overcomplete Kernel Dictionaries\\nIn signal analysis and synthesis, linear approximation theory considers a\\nlinear decomposition of any given signal in a set of atoms, collected into a\\nso-called dictionary. Relevant sparse representations are obtained by relaxing\\nthe orthogonality condition of the atoms, yielding overcomplete dictionaries\\nwith an extended number of atoms. More generally than the linear decomposition,\\novercomplete kernel dictionaries provide an elegant nonlinear extension by\\ndefining the atoms through a mapping kernel function (e.g., the gaussian\\nkernel). Models based on such kernel dictionaries are used in neural networks,\\ngaussian processes and online learning with kernels.\\n  The quality of an overcomplete dictionary is evaluated with a diversity\\nmeasure the distance, the approximation, the coherence and the Babel measures.\\nIn this paper, we develop a framework to examine overcomplete kernel\\ndictionaries with the entropy from information theory. Indeed, a higher value\\nof the entropy is associated to a further uniform spread of the atoms over the\\nspace. For each of the aforementioned diversity measures, we derive lower\\nbounds on the entropy. Several definitions of the entropy are examined, with an\\nextensive analysis in both the input space and the mapped feature space.',\n",
              " \"Rotation-invariant convolutional neural networks for galaxy morphology\\n  prediction\\nMeasuring the morphological parameters of galaxies is a key requirement for\\nstudying their formation and evolution. Surveys such as the Sloan Digital Sky\\nSurvey (SDSS) have resulted in the availability of very large collections of\\nimages, which have permitted population-wide analyses of galaxy morphology.\\nMorphological analysis has traditionally been carried out mostly via visual\\ninspection by trained experts, which is time-consuming and does not scale to\\nlarge ($\\\\gtrsim10^4$) numbers of images.\\n  Although attempts have been made to build automated classification systems,\\nthese have not been able to achieve the desired level of accuracy. The Galaxy\\nZoo project successfully applied a crowdsourcing strategy, inviting online\\nusers to classify images by answering a series of questions. Unfortunately,\\neven this approach does not scale well enough to keep up with the increasing\\navailability of galaxy images.\\n  We present a deep neural network model for galaxy morphology classification\\nwhich exploits translational and rotational symmetry. It was developed in the\\ncontext of the Galaxy Challenge, an international competition to build the best\\nmodel for morphology classification based on annotated images from the Galaxy\\nZoo project.\\n  For images with high agreement among the Galaxy Zoo participants, our model\\nis able to reproduce their consensus with near-perfect accuracy ($> 99\\\\%$) for\\nmost questions. Confident model predictions are highly accurate, which makes\\nthe model suitable for filtering large collections of images and forwarding\\nchallenging images to experts for manual annotation. This approach greatly\\nreduces the experts' workload without affecting accuracy. The application of\\nthese algorithms to larger sets of training data will be critical for analysing\\nresults from future surveys such as the LSST.\",\n",
              " 'Kernel Nonnegative Matrix Factorization Without the Curse of the\\n  Pre-image - Application to Unmixing Hyperspectral Images\\nThe nonnegative matrix factorization (NMF) is widely used in signal and image\\nprocessing, including bio-informatics, blind source separation and\\nhyperspectral image analysis in remote sensing. A great challenge arises when\\ndealing with a nonlinear formulation of the NMF. Within the framework of kernel\\nmachines, the models suggested in the literature do not allow the\\nrepresentation of the factorization matrices, which is a fallout of the curse\\nof the pre-image. In this paper, we propose a novel kernel-based model for the\\nNMF that does not suffer from the pre-image problem, by investigating the\\nestimation of the factorization matrices directly in the input space. For\\ndifferent kernel functions, we describe two schemes for iterative algorithms:\\nan additive update rule based on a gradient descent scheme and a multiplicative\\nupdate rule in the same spirit as in the Lee and Seung algorithm. Within the\\nproposed framework, we develop several extensions to incorporate constraints,\\nincluding sparseness, smoothness, and spatial regularization with a\\ntotal-variation-like penalty. The effectiveness of the proposed method is\\ndemonstrated with the problem of unmixing hyperspectral images, using\\nwell-known real images and results with state-of-the-art techniques.',\n",
              " 'Approximation errors of online sparsification criteria\\nMany machine learning frameworks, such as resource-allocating networks,\\nkernel-based methods, Gaussian processes, and radial-basis-function networks,\\nrequire a sparsification scheme in order to address the online learning\\nparadigm. For this purpose, several online sparsification criteria have been\\nproposed to restrict the model definition on a subset of samples. The most\\nknown criterion is the (linear) approximation criterion, which discards any\\nsample that can be well represented by the already contributing samples, an\\noperation with excessive computational complexity. Several computationally\\nefficient sparsification criteria have been introduced in the literature, such\\nas the distance, the coherence and the Babel criteria. In this paper, we\\nprovide a framework that connects these sparsification criteria to the issue of\\napproximating samples, by deriving theoretical bounds on the approximation\\nerrors. Moreover, we investigate the error of approximating any feature, by\\nproposing upper-bounds on the approximation error for each of the\\naforementioned sparsification criteria. Two classes of features are described\\nin detail, the empirical mean and the principal axes in the kernel principal\\ncomponent analysis.',\n",
              " 'Discrete Deep Feature Extraction: A Theory and New Architectures\\nFirst steps towards a mathematical theory of deep convolutional neural\\nnetworks for feature extraction were made---for the continuous-time case---in\\nMallat, 2012, and Wiatowski and B\\\\\"olcskei, 2015. This paper considers the\\ndiscrete case, introduces new convolutional neural network architectures, and\\nproposes a mathematical framework for their analysis. Specifically, we\\nestablish deformation and translation sensitivity results of local and global\\nnature, and we investigate how certain structural properties of the input\\nsignal are reflected in the corresponding feature vectors. Our theory applies\\nto general filters and general Lipschitz-continuous non-linearities and pooling\\noperators. Experiments on handwritten digit classification and facial landmark\\ndetection---including feature importance evaluation---complement the\\ntheoretical findings.',\n",
              " 'Neural Responding Machine for Short-Text Conversation\\nWe propose Neural Responding Machine (NRM), a neural network-based response\\ngenerator for Short-Text Conversation. NRM takes the general encoder-decoder\\nframework: it formalizes the generation of response as a decoding process based\\non the latent representation of the input text, while both encoding and\\ndecoding are realized with recurrent neural networks (RNN). The NRM is trained\\nwith a large amount of one-round conversation data collected from a\\nmicroblogging service. Empirical study shows that NRM can generate\\ngrammatically correct and content-wise appropriate responses to over 75% of the\\ninput text, outperforming state-of-the-arts in the same setting, including\\nretrieval-based and SMT-based models.',\n",
              " 'Deep Active Learning for Dialogue Generation\\nWe propose an online, end-to-end, neural generative conversational model for\\nopen-domain dialogue. It is trained using a unique combination of offline\\ntwo-phase supervised learning and online human-in-the-loop active learning.\\nWhile most existing research proposes offline supervision or hand-crafted\\nreward functions for online reinforcement, we devise a novel interactive\\nlearning mechanism based on hamming-diverse beam search for response generation\\nand one-character user-feedback at each step. Experiments show that our model\\ninherently promotes the generation of semantically relevant and interesting\\nresponses, and can be used to train agents with customized personas, moods and\\nconversational styles.',\n",
              " 'Teaching Machines to Read and Comprehend\\nTeaching machines to read natural language documents remains an elusive\\nchallenge. Machine reading systems can be tested on their ability to answer\\nquestions posed on the contents of documents that they have seen, but until now\\nlarge scale training and test datasets have been missing for this type of\\nevaluation. In this work we define a new methodology that resolves this\\nbottleneck and provides large scale supervised reading comprehension data. This\\nallows us to develop a class of attention based deep neural networks that learn\\nto read real documents and answer complex questions with minimal prior\\nknowledge of language structure.',\n",
              " 'Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models\\n  of Meaning\\nDeep compositional models of meaning acting on distributional representations\\nof words in order to produce vectors of larger text constituents are evolving\\nto a popular area of NLP research. We detail a compositional distributional\\nframework based on a rich form of word embeddings that aims at facilitating the\\ninteractions between words in the context of a sentence. Embeddings and\\ncomposition layers are jointly learned against a generic objective that\\nenhances the vectors with syntactic information from the surrounding context.\\nFurthermore, each word is associated with a number of senses, the most\\nplausible of which is selected dynamically during the composition process. We\\nevaluate the produced vectors qualitatively and quantitatively with positive\\nresults. At the sentence level, the effectiveness of the framework is\\ndemonstrated on the MSRPar task, for which we report results within the\\nstate-of-the-art range.',\n",
              " 'A Deep Architecture for Semantic Matching with Multiple Positional\\n  Sentence Representations\\nMatching natural language sentences is central for many applications such as\\ninformation retrieval and question answering. Existing deep models rely on a\\nsingle sentence representation or multiple granularity representations for\\nmatching. However, such methods cannot well capture the contextualized local\\ninformation in the matching process. To tackle this problem, we present a new\\ndeep architecture to match two sentences with multiple positional sentence\\nrepresentations. Specifically, each positional sentence representation is a\\nsentence representation at this position, generated by a bidirectional long\\nshort term memory (Bi-LSTM). The matching score is finally produced by\\naggregating interactions between these different positional sentence\\nrepresentations, through $k$-Max pooling and a multi-layer perceptron. Our\\nmodel has several advantages: (1) By using Bi-LSTM, rich context of the whole\\nsentence is leveraged to capture the contextualized local information in each\\npositional sentence representation; (2) By matching with multiple positional\\nsentence representations, it is flexible to aggregate different important\\ncontextualized local information in a sentence to support the matching; (3)\\nExperiments on different tasks such as question answering and sentence\\ncompletion demonstrate the superiority of our model.',\n",
              " 'LSTM Neural Reordering Feature for Statistical Machine Translation\\nArtificial neural networks are powerful models, which have been widely\\napplied into many aspects of machine translation, such as language modeling and\\ntranslation modeling. Though notable improvements have been made in these\\nareas, the reordering problem still remains a challenge in statistical machine\\ntranslations. In this paper, we present a novel neural reordering model that\\ndirectly models word pairs and alignment. By utilizing LSTM recurrent neural\\nnetworks, much longer context could be learned for reordering prediction.\\nExperimental results on NIST OpenMT12 Arabic-English and Chinese-English\\n1000-best rescoring task show that our LSTM neural reordering feature is robust\\nand achieves significant improvements over various baseline systems.',\n",
              " 'Learning Natural Language Inference with LSTM\\nNatural language inference (NLI) is a fundamentally important task in natural\\nlanguage processing that has many applications. The recently released Stanford\\nNatural Language Inference (SNLI) corpus has made it possible to develop and\\nevaluate learning-centered methods such as deep neural networks for natural\\nlanguage inference (NLI). In this paper, we propose a special long short-term\\nmemory (LSTM) architecture for NLI. Our model builds on top of a recently\\nproposed neural attention model for NLI but is based on a significantly\\ndifferent idea. Instead of deriving sentence embeddings for the premise and the\\nhypothesis to be used for classification, our solution uses a match-LSTM to\\nperform word-by-word matching of the hypothesis with the premise. This LSTM is\\nable to place more emphasis on important word-level matching results. In\\nparticular, we observe that this LSTM remembers important mismatches that are\\ncritical for predicting the contradiction or the neutral relationship label. On\\nthe SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the\\nstate of the art.',\n",
              " \"Quantifying the vanishing gradient and long distance dependency problem\\n  in recursive neural networks and recursive LSTMs\\nRecursive neural networks (RNN) and their recently proposed extension\\nrecursive long short term memory networks (RLSTM) are models that compute\\nrepresentations for sentences, by recursively combining word embeddings\\naccording to an externally provided parse tree. Both models thus, unlike\\nrecurrent networks, explicitly make use of the hierarchical structure of a\\nsentence. In this paper, we demonstrate that RNNs nevertheless suffer from the\\nvanishing gradient and long distance dependency problem, and that RLSTMs\\ngreatly improve over RNN's on these problems. We present an artificial learning\\ntask that allows us to quantify the severity of these problems for both models.\\nWe further show that a ratio of gradients (at the root node and a focal leaf\\nnode) is highly indicative of the success of backpropagation at optimizing the\\nrelevant weights low in the tree. This paper thus provides an explanation for\\nexisting, superior results of RLSTMs on tasks such as sentiment analysis, and\\nsuggests that the benefits of including hierarchical structure and of including\\nLSTM-style gating are complementary.\",\n",
              " 'Implicit Discourse Relation Classification via Multi-Task Neural\\n  Networks\\nWithout discourse connectives, classifying implicit discourse relations is a\\nchallenging task and a bottleneck for building a practical discourse parser.\\nPrevious research usually makes use of one kind of discourse framework such as\\nPDTB or RST to improve the classification performance on discourse relations.\\nActually, under different discourse annotation frameworks, there exist multiple\\ncorpora which have internal connections. To exploit the combination of\\ndifferent discourse corpora, we design related discourse classification tasks\\nspecific to a corpus, and propose a novel Convolutional Neural Network embedded\\nmulti-task learning system to synthesize these tasks by learning both unique\\nand shared representations for each task. The experimental results on the PDTB\\nimplicit discourse relation classification task demonstrate that our model\\nachieves significant gains over baseline systems.',\n",
              " 'Enhancing Sentence Relation Modeling with Auxiliary Character-level\\n  Embedding\\nNeural network based approaches for sentence relation modeling automatically\\ngenerate hidden matching features from raw sentence pairs. However, the quality\\nof matching feature representation may not be satisfied due to complex semantic\\nrelations such as entailment or contradiction. To address this challenge, we\\npropose a new deep neural network architecture that jointly leverage\\npre-trained word embedding and auxiliary character embedding to learn sentence\\nmeanings. The two kinds of word sequence representations as inputs into\\nmulti-layer bidirectional LSTM to learn enhanced sentence representation. After\\nthat, we construct matching features followed by another temporal CNN to learn\\nhigh-level hidden matching feature representations. Experimental results\\ndemonstrate that our approach consistently outperforms the existing methods on\\nstandard evaluation datasets.',\n",
              " 'Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks\\n  with Feedback Negative Sampling\\nPrevious studies in Open Information Extraction (Open IE) are mainly based on\\nextraction patterns. They manually define patterns or automatically learn them\\nfrom a large corpus. However, these approaches are limited when grasping the\\ncontext of a sentence, and they fail to capture implicit relations. In this\\npaper, we address this problem with the following methods. First, we exploit\\nlong short-term memory (LSTM) networks to extract higher-level features along\\nthe shortest dependency paths, connecting headwords of relations and arguments.\\nThe path-level features from LSTM networks provide useful clues regarding\\ncontextual information and the validity of arguments. Second, we constructed\\nsamples to train LSTM networks without the need for manual labeling. In\\nparticular, feedback negative sampling picks highly negative samples among\\nnon-positive samples through a model trained with positive samples. The\\nexperimental results show that our approach produces more precise and abundant\\nextractions than state-of-the-art open IE systems. To the best of our\\nknowledge, this is the first work to apply deep learning to Open IE.',\n",
              " 'Question Answering over Knowledge Base with Neural Attention Combining\\n  Global Knowledge Information\\nWith the rapid growth of knowledge bases (KBs) on the web, how to take full\\nadvantage of them becomes increasingly important. Knowledge base-based question\\nanswering (KB-QA) is one of the most promising approaches to access the\\nsubstantial knowledge. Meantime, as the neural network-based (NN-based) methods\\ndevelop, NN-based KB-QA has already achieved impressive results. However,\\nprevious work did not put emphasis on question representation, and the question\\nis converted into a fixed vector regardless of its candidate answers. This\\nsimple representation strategy is unable to express the proper information of\\nthe question. Hence, we present a neural attention-based model to represent the\\nquestions dynamically according to the different focuses of various candidate\\nanswer aspects. In addition, we leverage the global knowledge inside the\\nunderlying KB, aiming at integrating the rich KB information into the\\nrepresentation of the answers. And it also alleviates the out of vocabulary\\n(OOV) problem, which helps the attention model to represent the question more\\nprecisely. The experimental results on WEBQUESTIONS demonstrate the\\neffectiveness of the proposed approach.',\n",
              " 'Generating Natural Language Inference Chains\\nThe ability to reason with natural language is a fundamental prerequisite for\\nmany NLP tasks such as information extraction, machine translation and question\\nanswering. To quantify this ability, systems are commonly tested whether they\\ncan recognize textual entailment, i.e., whether one sentence can be inferred\\nfrom another one. However, in most NLP applications only single source\\nsentences instead of sentence pairs are available. Hence, we propose a new task\\nthat measures how well a model can generate an entailed sentence from a source\\nsentence. We take entailment-pairs of the Stanford Natural Language Inference\\ncorpus and train an LSTM with attention. On a manually annotated test set we\\nfound that 82% of generated sentences are correct, an improvement of 10.3% over\\nan LSTM baseline. A qualitative analysis shows that this model is not only\\ncapable of shortening input sentences, but also inferring new statements via\\nparaphrasing and phrase entailment. We then apply this model recursively to\\ninput-output pairs, thereby generating natural language inference chains that\\ncan be used to automatically construct an entailment graph from source\\nsentences. Finally, by swapping source and target sentences we can also train a\\nmodel that given an input sentence invents additional information to generate a\\nnew sentence.',\n",
              " 'MuFuRU: The Multi-Function Recurrent Unit\\nRecurrent neural networks such as the GRU and LSTM found wide adoption in\\nnatural language processing and achieve state-of-the-art results for many\\ntasks. These models are characterized by a memory state that can be written to\\nand read from by applying gated composition operations to the current input and\\nthe previous state. However, they only cover a small subset of potentially\\nuseful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that\\nallow for arbitrary differentiable functions as composition operations.\\nFurthermore, MuFuRUs allow for an input- and state-dependent choice of these\\ncomposition operations that is learned. Our experiments demonstrate that the\\nadditional functionality helps in different sequence modeling tasks, including\\nthe evaluation of propositional logic formulae, language modeling and sentiment\\nanalysis.',\n",
              " 'LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in\\n  Recurrent Neural Networks\\nRecurrent neural networks, and in particular long short-term memory (LSTM)\\nnetworks, are a remarkably effective tool for sequence modeling that learn a\\ndense black-box hidden representation of their sequential input. Researchers\\ninterested in better understanding these models have studied the changes in\\nhidden state representations over time and noticed some interpretable patterns\\nbut also significant noise. In this work, we present LSTMVIS, a visual analysis\\ntool for recurrent neural networks with a focus on understanding these hidden\\nstate dynamics. The tool allows users to select a hypothesis input range to\\nfocus on local state changes, to match these states changes to similar patterns\\nin a large data set, and to align these results with structural annotations\\nfrom their domain. We show several use cases of the tool for analyzing specific\\nhidden state properties on dataset containing nesting, phrase structure, and\\nchord progressions, and demonstrate how the tool can be used to isolate\\npatterns for further statistical analysis. We characterize the domain, the\\ndifferent stakeholders, and their goals and tasks.',\n",
              " \"Compression of Neural Machine Translation Models via Pruning\\nNeural Machine Translation (NMT), like many other deep learning domains,\\ntypically suffers from over-parameterization, resulting in large storage sizes.\\nThis paper examines three simple magnitude-based pruning schemes to compress\\nNMT models, namely class-blind, class-uniform, and class-distribution, which\\ndiffer in terms of how pruning thresholds are computed for the different\\nclasses of weights in the NMT architecture. We demonstrate the efficacy of\\nweight pruning as a compression technique for a state-of-the-art NMT system. We\\nshow that an NMT model with over 200 million parameters can be pruned by 40%\\nwith very little performance loss as measured on the WMT'14 English-German\\ntranslation task. This sheds light on the distribution of redundancy in the NMT\\narchitecture. Our main result is that with retraining, we can recover and even\\nsurpass the original performance with an 80%-pruned model.\",\n",
              " 'Constructing a Natural Language Inference Dataset using Generative\\n  Neural Networks\\nNatural Language Inference is an important task for Natural Language\\nUnderstanding. It is concerned with classifying the logical relation between\\ntwo sentences. In this paper, we propose several text generative neural\\nnetworks for generating text hypothesis, which allows construction of new\\nNatural Language Inference datasets. To evaluate the models, we propose a new\\nmetric -- the accuracy of the classifier trained on the generated dataset. The\\naccuracy obtained by our best generative model is only 2.7% lower than the\\naccuracy of the classifier trained on the original, human crafted dataset.\\nFurthermore, the best generated dataset combined with the original dataset\\nachieves the highest accuracy. The best model learns a mapping embedding for\\neach training example. By comparing various metrics we show that datasets that\\nobtain higher ROUGE or METEOR scores do not necessarily yield higher\\nclassification accuracies. We also provide analysis of what are the\\ncharacteristics of a good dataset including the distinguishability of the\\ngenerated datasets from the original one.',\n",
              " 'Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain\\n  Factoid Question Answering\\nWhile question answering (QA) with neural network, i.e. neural QA, has\\nachieved promising results in recent years, lacking of large scale real-word QA\\ndataset is still a challenge for developing and evaluating neural QA system. To\\nalleviate this problem, we propose a large scale human annotated real-world QA\\ndataset WebQA with more than 42k questions and 556k evidences. As existing\\nneural QA methods resolve QA either as sequence generation or\\nclassification/ranking problem, they face challenges of expensive softmax\\ncomputation, unseen answers handling or separate candidate answer generation\\ncomponent. In this work, we cast neural QA as a sequence labeling problem and\\npropose an end-to-end sequence labeling model, which overcomes all the above\\nchallenges. Experimental results on WebQA show that our model outperforms the\\nbaselines significantly with an F1 score of 74.69% with word-based input, and\\nthe performance drops only 3.72 F1 points with more challenging character-based\\ninput.',\n",
              " 'Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM\\n  Encoder-Decoder\\nWe present Tweet2Vec, a novel method for generating general-purpose vector\\nrepresentation of tweets. The model learns tweet embeddings using\\ncharacter-level CNN-LSTM encoder-decoder. We trained our model on 3 million,\\nrandomly selected English-language tweets. The model was evaluated using two\\nmethods: tweet semantic similarity and tweet sentiment categorization,\\noutperforming the previous state-of-the-art in both tasks. The evaluations\\ndemonstrate the power of the tweet embeddings generated by our model for\\nvarious tweet categorization tasks. The vector representations generated by our\\nmodel are generic, and hence can be applied to a variety of tasks. Though the\\nmodel presented in this paper is trained on English-language tweets, the method\\npresented can be used to learn tweet embeddings for different languages.',\n",
              " 'Online Segment to Segment Neural Transduction\\nWe introduce an online neural sequence to sequence model that learns to\\nalternate between encoding and decoding segments of the input as it is read. By\\nindependently tracking the encoding and decoding representations our algorithm\\npermits exact polynomial marginalization of the latent segmentation during\\ntraining, and during decoding beam search is employed to find the best\\nalignment path together with the predicted output sequence. Our model tackles\\nthe bottleneck of vanilla encoder-decoders that have to read and memorize the\\nentire input sequence in their fixed-length hidden states before producing any\\noutput. It is different from previous attentive models in that, instead of\\ntreating the attention weights as output of a deterministic function, our model\\nassigns attention weights to a sequential latent variable which can be\\nmarginalized out and permits online generation. Experiments on abstractive\\nsentence summarization and morphological inflection show significant\\nperformance gains over the baseline encoder-decoders.',\n",
              " 'Semantic Parsing with Semi-Supervised Sequential Autoencoders\\nWe present a novel semi-supervised approach for sequence transduction and\\napply it to semantic parsing. The unsupervised component is based on a\\ngenerative model in which latent sentences generate the unpaired logical forms.\\nWe apply this method to a number of semantic parsing tasks focusing on domains\\nwith limited access to labelled training data and extend those datasets with\\nsynthetically generated logical forms.',\n",
              " 'Exploiting Sentence and Context Representations in Deep Neural Models\\n  for Spoken Language Understanding\\nThis paper presents a deep learning architecture for the semantic decoder\\ncomponent of a Statistical Spoken Dialogue System. In a slot-filling dialogue,\\nthe semantic decoder predicts the dialogue act and a set of slot-value pairs\\nfrom a set of n-best hypotheses returned by the Automatic Speech Recognition.\\nMost current models for spoken language understanding assume (i) word-aligned\\nsemantic annotations as in sequence taggers and (ii) delexicalisation, or a\\nmapping of input words to domain-specific concepts using heuristics that try to\\ncapture morphological variation but that do not scale to other domains nor to\\nlanguage variation (e.g., morphology, synonyms, paraphrasing ). In this work\\nthe semantic decoder is trained using unaligned semantic annotations and it\\nuses distributed semantic representation learning to overcome the limitations\\nof explicit delexicalisation. The proposed architecture uses a convolutional\\nneural network for the sentence representation and a long-short term memory\\nnetwork for the context representation. Results are presented for the publicly\\navailable DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a\\nsignificantly higher word error rate (WER).',\n",
              " 'The Neural Noisy Channel\\nWe formulate sequence to sequence transduction as a noisy channel decoding\\nproblem and use recurrent neural networks to parameterise the source and\\nchannel models. Unlike direct models which can suffer from explaining-away\\neffects during training, noisy channel models must produce outputs that explain\\ntheir inputs, and their component models can be trained with not only paired\\ntraining samples but also unpaired samples from the marginal output\\ndistribution. Using a latent variable to control how much of the conditioning\\nsequence the channel model needs to read in order to generate a subsequent\\nsymbol, we obtain a tractable and effective beam search decoder. Experimental\\nresults on abstractive sentence summarisation, morphological inflection, and\\nmachine translation show that noisy channel models outperform direct models,\\nand that they significantly benefit from increased amounts of unpaired output\\ndata that direct models cannot easily use.',\n",
              " 'Generative Deep Neural Networks for Dialogue: A Short Review\\nResearchers have recently started investigating deep neural networks for\\ndialogue applications. In particular, generative sequence-to-sequence (Seq2Seq)\\nmodels have shown promising results for unstructured tasks, such as word-level\\ndialogue response generation. The hope is that such models will be able to\\nleverage massive amounts of data to learn meaningful natural language\\nrepresentations and response generation strategies, while requiring a minimum\\namount of domain knowledge and hand-crafting. An important challenge is to\\ndevelop models that can effectively incorporate dialogue context and generate\\nmeaningful and diverse responses. In support of this goal, we review recently\\nproposed models based on generative encoder-decoder neural network\\narchitectures, and show that these models have better ability to incorporate\\nlong-term dialogue history, to model uncertainty and ambiguity in dialogue, and\\nto generate responses with high-level compositional structure.',\n",
              " 'Learning Python Code Suggestion with a Sparse Pointer Network\\nTo enhance developer productivity, all modern integrated development\\nenvironments (IDEs) include code suggestion functionality that proposes likely\\nnext tokens at the cursor. While current IDEs work well for statically-typed\\nlanguages, their reliance on type annotations means that they do not provide\\nthe same level of support for dynamic programming languages as for\\nstatically-typed languages. Moreover, suggestion engines in modern IDEs do not\\npropose expressions or multi-statement idiomatic code. Recent work has shown\\nthat language models can improve code suggestion systems by learning from\\nsoftware repositories. This paper introduces a neural language model with a\\nsparse pointer network aimed at capturing very long-range dependencies. We\\nrelease a large-scale code suggestion corpus of 41M lines of Python code\\ncrawled from GitHub. On this corpus, we found standard neural language models\\nto perform well at suggesting local phenomena, but struggle to refer to\\nidentifiers that are introduced many tokens in the past. By augmenting a neural\\nlanguage model with a pointer network specialized in referring to predefined\\nclasses of identifiers, we obtain a much lower perplexity and a 5 percentage\\npoints increase in accuracy for code suggestion compared to an LSTM baseline.\\nIn fact, this increase in code suggestion accuracy is due to a 13 times more\\naccurate prediction of identifiers. Furthermore, a qualitative analysis shows\\nthis model indeed captures interesting long-range dependencies, like referring\\nto a class member defined over 60 tokens in the past.',\n",
              " 'OpenNMT: Open-Source Toolkit for Neural Machine Translation\\nWe describe an open-source toolkit for neural machine translation (NMT). The\\ntoolkit prioritizes efficiency, modularity, and extensibility with the goal of\\nsupporting NMT research into model architectures, feature representations, and\\nsource modalities, while maintaining competitive performance and reasonable\\ntraining requirements. The toolkit consists of modeling and translation\\nsupport, as well as detailed pedagogical documentation about the underlying\\ntechniques.',\n",
              " 'Making Neural QA as Simple as Possible but not Simpler\\nRecent development of large-scale question answering (QA) datasets triggered\\na substantial amount of research into end-to-end neural architectures for QA.\\nIncreasingly complex systems have been conceived without comparison to simpler\\nneural baseline systems that would justify their complexity. In this work, we\\npropose a simple heuristic that guides the development of neural baseline\\nsystems for the extractive QA task. We find that there are two ingredients\\nnecessary for building a high-performing neural QA system: first, the awareness\\nof question words while processing the context and second, a composition\\nfunction that goes beyond simple bag-of-words modeling, such as recurrent\\nneural networks. Our results show that FastQA, a system that meets these two\\nrequirements, can achieve very competitive performance compared with existing\\nmodels. We argue that this surprising finding puts results of previous systems\\nand the complexity of recent QA datasets into perspective.',\n",
              " 'Survey of the State of the Art in Natural Language Generation: Core\\n  tasks, applications and evaluation\\nThis paper surveys the current state of the art in Natural Language\\nGeneration (NLG), defined as the task of generating text or speech from\\nnon-linguistic input. A survey of NLG is timely in view of the changes that the\\nfield has undergone over the past decade or so, especially in relation to new\\n(usually data-driven) methods, as well as new applications of NLG technology.\\nThis survey therefore aims to (a) give an up-to-date synthesis of research on\\nthe core tasks in NLG and the architectures adopted in which such tasks are\\norganised; (b) highlight a number of relatively recent research topics that\\nhave arisen partly as a result of growing synergies between NLG and other areas\\nof artificial intelligence; (c) draw attention to the challenges in NLG\\nevaluation, relating them to similar challenges faced in other areas of Natural\\nLanguage Processing, with an emphasis on different evaluation methods and the\\nrelationships between them.',\n",
              " 'A Constrained Sequence-to-Sequence Neural Model for Sentence\\n  Simplification\\nSentence simplification reduces semantic complexity to benefit people with\\nlanguage impairments. Previous simplification studies on the sentence level and\\nword level have achieved promising results but also meet great challenges. For\\nsentence-level studies, sentences after simplification are fluent but sometimes\\nare not really simplified. For word-level studies, words are simplified but\\nalso have potential grammar errors due to different usages of words before and\\nafter simplification. In this paper, we propose a two-step simplification\\nframework by combining both the word-level and the sentence-level\\nsimplifications, making use of their corresponding advantages. Based on the\\ntwo-step framework, we implement a novel constrained neural generation model to\\nsimplify sentences given simplified words. The final results on Wikipedia and\\nSimple Wikipedia aligned datasets indicate that our method yields better\\nperformance than various baselines.',\n",
              " 'Improved Neural Relation Detection for Knowledge Base Question Answering\\nRelation detection is a core component for many NLP applications including\\nKnowledge Base Question Answering (KBQA). In this paper, we propose a\\nhierarchical recurrent neural network enhanced by residual learning that\\ndetects KB relations given an input question. Our method uses deep residual\\nbidirectional LSTMs to compare questions and relation names via different\\nhierarchies of abstraction. Additionally, we propose a simple KBQA system that\\nintegrates entity linking and our proposed relation detector to enable one\\nenhance another. Experimental results evidence that our approach achieves not\\nonly outstanding relation detection performance, but more importantly, it helps\\nour KBQA system to achieve state-of-the-art accuracy for both single-relation\\n(SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.',\n",
              " 'ASR error management for improving spoken language understanding\\nThis paper addresses the problem of automatic speech recognition (ASR) error\\ndetection and their use for improving spoken language understanding (SLU)\\nsystems. In this study, the SLU task consists in automatically extracting, from\\nASR transcriptions , semantic concepts and concept/values pairs in a e.g\\ntouristic information system. An approach is proposed for enriching the set of\\nsemantic labels with error specific labels and by using a recently proposed\\nneural approach based on word embeddings to compute well calibrated ASR\\nconfidence measures. Experimental results are reported showing that it is\\npossible to decrease significantly the Concept/Value Error Rate with a state of\\nthe art system, outperforming previously published results performance on the\\nsame experimental data. It also shown that combining an SLU approach based on\\nconditional random fields with a neural encoder/decoder attention based\\narchitecture , it is possible to effectively identifying confidence islands and\\nuncertain semantic output segments useful for deciding appropriate error\\nhandling actions by the dialogue manager strategy .',\n",
              " 'Dynamic Integration of Background Knowledge in Neural NLU Systems\\nCommon-sense or background knowledge is required to understand natural\\nlanguage, but in most neural natural language understanding (NLU) systems, the\\nrequisite background knowledge is indirectly acquired from static corpora. We\\ndevelop a new reading architecture for the dynamic integration of explicit\\nbackground knowledge in NLU models. A new task-agnostic reading module provides\\nrefined word representations to a task-specific NLU architecture by processing\\nbackground knowledge in the form of free-text statements, together with the\\ntask-specific inputs. Strong performance on the tasks of document question\\nanswering (DQA) and recognizing textual entailment (RTE) demonstrate the\\neffectiveness and flexibility of our approach. Analysis shows that our models\\nlearn to exploit knowledge selectively and in a semantically appropriate way.',\n",
              " \"Rethinking Skip-thought: A Neighborhood based Approach\\nWe study the skip-thought model with neighborhood information as weak\\nsupervision. More specifically, we propose a skip-thought neighbor model to\\nconsider the adjacent sentences as a neighborhood. We train our skip-thought\\nneighbor model on a large corpus with continuous sentences, and then evaluate\\nthe trained model on 7 tasks, which include semantic relatedness, paraphrase\\ndetection, and classification benchmarks. Both quantitative comparison and\\nqualitative investigation are conducted. We empirically show that, our\\nskip-thought neighbor model performs as well as the skip-thought model on\\nevaluation tasks. In addition, we found that, incorporating an autoencoder path\\nin our model didn't aid our model to perform better, while it hurts the\\nperformance of the skip-thought model.\",\n",
              " 'Neural Domain Adaptation for Biomedical Question Answering\\nFactoid question answering (QA) has recently benefited from the development\\nof deep learning (DL) systems. Neural network models outperform traditional\\napproaches in domains where large datasets exist, such as SQuAD (ca. 100,000\\nquestions) for Wikipedia articles. However, these systems have not yet been\\napplied to QA in more specific domains, such as biomedicine, because datasets\\nare generally too small to train a DL system from scratch. For example, the\\nBioASQ dataset for biomedical QA comprises less then 900 factoid (single\\nanswer) and list (multiple answers) QA instances. In this work, we adapt a\\nneural QA system trained on a large open-domain dataset (SQuAD, source) to a\\nbiomedical dataset (BioASQ, target) by employing various transfer learning\\ntechniques. Our network architecture is based on a state-of-the-art QA system,\\nextended with biomedical word embeddings and a novel mechanism to answer list\\nquestions. In contrast to existing biomedical QA systems, our system does not\\nrely on domain-specific ontologies, parsers or entity taggers, which are\\nexpensive to create. Despite this fact, our systems achieve state-of-the-art\\nresults on factoid questions and competitive results on list questions.',\n",
              " 'Neural Models for Key Phrase Detection and Question Generation\\nWe propose a two-stage neural model to tackle question generation from\\ndocuments. Our model first estimates the probability that word sequences in a\\ndocument compose \"interesting\" answers using a neural model trained on a\\nquestion-answering corpus. We thus take a data-driven approach to\\ninterestingness. Predicted key phrases then act as target answers that\\ncondition a sequence-to-sequence question generation model with a copy\\nmechanism. Empirically, our neural key phrase detection model significantly\\noutperforms an entity-tagging baseline system and existing rule-based\\napproaches. We demonstrate that the question generator formulates good quality\\nnatural language questions from extracted key phrases, and a human study\\nindicates that our system\\'s generated question-answer pairs are competitive\\nwith those of an earlier approach. We foresee our system being used in an\\neducational setting to assess reading comprehension and also as a data\\naugmentation technique for semi-supervised learning.',\n",
              " 'Neural Question Answering at BioASQ 5B\\nThis paper describes our submission to the 2017 BioASQ challenge. We\\nparticipated in Task B, Phase B which is concerned with biomedical question\\nanswering (QA). We focus on factoid and list question, using an extractive QA\\nmodel, that is, we restrict our system to output substrings of the provided\\ntext snippets. At the core of our system, we use FastQA, a state-of-the-art\\nneural QA system. We extended it with biomedical word embeddings and changed\\nits answer layer to be able to answer list questions in addition to factoid\\nquestions. We pre-trained the model on a large-scale open-domain QA dataset,\\nSQuAD, and then fine-tuned the parameters on the BioASQ training set. With our\\napproach, we achieve state-of-the-art results on factoid questions and\\ncompetitive results on list questions.',\n",
              " 'A Deep Network with Visual Text Composition Behavior\\nWhile natural languages are compositional, how state-of-the-art neural models\\nachieve compositionality is still unclear. We propose a deep network, which not\\nonly achieves competitive accuracy for text classification, but also exhibits\\ncompositional behavior. That is, while creating hierarchical representations of\\na piece of text, such as a sentence, the lower layers of the network distribute\\ntheir layer-specific attention weights to individual words. In contrast, the\\nhigher layers compose meaningful phrases and clauses, whose lengths increase as\\nthe networks get deeper until fully composing the sentence.',\n",
              " 'Semi-supervised emotion lexicon expansion with label propagation and\\n  specialized word embeddings\\nThere exist two main approaches to automatically extract affective\\norientation: lexicon-based and corpus-based. In this work, we argue that these\\ntwo methods are compatible and show that combining them can improve the\\naccuracy of emotion classifiers. In particular, we introduce a novel variant of\\nthe Label Propagation algorithm that is tailored to distributed word\\nrepresentations, we apply batch gradient descent to accelerate the optimization\\nof label propagation and to make the optimization feasible for large graphs,\\nand we propose a reproducible method for emotion lexicon expansion. We conclude\\nthat label propagation can expand an emotion lexicon in a meaningful way and\\nthat the expanded emotion lexicon can be leveraged to improve the accuracy of\\nan emotion classifier.',\n",
              " 'Modelling Protagonist Goals and Desires in First-Person Narrative\\nMany genres of natural language text are narratively structured, a testament\\nto our predilection for organizing our experiences as narratives. There is\\nbroad consensus that understanding a narrative requires identifying and\\ntracking the goals and desires of the characters and their narrative outcomes.\\nHowever, to date, there has been limited work on computational models for this\\nproblem. We introduce a new dataset, DesireDB, which includes gold-standard\\nlabels for identifying statements of desire, textual evidence for desire\\nfulfillment, and annotations for whether the stated desire is fulfilled given\\nthe evidence in the narrative context. We report experiments on tracking desire\\nfulfillment using different methods, and show that LSTM Skip-Thought model\\nachieves F-measure of 0.7 on our corpus.',\n",
              " 'Understanding Grounded Language Learning Agents\\nNeural network-based systems can now learn to locate the referents of words\\nand phrases in images, answer questions about visual scenes, and even execute\\nsymbolic instructions as first-person actors in partially-observable worlds. To\\nachieve this so-called grounded language learning, models must overcome certain\\nwell-studied learning challenges that are also fundamental to infants learning\\ntheir first words. While it is notable that models with no meaningful prior\\nknowledge overcome these learning obstacles, AI researchers and practitioners\\ncurrently lack a clear understanding of exactly how they do so. Here we address\\nthis question as a way of achieving a clearer general understanding of grounded\\nlanguage learning, both to inform future research and to improve confidence in\\nmodel predictions. For maximum control and generality, we focus on a simple\\nneural network-based language learning agent trained via policy-gradient\\nmethods to interpret synthetic linguistic instructions in a simulated 3D world.\\nWe apply experimental paradigms from developmental psychology to this agent,\\nexploring the conditions under which established human biases and learning\\neffects emerge. We further propose a novel way to visualise and analyse\\nsemantic representation in grounded language learning agents that yields a\\nplausible computational account of the observed effects.',\n",
              " \"Just ASK: Building an Architecture for Extensible Self-Service Spoken\\n  Language Understanding\\nThis paper presents the design of the machine learning architecture that\\nunderlies the Alexa Skills Kit (ASK) a large scale Spoken Language\\nUnderstanding (SLU) Software Development Kit (SDK) that enables developers to\\nextend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the\\ninfrastructure powers over 25,000 skills deployed through the ASK, as well as\\nAWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability\\nand a rapid iteration cycle for third party developers. It imposes inductive\\nbiases that allow it to learn robust SLU models from extremely small and sparse\\ndatasets and, in doing so, removes significant barriers to entry for software\\ndevelopers and dialogue systems researchers.\",\n",
              " 'The NarrativeQA Reading Comprehension Challenge\\nReading comprehension (RC)---in contrast to information retrieval---requires\\nintegrating information and reasoning about events, entities, and their\\nrelations across a full document. Question answering is conventionally used to\\nassess RC ability, in both artificial agents and children learning to read.\\nHowever, existing RC datasets and tasks are dominated by questions that can be\\nsolved by selecting answers using superficial information (e.g., local context\\nsimilarity or global term frequency); they thus fail to test for the essential\\nintegrative aspect of RC. To encourage progress on deeper comprehension of\\nlanguage, we present a new dataset and set of tasks in which the reader must\\nanswer questions about stories by reading entire books or movie scripts. These\\ntasks are designed so that successfully answering their questions requires\\nunderstanding the underlying narrative rather than relying on shallow pattern\\nmatching or salience. We show that although humans solve the tasks easily,\\nstandard RC models struggle on the tasks presented here. We provide an analysis\\nof the dataset and the challenges it presents.',\n",
              " 'Cognitive Database: A Step towards Endowing Relational Databases with\\n  Artificial Intelligence Capabilities\\nWe propose Cognitive Databases, an approach for transparently enabling\\nArtificial Intelligence (AI) capabilities in relational databases. A novel\\naspect of our design is to first view the structured data source as meaningful\\nunstructured text, and then use the text to build an unsupervised neural\\nnetwork model using a Natural Language Processing (NLP) technique called word\\nembedding. This model captures the hidden inter-/intra-column relationships\\nbetween database tokens of different types. For each database token, the model\\nincludes a vector that encodes contextual semantic relationships. We seamlessly\\nintegrate the word embedding model into existing SQL query infrastructure and\\nuse it to enable a new class of SQL-based analytics queries called cognitive\\nintelligence (CI) queries. CI queries use the model vectors to enable complex\\nqueries such as semantic matching, inductive reasoning queries such as\\nanalogies, predictive queries using entities not present in a database, and,\\nmore generally, using knowledge from external sources. We demonstrate unique\\ncapabilities of Cognitive Databases using an Apache Spark based prototype to\\nexecute inductive reasoning CI queries over a multi-modal database containing\\ntext and images. We believe our first-of-a-kind system exemplifies using AI\\nfunctionality to endow relational databases with capabilities that were\\npreviously very hard to realize in practice.',\n",
              " 'Feudal Reinforcement Learning for Dialogue Management in Large Domains\\nReinforcement learning (RL) is a promising approach to solve dialogue policy\\noptimisation. Traditional RL algorithms, however, fail to scale to large\\ndomains due to the curse of dimensionality. We propose a novel Dialogue\\nManagement architecture, based on Feudal RL, which decomposes the decision into\\ntwo steps; a first step where a master policy selects a subset of primitive\\nactions, and a second step where a primitive action is chosen from the selected\\nsubset. The structural information included in the domain ontology is used to\\nabstract the dialogue state space, taking the decisions at each step using\\ndifferent parts of the abstracted state. This, combined with an information\\nsharing mechanism between slots, increases the scalability to large domains. We\\nshow that an implementation of this approach, based on Deep-Q Networks,\\nsignificantly outperforms previous state of the art in several dialogue domains\\nand environments, without the need of any additional reward signal.',\n",
              " 'An Analysis of Neural Language Modeling at Multiple Scales\\nMany of the leading approaches in language modeling introduce novel, complex\\nand specialized architectures. We take existing state-of-the-art word level\\nlanguage models based on LSTMs and QRNNs and extend them to both larger\\nvocabularies as well as character-level granularity. When properly tuned, LSTMs\\nand QRNNs achieve state-of-the-art results on character-level (Penn Treebank,\\nenwik8) and word-level (WikiText-103) datasets, respectively. Results are\\nobtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single\\nmodern GPU.',\n",
              " 'Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy\\n  and Reverberant Environments\\nWe propose a spatial diffuseness feature for deep neural network (DNN)-based\\nautomatic speech recognition to improve recognition accuracy in reverberant and\\nnoisy environments. The feature is computed in real-time from multiple\\nmicrophone signals without requiring knowledge or estimation of the direction\\nof arrival, and represents the relative amount of diffuse noise in each time\\nand frequency bin. It is shown that using the diffuseness feature as an\\nadditional input to a DNN-based acoustic model leads to a reduced word error\\nrate for the REVERB challenge corpus, both compared to logmelspec features\\nextracted from noisy signals, and features enhanced by spectral subtraction.',\n",
              " 'Character-Aware Neural Language Models\\nWe describe a simple neural language model that relies only on\\ncharacter-level inputs. Predictions are still made at the word-level. Our model\\nemploys a convolutional neural network (CNN) and a highway network over\\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\\nneural network language model (RNN-LM). On the English Penn Treebank the model\\nis on par with the existing state-of-the-art despite having 60% fewer\\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\\nSpanish, Russian), the model outperforms word-level/morpheme-level LSTM\\nbaselines, again with fewer parameters. The results suggest that on many\\nlanguages, character inputs are sufficient for language modeling. Analysis of\\nword representations obtained from the character composition part of the model\\nreveals that the model is able to encode, from characters only, both semantic\\nand orthographic information.',\n",
              " 'Neural-based machine translation for medical text domain. Based on\\n  European Medicines Agency leaflet texts\\nThe quality of machine translation is rapidly evolving. Today one can find\\nseveral machine translation systems on the web that provide reasonable\\ntranslations, although the systems are not perfect. In some specific domains,\\nthe quality may decrease. A recently proposed approach to this domain is neural\\nmachine translation. It aims at building a jointly-tuned single neural network\\nthat maximizes translation performance, a very different approach from\\ntraditional statistical machine translation. Recently proposed neural machine\\ntranslation models often belong to the encoder-decoder family in which a source\\nsentence is encoded into a fixed length vector that is, in turn, decoded to\\ngenerate a translation. The present research examines the effects of different\\ntraining methods on a Polish-English Machine Translation system used for\\nmedical data. The European Medicines Agency parallel text corpus was used as\\nthe basis for training of neural and statistical network-based translation\\nsystems. The main machine translation evaluation metrics have also been used in\\nanalysis of the systems. A comparison and implementation of a real-time medical\\ntranslator is the main focus of our experiments.',\n",
              " 'Conditional Generation and Snapshot Learning in Neural Dialogue Systems\\nRecently a variety of LSTM-based conditional language models (LM) have been\\napplied across a range of language generation tasks. In this work we study\\nvarious model architectures and different ways to represent and aggregate the\\nsource information in an end-to-end neural dialogue system framework. A method\\ncalled snapshot learning is also proposed to facilitate learning from\\nsupervised sequential signals by applying a companion cross-entropy objective\\nfunction to the conditioning vector. The experimental and analytical results\\ndemonstrate firstly that competition occurs between the conditioning vector and\\nthe LM, and the differing architectures provide different trade-offs between\\nthe two. Secondly, the discriminative power and transparency of the\\nconditioning vector is key to providing both model interpretability and better\\nperformance. Thirdly, snapshot learning leads to consistent performance\\nimprovements independent of which architecture is used.',\n",
              " 'Dialog state tracking, a machine reading approach using Memory Network\\nIn an end-to-end dialog system, the aim of dialog state tracking is to\\naccurately estimate a compact representation of the current dialog status from\\na sequence of noisy observations produced by the speech recognition and the\\nnatural language understanding modules. This paper introduces a novel method of\\ndialog state tracking based on the general paradigm of machine reading and\\nproposes to solve it using an End-to-End Memory Network, MemN2N, a\\nmemory-enhanced neural network architecture. We evaluate the proposed approach\\non the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has\\nbeen converted for the occasion in order to frame the hidden state variable\\ninference as a question-answering task based on a sequence of utterances\\nextracted from a dialog. We show that the proposed tracker gives encouraging\\nresults. Then, we propose to extend the DSTC-2 dataset with specific reasoning\\ncapabilities requirement like counting, list maintenance, yes-no question\\nanswering and indefinite knowledge management. Finally, we present encouraging\\nresults using our proposed MemN2N based tracking model.',\n",
              " \"A Physical Metaphor to Study Semantic Drift\\nIn accessibility tests for digital preservation, over time we experience\\ndrifts of localized and labelled content in statistical models of evolving\\nsemantics represented as a vector field. This articulates the need to detect,\\nmeasure, interpret and model outcomes of knowledge dynamics. To this end we\\nemploy a high-performance machine learning algorithm for the training of\\nextremely large emergent self-organizing maps for exploratory data analysis.\\nThe working hypothesis we present here is that the dynamics of semantic drifts\\ncan be modeled on a relaxed version of Newtonian mechanics called social\\nmechanics. By using term distances as a measure of semantic relatedness vs.\\ntheir PageRank values indicating social importance and applied as variable\\n`term mass', gravitation as a metaphor to express changes in the semantic\\ncontent of a vector field lends a new perspective for experimentation. From\\n`term gravitation' over time, one can compute its generating potential whose\\nfluctuations manifest modifications in pairwise term similarity vs. social\\nimportance, thereby updating Osgood's semantic differential. The dataset\\nexamined is the public catalog metadata of Tate Galleries, London.\",\n",
              " \"Optimizing Neural Network Hyperparameters with Gaussian Processes for\\n  Dialog Act Classification\\nSystems based on artificial neural networks (ANNs) have achieved\\nstate-of-the-art results in many natural language processing tasks. Although\\nANNs do not require manually engineered features, ANNs have many\\nhyperparameters to be optimized. The choice of hyperparameters significantly\\nimpacts models' performances. However, the ANN hyperparameters are typically\\nchosen by manual, grid, or random search, which either requires expert\\nexperiences or is computationally expensive. Recent approaches based on\\nBayesian optimization using Gaussian processes (GPs) is a more systematic way\\nto automatically pinpoint optimal or near-optimal machine learning\\nhyperparameters. Using a previously published ANN model yielding\\nstate-of-the-art results for dialog act classification, we demonstrate that\\noptimizing hyperparameters using GP further improves the results, and reduces\\nthe computational time by a factor of 4 compared to a random search. Therefore\\nit is a useful technique for tuning ANN models to yield the best performances\\nfor natural language processing tasks.\",\n",
              " 'A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder\\nSpeech Translation has always been about giving source text or audio input\\nand waiting for system to give translated output in desired form. In this\\npaper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice\\near-piece translation device. We introduce and survey the recent advances made\\nin the field of Speech Engineering, to employ in the ADD, particularly focusing\\non the three major processing steps of Recognition, Translation and Synthesis.\\nWe tackle the problem of machine understanding of natural language by designing\\na recognition unit for source audio to text, a translation unit for source\\nlanguage text to target language text, and a synthesis unit for target language\\ntext to target language speech. Speech from the surroundings will be recorded\\nby the recognition unit present on the ear-piece and translation will start as\\nsoon as one sentence is successfully read. This way, we hope to give translated\\noutput as and when input is being read. The recognition unit will use Hidden\\nMarkov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory\\ncells, and the synthesis unit, HMM based speech synthesis system HTS. This\\nsystem will initially be built as an English to Tamil translation device.',\n",
              " 'Learning to Reason With Adaptive Computation\\nMulti-hop inference is necessary for machine learning systems to successfully\\nsolve tasks such as Recognising Textual Entailment and Machine Reading. In this\\nwork, we demonstrate the effectiveness of adaptive computation for learning the\\nnumber of inference steps required for examples of different complexity and\\nthat learning the correct number of inference steps is difficult. We introduce\\nthe first model involving Adaptive Computation Time which provides a small\\nperformance benefit on top of a similar model without an adaptive component as\\nwell as enabling considerable insight into the reasoning process of the model.',\n",
              " \"Feature-Augmented Neural Networks for Patient Note De-identification\\nPatient notes contain a wealth of information of potentially great interest\\nto medical investigators. However, to protect patients' privacy, Protected\\nHealth Information (PHI) must be removed from the patient notes before they can\\nbe legally released, a process known as patient note de-identification. The\\nmain objective for a de-identification system is to have the highest possible\\nrecall. Recently, the first neural-network-based de-identification system has\\nbeen proposed, yielding state-of-the-art results. Unlike other systems, it does\\nnot rely on human-engineered features, which allows it to be quickly deployed,\\nbut does not leverage knowledge from human experts or from electronic health\\nrecords (EHRs). In this work, we explore a method to incorporate\\nhuman-engineered features as well as features derived from EHRs to a\\nneural-network-based de-identification system. Our results show that the\\naddition of features, especially the EHR-derived features, further improves the\\nstate-of-the-art in patient note de-identification, including for some of the\\nmost sensitive PHI types such as patient names. Since in a real-life setting\\npatient notes typically come with EHRs, we recommend developers of\\nde-identification systems to leverage the information EHRs contain.\",\n",
              " 'Direct Acoustics-to-Word Models for English Conversational Speech\\n  Recognition\\nRecent work on end-to-end automatic speech recognition (ASR) has shown that\\nthe connectionist temporal classification (CTC) loss can be used to convert\\nacoustics to phone or character sequences. Such systems are used with a\\ndictionary and separately-trained Language Model (LM) to produce word\\nsequences. However, they are not truly end-to-end in the sense of mapping\\nacoustics directly to words without an intermediate phone representation. In\\nthis paper, we present the first results employing direct acoustics-to-word CTC\\nmodels on two well-known public benchmark tasks: Switchboard and CallHome.\\nThese models do not require an LM or even a decoder at run-time and hence\\nrecognize speech with minimal complexity. However, due to the large number of\\nword output units, CTC word models require orders of magnitude more data to\\ntrain reliably compared to traditional systems. We present some techniques to\\nmitigate this issue. Our CTC word model achieves a word error rate of\\n13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or\\ndecoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also\\npresent rescoring results on CTC word model lattices to quantify the\\nperformance benefits of a LM, and contrast the performance of word and phone\\nCTC models.',\n",
              " 'Factorization tricks for LSTM networks\\nWe present two simple ways of reducing the number of parameters and\\naccelerating the training of large Long Short-Term Memory (LSTM) networks: the\\nfirst one is \"matrix factorization by design\" of LSTM matrix into the product\\nof two smaller matrices, and the second one is partitioning of LSTM matrix, its\\ninputs and states into the independent groups. Both approaches allow us to\\ntrain large LSTM networks significantly faster to the near state-of the art\\nperplexity while using significantly less RNN parameters.',\n",
              " \"NeuroNER: an easy-to-use program for named-entity recognition based on\\n  neural networks\\nNamed-entity recognition (NER) aims at identifying entities of interest in a\\ntext. Artificial neural networks (ANNs) have recently been shown to outperform\\nexisting NER systems. However, ANNs remain challenging to use for non-expert\\nusers. In this paper, we present NeuroNER, an easy-to-use named-entity\\nrecognition tool based on ANNs. Users can annotate entities using a graphical\\nweb-based user interface (BRAT): the annotations are then used to train an ANN,\\nwhich in turn predict entities' locations and categories in new texts. NeuroNER\\nmakes this annotation-training-prediction flow smooth and accessible to anyone.\",\n",
              " 'Syllable-aware Neural Language Models: A Failure to Beat Character-aware\\n  Ones\\nSyllabification does not seem to improve word-level RNN language modeling\\nquality when compared to character-based segmentation. However, our best\\nsyllable-aware language model, achieving performance comparable to the\\ncompetitive character-aware model, has 18%-33% fewer parameters and is trained\\n1.2-2.2 times faster.',\n",
              " 'A Benchmarking Environment for Reinforcement Learning Based Task\\n  Oriented Dialogue Management\\nDialogue assistants are rapidly becoming an indispensable daily aid. To avoid\\nthe significant effort needed to hand-craft the required dialogue flow, the\\nDialogue Management (DM) module can be cast as a continuous Markov Decision\\nProcess (MDP) and trained through Reinforcement Learning (RL). Several RL\\nmodels have been investigated over recent years. However, the lack of a common\\nbenchmarking framework makes it difficult to perform a fair comparison between\\ndifferent models and their capability to generalise to different environments.\\nTherefore, this paper proposes a set of challenging simulated environments for\\ndialogue model development and evaluation. To provide some baselines, we\\ninvestigate a number of representative parametric algorithms, namely deep\\nreinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and\\ncompare them to a non-parametric model, GP-SARSA. Both the environments and\\npolicy models are implemented using the publicly available PyDial toolkit and\\nreleased on-line, in order to establish a testbed framework for further\\nexperiments and to facilitate experimental reproducibility.',\n",
              " 'Reusing Weights in Subword-aware Neural Language Models\\nWe propose several ways of reusing subword embeddings and other weights in\\nsubword-aware neural language models. The proposed techniques do not benefit a\\ncompetitive character-aware model, but some of them improve the performance of\\nsyllable- and morpheme-aware models while showing significant reductions in\\nmodel sizes. We discover a simple hands-on principle: in a multi-layer input\\nembedding model, layers should be tied consecutively bottom-up if reused at\\noutput. Our best morpheme-aware model with properly reused weights beats the\\ncompetitive word-level model by a large margin across multiple languages and\\nhas 20%-87% fewer parameters.',\n",
              " 'Multi-task Learning of Pairwise Sequence Classification Tasks Over\\n  Disparate Label Spaces\\nWe combine multi-task learning and semi-supervised learning by inducing a\\njoint embedding space between disparate label spaces and learning transfer\\nfunctions between label embeddings, enabling us to jointly leverage unlabelled\\ndata and auxiliary, annotated datasets. We evaluate our approach on a variety\\nof sequence classification tasks with disparate label spaces. We outperform\\nstrong single and multi-task baselines and achieve a new state-of-the-art for\\naspect- and topic-based sentiment analysis.',\n",
              " 'Dynamic Memory Networks for Visual and Textual Question Answering\\nNeural network architectures with memory and attention mechanisms exhibit\\ncertain reasoning capabilities required for question answering. One such\\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\\nvariety of language tasks. However, it was not shown whether the architecture\\nachieves strong results for question answering when supporting facts are not\\nmarked during training or whether it could be applied to other modalities such\\nas images. Based on an analysis of the DMN, we propose several improvements to\\nits memory and input modules. Together with these changes we introduce a novel\\ninput module for images in order to be able to answer visual questions. Our new\\nDMN+ model improves the state of the art on both the Visual Question Answering\\ndataset and the \\\\babi-10k text question-answering dataset without supporting\\nfact supervision.',\n",
              " 'Picture It In Your Mind: Generating High Level Visual Representations\\n  From Textual Descriptions\\nIn this paper we tackle the problem of image search when the query is a short\\ntextual description of the image the user is looking for. We choose to\\nimplement the actual search process as a similarity search in a visual feature\\nspace, by learning to translate a textual query into a visual representation.\\nSearching in the visual feature space has the advantage that any update to the\\ntranslation model does not require to reprocess the, typically huge, image\\ncollection on which the search is performed. We propose Text2Vis, a neural\\nnetwork that generates a visual representation, in the visual feature space of\\nthe fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis\\noptimizes two loss functions, using a stochastic loss-selection method. A\\nvisual-focused loss is aimed at learning the actual text-to-visual feature\\nmapping, while a text-focused loss is aimed at modeling the higher-level\\nsemantic concepts expressed in language and countering the overfit on\\nnon-relevant visual components of the visual loss. We report preliminary\\nresults on the MS-COCO dataset.',\n",
              " \"Where to put the Image in an Image Caption Generator\\nWhen a recurrent neural network language model is used for caption\\ngeneration, the image information can be fed to the neural network either by\\ndirectly incorporating it in the RNN -- conditioning the language model by\\n`injecting' image features -- or in a layer following the RNN -- conditioning\\nthe language model by `merging' image features. While both options are attested\\nin the literature, there is as yet no systematic comparison between the two. In\\nthis paper we empirically show that it is not especially detrimental to\\nperformance whether one architecture is used or another. The merge architecture\\ndoes have practical advantages, as conditioning by merging allows the RNN's\\nhidden state vector to shrink in size by up to four times. Our results suggest\\nthat the visual and linguistic modalities for caption generation need not be\\njointly encoded by the RNN as that yields large, memory-intensive models with\\nfew tangible advantages in performance; rather, the multimodal integration\\nshould be delayed to a subsequent stage.\",\n",
              " 'A Focused Dynamic Attention Model for Visual Question Answering\\nVisual Question and Answering (VQA) problems are attracting increasing\\ninterest from multiple research disciplines. Solving VQA problems requires\\ntechniques from both computer vision for understanding the visual contents of a\\npresented image or video, as well as the ones from natural language processing\\nfor understanding semantics of the question and generating the answers.\\nRegarding visual content modeling, most of existing VQA methods adopt the\\nstrategy of extracting global features from the image or video, which\\ninevitably fails in capturing fine-grained information such as spatial\\nconfiguration of multiple objects. Extracting features from auto-generated\\nregions -- as some region-based image recognition methods do -- cannot\\nessentially address this problem and may introduce some overwhelming irrelevant\\nfeatures with the question. In this work, we propose a novel Focused Dynamic\\nAttention (FDA) model to provide better aligned image content representation\\nwith proposed questions. Being aware of the key words in the question, FDA\\nemploys off-the-shelf object detector to identify important regions and fuse\\nthe information from the regions and global features via an LSTM unit. Such\\nquestion-driven representations are then combined with question representation\\nand fed into a reasoning unit for generating the answers. Extensive evaluation\\non a large-scale benchmark dataset, VQA, clearly demonstrate the superior\\nperformance of FDA over well-established baselines.',\n",
              " 'Simple Image Description Generator via a Linear Phrase-Based Approach\\nGenerating a novel textual description of an image is an interesting problem\\nthat connects computer vision and natural language processing. In this paper,\\nwe present a simple model that is able to generate descriptive sentences given\\na sample image. This model has a strong focus on the syntax of the\\ndescriptions. We train a purely bilinear model that learns a metric between an\\nimage representation (generated from a previously trained Convolutional Neural\\nNetwork) and phrases that are used to described them. The system is then able\\nto infer phrases from a given image sample. Based on caption syntax statistics,\\nwe propose a simple language model that can produce relevant descriptions for a\\ngiven test image using the phrases inferred. Our approach, which is\\nconsiderably simpler than state-of-the-art models, achieves comparable results\\non the recently release Microsoft COCO dataset.',\n",
              " 'Multimodal Convolutional Neural Networks for Matching Image and Sentence\\nIn this paper, we propose multimodal convolutional neural networks (m-CNNs)\\nfor matching image and sentence. Our m-CNN provides an end-to-end framework\\nwith convolutional architectures to exploit image representation, word\\ncomposition, and the matching relations between the two modalities. More\\nspecifically, it consists of one image CNN encoding the image content, and one\\nmatching CNN learning the joint representation of image and sentence. The\\nmatching CNN composes words to different semantic fragments and learns the\\ninter-modal relations between image and the composed fragments at different\\nlevels, thus fully exploit the matching relations between image and sentence.\\nExperimental results on benchmark databases of bidirectional image and sentence\\nretrieval demonstrate that the proposed m-CNNs can effectively capture the\\ninformation necessary for image and sentence matching. Specifically, our\\nproposed m-CNNs for bidirectional image and sentence retrieval on Flickr30K and\\nMicrosoft COCO databases achieve the state-of-the-art performances.',\n",
              " 'Learning to Compose Neural Networks for Question Answering\\nWe describe a question answering model that applies to both images and\\nstructured knowledge bases. The model uses natural language strings to\\nautomatically assemble neural networks from a collection of composable modules.\\nParameters for these modules are learned jointly with network-assembly\\nparameters via reinforcement learning, with only (world, question, answer)\\ntriples as supervision. Our approach, which we term a dynamic neural model\\nnetwork, achieves state-of-the-art results on benchmark datasets in both visual\\nand structured domains.',\n",
              " 'Signer-independent Fingerspelling Recognition with Deep Neural Network\\n  Adaptation\\nWe study the problem of recognition of fingerspelled letter sequences in\\nAmerican Sign Language in a signer-independent setting. Fingerspelled sequences\\nare both challenging and important to recognize, as they are used for many\\ncontent words such as proper nouns and technical terms. Previous work has shown\\nthat it is possible to achieve almost 90% accuracies on fingerspelling\\nrecognition in a signer-dependent setting. However, the more realistic\\nsigner-independent setting presents challenges due to significant variations\\namong signers, coupled with the dearth of available training data. We\\ninvestigate this problem with approaches inspired by automatic speech\\nrecognition. We start with the best-performing approaches from prior work,\\nbased on tandem models and segmental conditional random fields (SCRFs), with\\nfeatures based on deep neural network (DNN) classifiers of letters and\\nphonological features. Using DNN adaptation, we find that it is possible to\\nbridge a large part of the gap between signer-dependent and signer-independent\\nperformance. Using only about 115 transcribed words for adaptation from the\\ntarget signer, we obtain letter accuracies of up to 82.7% with frame-level\\nadaptation labels and 69.7% with only word labels.',\n",
              " 'Full-Network Embedding in a Multimodal Embedding Pipeline\\nThe current state-of-the-art for image annotation and image retrieval tasks\\nis obtained through deep neural networks, which combine an image representation\\nand a text representation into a shared embedding space. In this paper we\\nevaluate the impact of using the Full-Network embedding in this setting,\\nreplacing the original image representation in a competitive multimodal\\nembedding generation scheme. Unlike the one-layer image embeddings typically\\nused by most approaches, the Full-Network embedding provides a multi-scale\\nrepresentation of images, which results in richer characterizations. To measure\\nthe influence of the Full-Network embedding, we evaluate its performance on\\nthree different datasets, and compare the results with the original multimodal\\nembedding generation scheme when using a one-layer image embedding, and with\\nthe rest of the state-of-the-art. Results for image annotation and image\\nretrieval tasks indicate that the Full-Network embedding is consistently\\nsuperior to the one-layer embedding. These results motivate the integration of\\nthe Full-Network embedding on any multimodal embedding generation scheme,\\nsomething feasible thanks to the flexibility of the approach.',\n",
              " \"What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption\\n  Generator?\\nIn neural image captioning systems, a recurrent neural network (RNN) is\\ntypically viewed as the primary `generation' component. This view suggests that\\nthe image features should be `injected' into the RNN. This is in fact the\\ndominant view in the literature. Alternatively, the RNN can instead be viewed\\nas only encoding the previously generated words. This view suggests that the\\nRNN should only be used to encode linguistic features and that only the final\\nrepresentation should be `merged' with the image features at a later stage.\\nThis paper compares these two architectures. We find that, in general, late\\nmerging outperforms injection, suggesting that RNNs are better viewed as\\nencoders, rather than generators.\",\n",
              " 'A Fixed-Size Encoding Method for Variable-Length Sequences with its\\n  Application to Neural Network Language Models\\nIn this paper, we propose the new fixed-size ordinally-forgetting encoding\\n(FOFE) method, which can almost uniquely encode any variable-length sequence of\\nwords into a fixed-size representation. FOFE can model the word order in a\\nsequence using a simple ordinally-forgetting mechanism according to the\\npositions of words. In this work, we have applied FOFE to feedforward neural\\nnetwork language models (FNN-LMs). Experimental results have shown that without\\nusing any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform\\nnot only the standard fixed-input FNN-LMs but also the popular RNN-LMs.',\n",
              " \"Transition-Based Dependency Parsing with Stack Long Short-Term Memory\\nWe propose a technique for learning representations of parser states in\\ntransition-based dependency parsers. Our primary innovation is a new control\\nstructure for sequence-to-sequence neural networks---the stack LSTM. Like the\\nconventional stack data structures used in transition-based parsing, elements\\ncan be pushed to or popped from the top of the stack in constant time, but, in\\naddition, an LSTM maintains a continuous space embedding of the stack contents.\\nThis lets us formulate an efficient parsing model that captures three facets of\\na parser's state: (i) unbounded look-ahead into the buffer of incoming words,\\n(ii) the complete history of actions taken by the parser, and (iii) the\\ncomplete contents of the stack of partially built tree fragments, including\\ntheir internal structures. Standard backpropagation techniques are used for\\ntraining and yield state-of-the-art parsing performance.\",\n",
              " 'A Semisupervised Approach for Language Identification based on Ladder\\n  Networks\\nIn this study we address the problem of training a neuralnetwork for language\\nidentification using both labeled and unlabeled speech samples in the form of\\ni-vectors. We propose a neural network architecture that can also handle\\nout-of-set languages. We utilize a modified version of the recently proposed\\nLadder Network semisupervised training procedure that optimizes the\\nreconstruction costs of a stack of denoising autoencoders. We show that this\\napproach can be successfully applied to the case where the training dataset is\\ncomposed of both labeled and unlabeled acoustic data. The results show enhanced\\nlanguage identification on the NIST 2015 language identification dataset.',\n",
              " 'First-Pass Large Vocabulary Continuous Speech Recognition using\\n  Bi-Directional Recurrent DNNs\\nWe present a method to perform first-pass large vocabulary continuous speech\\nrecognition using only a neural network and language model. Deep neural network\\nacoustic models are now commonplace in HMM-based speech recognition systems,\\nbut building such systems is a complex, domain-specific task. Recent work\\ndemonstrated the feasibility of discarding the HMM sequence modeling framework\\nby directly predicting transcript text from audio. This paper extends this\\napproach in two ways. First, we demonstrate that a straightforward recurrent\\nneural network architecture can achieve a high level of accuracy. Second, we\\npropose and evaluate a modified prefix-search decoding algorithm. This approach\\nto decoding enables first-pass speech recognition with a language model,\\ncompletely unaided by the cumbersome infrastructure of HMM-based systems.\\nExperiments on the Wall Street Journal corpus demonstrate fairly competitive\\nword error rates, and the importance of bi-directional network recurrence.',\n",
              " \"Applying deep learning techniques on medical corpora from the World Wide\\n  Web: a prototypical system and evaluation\\nBACKGROUND: The amount of biomedical literature is rapidly growing and it is\\nbecoming increasingly difficult to keep manually curated knowledge bases and\\nontologies up-to-date. In this study we applied the word2vec deep learning\\ntoolkit to medical corpora to test its potential for identifying relationships\\nfrom unstructured text. We evaluated the efficiency of word2vec in identifying\\nproperties of pharmaceuticals based on mid-sized, unstructured medical text\\ncorpora available on the web. Properties included relationships to diseases\\n('may treat') or physiological processes ('has physiological effect'). We\\ncompared the relationships identified by word2vec with manually curated\\ninformation from the National Drug File - Reference Terminology (NDF-RT)\\nontology as a gold standard. RESULTS: Our results revealed a maximum accuracy\\nof 49.28% which suggests a limited ability of word2vec to capture linguistic\\nregularities on the collected medical corpora compared with other published\\nresults. We were able to document the influence of different parameter settings\\non result accuracy and found and unexpected trade-off between ranking quality\\nand accuracy. Pre-processing corpora to reduce syntactic variability proved to\\nbe a good strategy for increasing the utility of the trained vector models.\\nCONCLUSIONS: Word2vec is a very efficient implementation for computing vector\\nrepresentations and for its ability to identify relationships in textual data\\nwithout any prior domain knowledge. We found that the ranking and retrieved\\nresults generated by word2vec were not of sufficient quality for automatic\\npopulation of knowledge bases and ontologies, but could serve as a starting\\npoint for further manual curation.\",\n",
              " 'Syntax-based Deep Matching of Short Texts\\nMany tasks in natural language processing, ranging from machine translation\\nto question answering, can be reduced to the problem of matching two sentences\\nor more generally two short texts. We propose a new approach to the problem,\\ncalled Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The\\napproach consists of two components, 1) a mining algorithm to discover patterns\\nfor matching two short-texts, defined in the product space of dependency trees,\\nand 2) a deep neural network for matching short texts using the mined patterns,\\nas well as a learning algorithm to build the network having a sparse structure.\\nWe test our algorithm on the problem of matching a tweet and a response in\\nsocial media, a hard matching problem proposed in [Wang et al., 2013], and show\\nthat DeepMatch$_{tree}$ can outperform a number of competitor models including\\none without using dependency trees and one based on word-embedding, all with\\nlarge margins',\n",
              " 'Ensemble of Generative and Discriminative Techniques for Sentiment\\n  Analysis of Movie Reviews\\nSentiment analysis is a common task in natural language processing that aims\\nto detect polarity of a text document (typically a consumer review). In the\\nsimplest settings, we discriminate only between positive and negative\\nsentiment, turning the task into a standard binary classification problem. We\\ncompare several ma- chine learning approaches to this problem, and combine them\\nto achieve the best possible results. We show how to use for this task the\\nstandard generative lan- guage models, which are slightly complementary to the\\nstate of the art techniques. We achieve strong results on a well-known dataset\\nof IMDB movie reviews. Our results are easily reproducible, as we publish also\\nthe code needed to repeat the experiments. This should simplify further advance\\nof the state of the art, as other researchers can combine their techniques with\\nours with little effort.',\n",
              " 'Diverse Embedding Neural Network Language Models\\nWe propose Diverse Embedding Neural Network (DENN), a novel architecture for\\nlanguage models (LMs). A DENNLM projects the input word history vector onto\\nmultiple diverse low-dimensional sub-spaces instead of a single\\nhigher-dimensional sub-space as in conventional feed-forward neural network\\nLMs. We encourage these sub-spaces to be diverse during network training\\nthrough an augmented loss function. Our language modeling experiments on the\\nPenn Treebank data set show the performance benefit of using a DENNLM.',\n",
              " 'Learning linearly separable features for speech recognition using\\n  convolutional neural networks\\nAutomatic speech recognition systems usually rely on spectral-based features,\\nsuch as MFCC of PLP. These features are extracted based on prior knowledge such\\nas, speech perception or/and speech production. Recently, convolutional neural\\nnetworks have been shown to be able to estimate phoneme conditional\\nprobabilities in a completely data-driven manner, i.e. using directly temporal\\nraw speech signal as input. This system was shown to yield similar or better\\nperformance than HMM/ANN based system on phoneme recognition task and on large\\nscale continuous speech recognition task, using less parameters. Motivated by\\nthese studies, we investigate the use of simple linear classifier in the\\nCNN-based framework. Thus, the network learns linearly separable features from\\nraw speech. We show that such system yields similar or better performance than\\nMLP based system using cepstral-based features as input.',\n",
              " 'Learning to Transduce with Unbounded Memory\\nRecently, strong results have been demonstrated by Deep Recurrent Neural\\nNetworks on natural language transduction problems. In this paper we explore\\nthe representational power of these models using synthetic grammars designed to\\nexhibit phenomena similar to those found in real transduction problems such as\\nmachine translation. These experiments lead us to propose new memory-based\\nrecurrent networks that implement continuously differentiable analogues of\\ntraditional data structures such as Stacks, Queues, and DeQues. We show that\\nthese architectures exhibit superior generalisation performance to Deep RNNs\\nand are often able to learn the underlying generating algorithms in our\\ntransduction experiments.',\n",
              " 'Feedforward Sequential Memory Neural Networks without Recurrent Feedback\\nWe introduce a new structure for memory neural networks, called feedforward\\nsequential memory networks (FSMN), which can learn long-term dependency without\\nusing recurrent feedback. The proposed FSMN is a standard feedforward neural\\nnetworks equipped with learnable sequential memory blocks in the hidden layers.\\nIn this work, we have applied FSMN to several language modeling (LM) tasks.\\nExperimental results have shown that the memory blocks in FSMN can learn\\neffective representations of long history. Experiments have shown that FSMN\\nbased language models can significantly outperform not only feedforward neural\\nnetwork (FNN) based LMs but also the popular recurrent neural network (RNN)\\nLMs.',\n",
              " 'Towards Structured Deep Neural Network for Automatic Speech Recognition\\nIn this paper we propose the Structured Deep Neural Network (structured DNN)\\nas a structured and deep learning framework. This approach can learn to find\\nthe best structured object (such as a label sequence) given a structured input\\n(such as a vector sequence) by globally considering the mapping relationships\\nbetween the structures rather than item by item.\\n  When automatic speech recognition is viewed as a special case of such a\\nstructured learning problem, where we have the acoustic vector sequence as the\\ninput and the phoneme label sequence as the output, it becomes possible to\\ncomprehensively learn utterance by utterance as a whole, rather than frame by\\nframe.\\n  Structured Support Vector Machine (structured SVM) was proposed to perform\\nASR with structured learning previously, but limited by the linear nature of\\nSVM. Here we propose structured DNN to use nonlinear transformations in\\nmulti-layers as a structured and deep learning approach. This approach was\\nshown to beat structured SVM in preliminary experiments on TIMIT.',\n",
              " \"Character-Level Incremental Speech Recognition with Recurrent Neural\\n  Networks\\nIn real-time speech recognition applications, the latency is an important\\nissue. We have developed a character-level incremental speech recognition (ISR)\\nsystem that responds quickly even during the speech, where the hypotheses are\\ngradually improved while the speaking proceeds. The algorithm employs a\\nspeech-to-character unidirectional recurrent neural network (RNN), which is\\nend-to-end trained with connectionist temporal classification (CTC), and an\\nRNN-based character-level language model (LM). The output values of the\\nCTC-trained RNN are character-level probabilities, which are processed by beam\\nsearch decoding. The RNN LM augments the decoding by providing long-term\\ndependency information. We propose tree-based online beam search with\\nadditional depth-pruning, which enables the system to process infinitely long\\ninput speech with low latency. This system not only responds quickly on speech\\nbut also can dictate out-of-vocabulary (OOV) words according to pronunciation.\\nThe proposed model achieves the word error rate (WER) of 8.90% on the Wall\\nStreet Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284\\ntraining set.\",\n",
              " 'Globally Normalized Transition-Based Neural Networks\\nWe introduce a globally normalized transition-based neural network model that\\nachieves state-of-the-art part-of-speech tagging, dependency parsing and\\nsentence compression results. Our model is a simple feed-forward neural network\\nthat operates on a task-specific transition system, yet achieves comparable or\\nbetter accuracies than recurrent models. We discuss the importance of global as\\nopposed to local normalization: a key insight is that the label bias problem\\nimplies that globally normalized models can be strictly more expressive than\\nlocally normalized models.',\n",
              " 'Clinical Information Extraction via Convolutional Neural Network\\nWe report an implementation of a clinical information extraction tool that\\nleverages deep neural network to annotate event spans and their attributes from\\nraw clinical notes and pathology reports. Our approach uses context words and\\ntheir part-of-speech tags and shape information as features. Then we hire\\ntemporal (1D) convolutional neural network to learn hidden feature\\nrepresentations. Finally, we use Multilayer Perceptron (MLP) to predict event\\nspans. The empirical evaluation demonstrates that our approach significantly\\noutperforms baselines.',\n",
              " 'Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations\\nWe propose zoneout, a novel method for regularizing RNNs. At each timestep,\\nzoneout stochastically forces some hidden units to maintain their previous\\nvalues. Like dropout, zoneout uses random noise to train a pseudo-ensemble,\\nimproving generalization. But by preserving instead of dropping hidden units,\\ngradient information and state information are more readily propagated through\\ntime, as in feedforward stochastic depth networks. We perform an empirical\\ninvestigation of various RNN regularizers, and find that zoneout gives\\nsignificant performance improvements across tasks. We achieve competitive\\nresults with relatively simple models in character- and word-level language\\nmodelling on the Penn Treebank and Text8 datasets, and combining with recurrent\\nbatch normalization yields state-of-the-art results on permuted sequential\\nMNIST.',\n",
              " 'Stance Detection with Bidirectional Conditional Encoding\\nStance detection is the task of classifying the attitude expressed in a text\\ntowards a target such as Hillary Clinton to be \"positive\", negative\" or\\n\"neutral\". Previous work has assumed that either the target is mentioned in the\\ntext or that training data for every target is given. This paper considers the\\nmore challenging version of this task, where targets are not always mentioned\\nand no training data is available for the test targets. We experiment with\\nconditional LSTM encoding, which builds a representation of the tweet that is\\ndependent on the target, and demonstrate that it outperforms encoding the tweet\\nand the target independently. Performance is improved further when the\\nconditional model is augmented with bidirectional encoding. We evaluate our\\napproach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving\\nperformance second best only to a system trained on semi-automatically labelled\\ntweets for the test target. When such weak supervision is added, our approach\\nachieves state-of-the-art results.',\n",
              " 'SMS Spam Filtering using Probabilistic Topic Modelling and Stacked\\n  Denoising Autoencoder\\nIn This paper we present a novel approach to spam filtering and demonstrate\\nits applicability with respect to SMS messages. Our approach requires minimum\\nfeatures engineering and a small set of la- belled data samples. Features are\\nextracted using topic modelling based on latent Dirichlet allocation, and then\\na comprehensive data model is created using a Stacked Denoising Autoencoder\\n(SDA). Topic modelling summarises the data providing ease of use and high\\ninterpretability by visualising the topics using word clouds. Given that the\\nSMS messages can be regarded as either spam (unwanted) or ham (wanted), the SDA\\nis able to model the messages and accurately discriminate between the two\\nclasses without the need for a pre-labelled training set. The results are\\ncompared against the state-of-the-art spam detection algorithms with our\\nproposed approach achieving over 97% accuracy which compares favourably to the\\nbest reported algorithms presented in the literature.',\n",
              " 'Bidirectional Recurrent Neural Networks for Medical Event Detection in\\n  Electronic Health Records\\nSequence labeling for extraction of medical events and their attributes from\\nunstructured text in Electronic Health Record (EHR) notes is a key step towards\\nsemantic understanding of EHRs. It has important applications in health\\ninformatics including pharmacovigilance and drug surveillance. The state of the\\nart supervised machine learning models in this domain are based on Conditional\\nRandom Fields (CRFs) with features calculated from fixed context windows. In\\nthis application, we explored various recurrent neural network frameworks and\\nshow that they significantly outperformed the CRF models.',\n",
              " 'Sequence Training and Adaptation of Highway Deep Neural Networks\\nHighway deep neural network (HDNN) is a type of depth-gated feedforward\\nneural network, which has shown to be easier to train with more hidden layers\\nand also generalise better compared to conventional plain deep neural networks\\n(DNNs). Previously, we investigated a structured HDNN architecture for speech\\nrecognition, in which the two gate functions were tied across all the hidden\\nlayers, and we were able to train a much smaller model without sacrificing the\\nrecognition accuracy. In this paper, we carry on the study of this architecture\\nwith sequence-discriminative training criterion and speaker adaptation\\ntechniques on the AMI meeting speech recognition corpus. We show that these two\\ntechniques improve speech recognition accuracy on top of the model trained with\\nthe cross entropy criterion. Furthermore, we demonstrate that the two gate\\nfunctions that are tied across all the hidden layers are able to control the\\ninformation flow over the whole network, and we can achieve considerable\\nimprovements by only updating these gate functions in both sequence training\\nand adaptation experiments.',\n",
              " \"Recurrent Highway Networks\\nMany sequential processing tasks require complex nonlinear transition\\nfunctions from one step to the next. However, recurrent neural networks with\\n'deep' transition functions remain difficult to train, even when using Long\\nShort-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of\\nrecurrent networks based on Gersgorin's circle theorem that illuminates several\\nmodeling and optimization issues and improves our understanding of the LSTM\\ncell. Based on this analysis we propose Recurrent Highway Networks, which\\nextend the LSTM architecture to allow step-to-step transition depths larger\\nthan one. Several language modeling experiments demonstrate that the proposed\\narchitecture results in powerful and efficient models. On the Penn Treebank\\ncorpus, solely increasing the transition depth from 1 to 10 improves word-level\\nperplexity from 90.6 to 65.4 using the same number of parameters. On the larger\\nWikipedia datasets for character prediction (text8 and enwik8), RHNs outperform\\nall previous results and achieve an entropy of 1.27 bits per character.\",\n",
              " 'Towards cross-lingual distributed representations without parallel text\\n  trained with adversarial autoencoders\\nCurrent approaches to learning vector representations of text that are\\ncompatible between different languages usually require some amount of parallel\\ntext, aligned at word, sentence or at least document level. We hypothesize\\nhowever, that different natural languages share enough semantic structure that\\nit should be possible, in principle, to learn compatible vector representations\\njust by analyzing the monolingual distribution of words.\\n  In order to evaluate this hypothesis, we propose a scheme to map word vectors\\ntrained on a source language to vectors semantically compatible with word\\nvectors trained on a target language using an adversarial autoencoder.\\n  We present preliminary qualitative results and discuss possible future\\ndevelopments of this technique, such as applications to cross-lingual sentence\\nrepresentations.',\n",
              " 'Memory Visualization for Gated Recurrent Neural Networks in Speech\\n  Recognition\\nRecurrent neural networks (RNNs) have shown clear superiority in sequence\\nmodeling, particularly the ones with gated units, such as long short-term\\nmemory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties\\nbehind the remarkable performance remain unclear in many applications, e.g.,\\nautomatic speech recognition (ASR). This paper employs visualization techniques\\nto study the behavior of LSTM and GRU when performing speech recognition tasks.\\nOur experiments show some interesting patterns in the gated memory, and some of\\nthem have inspired simple yet effective modifications on the network structure.\\nWe report two of such modifications: (1) lazy cell update in LSTM, and (2)\\nshortcut connections for residual learning. Both modifications lead to more\\ncomprehensible and powerful networks.',\n",
              " 'Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large\\n  Vocabulary Speech Recognition\\nWe present results that show it is possible to build a competitive, greatly\\nsimplified, large vocabulary continuous speech recognition system with whole\\nwords as acoustic units. We model the output vocabulary of about 100,000 words\\ndirectly using deep bi-directional LSTM RNNs with CTC loss. The model is\\ntrained on 125,000 hours of semi-supervised acoustic training data, which\\nenables us to alleviate the data sparsity problem for word models. We show that\\nthe CTC word models work very well as an end-to-end all-neural speech\\nrecognition model without the use of traditional context-dependent sub-word\\nphone units that require a pronunciation lexicon, and without any language\\nmodel removing the need to decode. We demonstrate that the CTC word models\\nperform better than a strong, more complex, state-of-the-art baseline with\\nsub-word units.',\n",
              " \"Unsupervised Pretraining for Sequence to Sequence Learning\\nThis work presents a general unsupervised learning method to improve the\\naccuracy of sequence to sequence (seq2seq) models. In our method, the weights\\nof the encoder and decoder of a seq2seq model are initialized with the\\npretrained weights of two language models and then fine-tuned with labeled\\ndata. We apply this method to challenging benchmarks in machine translation and\\nabstractive summarization and find that it significantly improves the\\nsubsequent supervised models. Our main result is that pretraining improves the\\ngeneralization of seq2seq models. We achieve state-of-the art results on the\\nWMT English$\\\\rightarrow$German task, surpassing a range of methods using both\\nphrase-based machine translation and neural machine translation. Our method\\nachieves a significant improvement of 1.3 BLEU from the previous best models on\\nboth WMT'14 and WMT'15 English$\\\\rightarrow$German. We also conduct human\\nevaluations on abstractive summarization and find that our method outperforms a\\npurely supervised learning baseline in a statistically significant manner.\",\n",
              " 'Structured Attention Networks\\nAttention networks have proven to be an effective approach for embedding\\ncategorical inference within a deep neural network. However, for many tasks we\\nmay want to model richer structural dependencies without abandoning end-to-end\\ntraining. In this work, we experiment with incorporating richer structural\\ndistributions, encoded using graphical models, within deep networks. We show\\nthat these structured attention networks are simple extensions of the basic\\nattention procedure, and that they allow for extending attention beyond the\\nstandard soft-selection approach, such as attending to partial segmentations or\\nto subtrees. We experiment with two different classes of structured attention\\nnetworks: a linear-chain conditional random field and a graph-based parsing\\nmodel, and describe how these models can be practically implemented as neural\\nnetwork layers. Experiments show that this approach is effective for\\nincorporating structural biases, and structured attention networks outperform\\nbaseline attention models on a variety of synthetic and real tasks: tree\\ntransduction, neural machine translation, question answering, and natural\\nlanguage inference. We further find that models trained in this way learn\\ninteresting unsupervised hidden representations that generalize simple\\nattention.',\n",
              " \"End-to-End Multi-View Networks for Text Classification\\nWe propose a multi-view network for text classification. Our method\\nautomatically creates various views of its input text, each taking the form of\\nsoft attention weights that distribute the classifier's focus among a set of\\nbase features. For a bag-of-words representation, each view focuses on a\\ndifferent subset of the text's words. Aggregating many such views results in a\\nmore discriminative and robust representation. Through a novel architecture\\nthat both stacks and concatenates views, we produce a network that emphasizes\\nboth depth and width, allowing training to converge quickly. Using our\\nmulti-view architecture, we establish new state-of-the-art accuracies on two\\nbenchmark tasks.\",\n",
              " 'Differentiable Scheduled Sampling for Credit Assignment\\nWe demonstrate that a continuous relaxation of the argmax operation can be\\nused to create a differentiable approximation to greedy decoding for\\nsequence-to-sequence (seq2seq) models. By incorporating this approximation into\\nthe scheduled sampling training procedure (Bengio et al., 2015)--a well-known\\ntechnique for correcting exposure bias--we introduce a new training objective\\nthat is continuous and differentiable everywhere and that can provide\\ninformative gradients near points where previous decoding decisions change\\ntheir value. In addition, by using a related approximation, we demonstrate a\\nsimilar approach to sampled-based training. Finally, we show that our approach\\noutperforms cross-entropy training and scheduled sampling procedures in two\\nsequence prediction tasks: named entity recognition and machine translation.',\n",
              " 'Phone-aware Neural Language Identification\\nPure acoustic neural models, particularly the LSTM-RNN model, have shown\\ngreat potential in language identification (LID). However, the phonetic\\ninformation has been largely overlooked by most of existing neural LID models,\\nalthough this information has been used in the conventional phonetic LID\\nsystems with a great success. We present a phone-aware neural LID architecture,\\nwhich is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR\\nsystem. By utilizing the phonetic knowledge, the LID performance can be\\nsignificantly improved. Interestingly, even if the test language is not\\ninvolved in the ASR training, the phonetic knowledge still presents a large\\ncontribution. Our experiments conducted on four languages within the Babel\\ncorpus demonstrated that the phone-aware approach is highly effective.',\n",
              " 'Detecting Off-topic Responses to Visual Prompts\\nAutomated methods for essay scoring have made great progress in recent years,\\nachieving accuracies very close to human annotators. However, a known weakness\\nof such automated scorers is not taking into account the semantic relevance of\\nthe submitted text. While there is existing work on detecting answer relevance\\ngiven a textual prompt, very little previous research has been done to\\nincorporate visual writing prompts. We propose a neural architecture and\\nseveral extensions for detecting off-topic responses to visual prompts and\\nevaluate it on a dataset of texts written by language learners.',\n",
              " 'Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation\\n  Functions in Quasi-Recurrent Neural Networks\\nIn this paper, we introduce a novel type of Rectified Linear Unit (ReLU),\\ncalled a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an\\nunbounded positive and negative image, can be used as a drop-in replacement for\\na tanh activation function in the recurrent step of Quasi-Recurrent Neural\\nNetworks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less\\nprone to the vanishing gradient problem, they are noise robust, and they induce\\nsparse activations.\\n  We independently reproduce the QRNN experiments of Bradbury et al. (2017) and\\ncompare our DReLU-based QRNNs with the original tanh-based QRNNs and Long\\nShort-Term Memory networks (LSTMs) on sentiment classification and word-level\\nlanguage modeling. Additionally, we evaluate on character-level language\\nmodeling, showing that we are able to stack up to eight QRNN layers with\\nDReLUs, thus making it possible to improve the current state-of-the-art in\\ncharacter-level language modeling over shallow architectures based on LSTMs.',\n",
              " 'Fidelity-Weighted Learning\\nTraining deep neural networks requires many training samples, but in practice\\ntraining labels are expensive to obtain and may be of varying quality, as some\\nmay be from trusted expert labelers while others might be from heuristics or\\nother sources of weak supervision such as crowd-sourcing. This creates a\\nfundamental quality versus-quantity trade-off in the learning process. Do we\\nlearn from the small amount of high-quality data or the potentially large\\namount of weakly-labeled data? We argue that if the learner could somehow know\\nand take the label-quality into account when learning the data representation,\\nwe could get the best of both worlds. To this end, we propose\\n\"fidelity-weighted learning\" (FWL), a semi-supervised student-teacher approach\\nfor training deep neural networks using weakly-labeled data. FWL modulates the\\nparameter updates to a student network (trained on the task we care about) on a\\nper-sample basis according to the posterior confidence of its label-quality\\nestimated by a teacher (who has access to the high-quality labels). Both\\nstudent and teacher are learned from the data. We evaluate FWL on two tasks in\\ninformation retrieval and natural language processing where we outperform\\nstate-of-the-art alternative semi-supervised methods, indicating that our\\napproach makes better use of strong and weak labels, and leads to better\\ntask-dependent data representations.',\n",
              " 'Feature Learning in Deep Neural Networks - Studies on Speech Recognition\\n  Tasks\\nRecent studies have shown that deep neural networks (DNNs) perform\\nsignificantly better than shallow networks and Gaussian mixture models (GMMs)\\non large vocabulary speech recognition tasks. In this paper, we argue that the\\nimproved accuracy achieved by the DNNs is the result of their ability to\\nextract discriminative internal representations that are robust to the many\\nsources of variability in speech signals. We show that these representations\\nbecome increasingly insensitive to small perturbations in the input with\\nincreasing network depth, which leads to better speech recognition performance\\nwith deeper networks. We also show that DNNs cannot extrapolate to test samples\\nthat are substantially different from the training examples. If the training\\ndata are sufficiently representative, however, internal features learned by the\\nDNN are relatively stable with respect to speaker differences, bandwidth\\ndifferences, and environment distortion. This enables DNN-based recognizers to\\nperform as well or better than state-of-the-art systems based on GMMs or\\nshallow networks without the need for explicit model adaptation or feature\\nnormalization.',\n",
              " 'Estimating Phoneme Class Conditional Probabilities from Raw Speech\\n  Signal using Convolutional Neural Networks\\nIn hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic\\nspeech recognition (ASR) system, the phoneme class conditional probabilities\\nare estimated by first extracting acoustic features from the speech signal\\nbased on prior knowledge such as, speech perception or/and speech production\\nknowledge, and, then modeling the acoustic features with an ANN. Recent\\nadvances in machine learning techniques, more specifically in the field of\\nimage processing and text processing, have shown that such divide and conquer\\nstrategy (i.e., separating feature extraction and modeling steps) may not be\\nnecessary. Motivated from these studies, in the framework of convolutional\\nneural networks (CNNs), this paper investigates a novel approach, where the\\ninput to the ANN is raw speech signal and the output is phoneme class\\nconditional probability estimates. On TIMIT phoneme recognition task, we study\\ndifferent ANN architectures to show the benefit of CNNs and compare the\\nproposed approach against conventional approach where, spectral-based feature\\nMFCC is extracted and modeled by a multilayer perceptron. Our studies show that\\nthe proposed approach can yield comparable or better phoneme recognition\\nperformance when compared to the conventional approach. It indicates that CNNs\\ncan learn features relevant for phoneme classification automatically from the\\nraw speech signal.',\n",
              " \"Recursive Neural Networks Can Learn Logical Semantics\\nTree-structured recursive neural networks (TreeRNNs) for sentence meaning\\nhave been successful for many applications, but it remains an open question\\nwhether the fixed-length representations that they learn can support tasks as\\ndemanding as logical deduction. We pursue this question by evaluating whether\\ntwo such models---plain TreeRNNs and tree-structured neural tensor networks\\n(TreeRNTNs)---can correctly learn to identify logical relationships such as\\nentailment and contradiction using these representations. In our first set of\\nexperiments, we generate artificial data from a logical grammar and use it to\\nevaluate the models' ability to learn to handle basic relational reasoning,\\nrecursive structures, and quantification. We then evaluate the models on the\\nmore natural SICK challenge data. Both models perform competitively on the SICK\\ndata and generalize well in all three experiments on simulated data, suggesting\\nthat they can learn suitable representations for logical inference in natural\\nlanguage.\",\n",
              " 'A Re-ranking Model for Dependency Parser with Recursive Convolutional\\n  Neural Network\\nIn this work, we address the problem to model all the nodes (words or\\nphrases) in a dependency tree with the dense representations. We propose a\\nrecursive convolutional neural network (RCNN) architecture to capture syntactic\\nand compositional-semantic representations of phrases and words in a dependency\\ntree. Different with the original recursive neural network, we introduce the\\nconvolution and pooling layers, which can model a variety of compositions by\\nthe feature maps and choose the most informative compositions by the pooling\\nlayers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list\\nof candidate dependency parsing trees. The experiments show that RCNN is very\\neffective to improve the state-of-the-art dependency parsing on both English\\nand Chinese datasets.',\n",
              " 'Deep Speaker Vectors for Semi Text-independent Speaker Verification\\nRecent research shows that deep neural networks (DNNs) can be used to extract\\ndeep speaker vectors (d-vectors) that preserve speaker characteristics and can\\nbe used in speaker verification. This new method has been tested on\\ntext-dependent speaker verification tasks, and improvement was reported when\\ncombined with the conventional i-vector method.\\n  This paper extends the d-vector approach to semi text-independent speaker\\nverification tasks, i.e., the text of the speech is in a limited set of short\\nphrases. We explore various settings of the DNN structure used for d-vector\\nextraction, and present a phone-dependent training which employs the posterior\\nfeatures obtained from an ASR system. The experimental results show that it is\\npossible to apply d-vectors on semi text-independent speaker recognition, and\\nthe phone-dependent training improves system performance.',\n",
              " 'Advances in Very Deep Convolutional Neural Networks for LVCSR\\nVery deep CNNs with small 3x3 kernels have recently been shown to achieve\\nvery strong performance as acoustic models in hybrid NN-HMM speech recognition\\nsystems. In this paper we investigate how to efficiently scale these models to\\nlarger datasets. Specifically, we address the design choice of pooling and\\npadding along the time dimension which renders convolutional evaluation of\\nsequences highly inefficient. We propose a new CNN design without timepadding\\nand without timepooling, which is slightly suboptimal for accuracy, but has two\\nsignificant advantages: it enables sequence training and deployment by allowing\\nefficient convolutional evaluation of full utterances, and, it allows for batch\\nnormalization to be straightforwardly adopted to CNNs on sequence data. Through\\nbatch normalization, we recover the lost peformance from removing the\\ntime-pooling, while keeping the benefit of efficient convolutional evaluation.\\nWe demonstrate the performance of our models both on larger scale data than\\nbefore, and after sequence training. Our very deep CNN model sequence trained\\non the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5\\ntest-set, matching with a single model the performance of the 2015 IBM system\\ncombination, which was the previous best published result.',\n",
              " 'Learning Compact Recurrent Neural Networks\\nRecurrent neural networks (RNNs), including long short-term memory (LSTM)\\nRNNs, have produced state-of-the-art results on a variety of speech recognition\\ntasks. However, these models are often too large in size for deployment on\\nmobile devices with memory and latency constraints. In this work, we study\\nmechanisms for learning compact RNNs and LSTMs via low-rank factorizations and\\nparameter sharing schemes. Our goal is to investigate redundancies in recurrent\\narchitectures where compression can be admitted without losing performance. A\\nhybrid strategy of using structured matrices in the bottom layers and shared\\nlow-rank factors on the top layers is found to be particularly effective,\\nreducing the parameters of a standard LSTM by 75%, at a small cost of 0.3%\\nincrease in WER, on a 2,000-hr English Voice Search task.',\n",
              " \"Dependency Parsing with LSTMs: An Empirical Evaluation\\nWe propose a transition-based dependency parser using Recurrent Neural\\nNetworks with Long Short-Term Memory (LSTM) units. This extends the feedforward\\nneural network parser of Chen and Manning (2014) and enables modelling of\\nentire sequences of shift/reduce transition decisions. On the Google Web\\nTreebank, our LSTM parser is competitive with the best feedforward parser on\\noverall accuracy and notably achieves more than 3% improvement for long-range\\ndependencies, which has proved difficult for previous transition-based parsers\\ndue to error propagation and limited context information. Our findings\\nadditionally suggest that dropout regularisation on the embedding layer is\\ncrucial to improve the LSTM's generalisation.\",\n",
              " 'Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\\n  and Application to Information Retrieval\\nThis paper develops a model that addresses sentence embedding, a hot topic in\\ncurrent natural language processing research, using recurrent neural networks\\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\\ngoes through the sentence, and when it reaches the last word, the hidden layer\\nof the network provides a semantic representation of the whole sentence. In\\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\\nclick-through data logged by a commercial web search engine. Visualization and\\nanalysis are performed to understand how the embedding process works. The model\\nis found to automatically attenuate the unimportant words and detects the\\nsalient keywords in the sentence. Furthermore, these detected keywords are\\nfound to automatically activate different cells of the LSTM-RNN, where words\\nbelonging to a similar topic activate the same cell. As a semantic\\nrepresentation of the sentence, the embedding vector can be used in many\\ndifferent applications. These automatic keyword detection and topic allocation\\nabilities enabled by the LSTM-RNN allow the network to perform document\\nretrieval, a difficult language processing task, where the similarity between\\nthe query and documents can be measured by the distance between their\\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\\nseveral existing state of the art methods. We emphasize that the proposed model\\ngenerates sentence embedding vectors that are specially useful for web document\\nretrieval tasks. A comparison with a well known general sentence embedding\\nmethod, the Paragraph Vector, is performed. The results show that the proposed\\nmethod in this paper significantly outperforms it for web document retrieval\\ntask.',\n",
              " 'Encoding Source Language with Convolutional Neural Network for Machine\\n  Translation\\nThe recently proposed neural network joint model (NNJM) (Devlin et al., 2014)\\naugments the n-gram target language model with a heuristically chosen source\\ncontext window, achieving state-of-the-art performance in SMT. In this paper,\\nwe give a more systematic treatment by summarizing the relevant source\\ninformation through a convolutional architecture guided by the target\\ninformation. With different guiding signals during decoding, our specifically\\ndesigned convolution+gating architectures can pinpoint the parts of a source\\nsentence that are relevant to predicting a target word, and fuse them with the\\ncontext of entire source sentence to form a unified representation. This\\nrepresentation, together with target language words, are fed to a deep neural\\nnetwork (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English\\ntranslation tasks show that the proposed model can achieve significant\\nimprovements over the previous NNJM by up to +1.08 BLEU points on average',\n",
              " 'Maximum a Posteriori Adaptation of Network Parameters in Deep Models\\nWe present a Bayesian approach to adapting parameters of a well-trained\\ncontext-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to\\nimprove automatic speech recognition performance. Given an abundance of DNN\\nparameters but with only a limited amount of data, the effectiveness of the\\nadapted DNN model can often be compromised. We formulate maximum a posteriori\\n(MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an\\naugmented linear hidden networks connected to the output tied states, or\\nsenones, and compare it to feature space MAP linear regression previously\\nproposed. Experimental evidences on the 20,000-word open vocabulary Wall Street\\nJournal task demonstrate the feasibility of the proposed framework. In\\nsupervised adaptation, the proposed MAP adaptation approach provides more than\\n10% relative error reduction and consistently outperforms the conventional\\ntransformation based methods. Furthermore, we present an initial attempt to\\ngenerate hierarchical priors to improve adaptation efficiency and effectiveness\\nwith limited adaptation data by exploiting similarities among senones.',\n",
              " 'Context-Dependent Translation Selection Using Convolutional Neural\\n  Network\\nWe propose a novel method for translation selection in statistical machine\\ntranslation, in which a convolutional neural network is employed to judge the\\nsimilarity between a phrase pair in two languages. The specifically designed\\nconvolutional architecture encodes not only the semantic similarity of the\\ntranslation pair, but also the context containing the phrase in the source\\nlanguage. Therefore, our approach is able to capture context-dependent semantic\\nsimilarities of translation pairs. We adopt a curriculum learning strategy to\\ntrain the model: we classify the training examples into easy, medium, and\\ndifficult categories, and gradually build the ability of representing phrase\\nand sentence level context by using training examples from easy to difficult.\\nExperimental results show that our approach significantly outperforms the\\nbaseline system by up to 1.4 BLEU points.',\n",
              " 'Convolutional Neural Network Architectures for Matching Natural Language\\n  Sentences\\nSemantic matching is of central importance to many natural language tasks\\n\\\\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to\\nadequately model the internal structures of language objects and the\\ninteraction between them. As a step toward this goal, we propose convolutional\\nneural network models for matching two sentences, by adapting the convolutional\\nstrategy in vision and speech. The proposed models not only nicely represent\\nthe hierarchical structures of sentences with their layer-by-layer composition\\nand pooling, but also capture the rich matching patterns at different levels.\\nOur models are rather generic, requiring no prior knowledge on language, and\\ncan hence be applied to matching tasks of different nature and in different\\nlanguages. The empirical study on a variety of matching tasks demonstrates the\\nefficacy of the proposed model on a variety of matching tasks and its\\nsuperiority to competitor models.',\n",
              " 'Long Short-Term Memory Over Tree Structures\\nThe chain-structured long short-term memory (LSTM) has showed to be effective\\nin a wide range of problems such as speech recognition and machine translation.\\nIn this paper, we propose to extend it to tree structures, in which a memory\\ncell can reflect the history memories of multiple child cells or multiple\\ndescendant cells in a recursive process. We call the model S-LSTM, which\\nprovides a principled way of considering long-distance interaction over\\nhierarchies, e.g., language or image parse structures. We leverage the models\\nfor semantic composition to understand the meaning of text, a fundamental\\nproblem in natural language understanding, and show that it outperforms a\\nstate-of-the-art recursive model by replacing its composition layers with the\\nS-LSTM memory blocks. We also show that utilizing the given structures is\\nhelpful in achieving a performance better than that without considering the\\nstructures.',\n",
              " 'Improving the Performance of Neural Machine Translation Involving\\n  Morphologically Rich Languages\\nThe advent of the attention mechanism in neural machine translation models\\nhas improved the performance of machine translation systems by enabling\\nselective lookup into the source sentence. In this paper, the efficiencies of\\ntranslation using bidirectional encoder attention decoder models were studied\\nwith respect to translation involving morphologically rich languages. The\\nEnglish - Tamil language pair was selected for this analysis. First, the use of\\nWord2Vec embedding for both the English and Tamil words improved the\\ntranslation results by 0.73 BLEU points over the baseline RNNSearch model with\\n4.84 BLEU score. The use of morphological segmentation before word\\nvectorization to split the morphologically rich Tamil words into their\\nrespective morphemes before the translation, caused a reduction in the target\\nvocabulary size by a factor of 8. Also, this model (RNNMorph) improved the\\nperformance of neural machine translation by 7.05 BLEU points over the\\nRNNSearch model used over the same corpus. Since the BLEU evaluation of the\\nRNNMorph model might be unreliable due to an increase in the number of matching\\ntokens per sentence, the performances of the translations were also compared by\\nmeans of human evaluation metrics of adequacy, fluency and relative ranking.\\nFurther, the use of morphological segmentation also improved the efficacy of\\nthe attention mechanism.',\n",
              " 'A recurrent neural network without chaos\\nWe introduce an exceptionally simple gated recurrent neural network (RNN)\\nthat achieves performance comparable to well-known gated architectures, such as\\nLSTMs and GRUs, on the word-level language modeling task. We prove that our\\nmodel has simple, predicable and non-chaotic dynamics. This stands in stark\\ncontrast to more standard gated architectures, whose underlying dynamical\\nsystems exhibit chaotic behavior.',\n",
              " \"End-to-end Phoneme Sequence Recognition using Convolutional Neural\\n  Networks\\nMost phoneme recognition state-of-the-art systems rely on a classical neural\\nnetwork classifiers, fed with highly tuned features, such as MFCC or PLP\\nfeatures. Recent advances in ``deep learning'' approaches questioned such\\nsystems, but while some attempts were made with simpler features such as\\nspectrograms, state-of-the-art systems still rely on MFCCs. This might be\\nviewed as a kind of failure from deep learning approaches, which are often\\nclaimed to have the ability to train with raw signals, alleviating the need of\\nhand-crafted features. In this paper, we investigate a convolutional neural\\nnetwork approach for raw speech signals. While convolutional architectures got\\ntremendous success in computer vision or text processing, they seem to have\\nbeen let down in the past recent years in the speech processing field. We show\\nthat it is possible to learn an end-to-end phoneme sequence classifier system\\ndirectly from raw signal, with similar performance on the TIMIT and WSJ\\ndatasets than existing systems based on MFCC, questioning the need of complex\\nhand-crafted features on large datasets.\",\n",
              " 'A Deep Learning Approach to Data-driven Parameterizations for\\n  Statistical Parametric Speech Synthesis\\nNearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral\\ncoefficients as the vocal tract parameterization of the speech signal. Mel\\nCepstral coefficients were never intended to work in a parametric speech\\nsynthesis framework, but as yet, there has been little success in creating a\\nbetter parameterization that is more suited to synthesis. In this paper, we use\\ndeep learning algorithms to investigate a data-driven parameterization\\ntechnique that is designed for the specific requirements of synthesis. We\\ncreate an invertible, low-dimensional, noise-robust encoding of the Mel Log\\nSpectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is\\nthen unwrapped and used as the initialization for a Multi-Layer Perceptron\\n(MLP). The MLP is fine-tuned by training it to reconstruct the input at the\\noutput layer. This MLP is then split down the middle to form encoding and\\ndecoding networks. These networks produce a parameterization of the Mel Log\\nSpectrum that is intended to better fulfill the requirements of synthesis.\\nResults are reported for experiments conducted using this resulting\\nparameterization with the ClusterGen speech synthesizer.',\n",
              " 'Addressing the Rare Word Problem in Neural Machine Translation\\nNeural Machine Translation (NMT) is a new approach to machine translation\\nthat has shown promising results that are comparable to traditional approaches.\\nA significant weakness in conventional NMT systems is their inability to\\ncorrectly translate very rare words: end-to-end NMTs tend to have relatively\\nsmall vocabularies with a single unk symbol that represents every possible\\nout-of-vocabulary (OOV) word. In this paper, we propose and implement an\\neffective technique to address this problem. We train an NMT system on data\\nthat is augmented by the output of a word alignment algorithm, allowing the NMT\\nsystem to emit, for each OOV word in the target sentence, the position of its\\ncorresponding word in the source sentence. This information is later utilized\\nin a post-processing step that translates every OOV word using a dictionary.\\nOur experiments on the WMT14 English to French translation task show that this\\nmethod provides a substantial improvement of up to 2.8 BLEU points over an\\nequivalent NMT system that does not use this technique. With 37.5 BLEU points,\\nour NMT system is the first to surpass the best result achieved on a WMT14\\ncontest task.',\n",
              " 'Investigating the Role of Prior Disambiguation in Deep-learning\\n  Compositional Models of Meaning\\nThis paper aims to explore the effect of prior disambiguation on neural\\nnetwork- based compositional models, with the hope that better semantic\\nrepresentations for text compounds can be produced. We disambiguate the input\\nword vectors before they are fed into a compositional deep net. A series of\\nevaluations shows the positive effect of prior disambiguation for such deep\\nmodels.',\n",
              " 'Deep Speech: Scaling up end-to-end speech recognition\\nWe present a state-of-the-art speech recognition system developed using\\nend-to-end deep learning. Our architecture is significantly simpler than\\ntraditional speech systems, which rely on laboriously engineered processing\\npipelines; these traditional systems also tend to perform poorly when used in\\nnoisy environments. In contrast, our system does not need hand-designed\\ncomponents to model background noise, reverberation, or speaker variation, but\\ninstead directly learns a function that is robust to such effects. We do not\\nneed a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our\\napproach is a well-optimized RNN training system that uses multiple GPUs, as\\nwell as a set of novel data synthesis techniques that allow us to efficiently\\nobtain a large amount of varied data for training. Our system, called Deep\\nSpeech, outperforms previously published results on the widely studied\\nSwitchboard Hub5\\'00, achieving 16.0% error on the full test set. Deep Speech\\nalso handles challenging noisy environments better than widely used,\\nstate-of-the-art commercial speech systems.',\n",
              " 'Incremental Adaptation Strategies for Neural Network Language Models\\nIt is today acknowledged that neural network language models outperform\\nbackoff language models in applications like speech recognition or statistical\\nmachine translation. However, training these models on large amounts of data\\ncan take several days. We present efficient techniques to adapt a neural\\nnetwork language model to new data. Instead of training a completely new model\\nor relying on mixture approaches, we propose two new methods: continued\\ntraining on resampled data or insertion of adaptation layers. We present\\nexperimental results in an CAT environment where the post-edits of professional\\ntranslators are used to improve an SMT system. Both methods are very fast and\\nachieve significant improvements without overfitting the small adaptation data.',\n",
              " 'Joint RNN-Based Greedy Parsing and Word Composition\\nThis paper introduces a greedy parser based on neural networks, which\\nleverages a new compositional sub-tree representation. The greedy parser and\\nthe compositional procedure are jointly trained, and tightly depends on\\neach-other. The composition procedure outputs a vector representation which\\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\\nComposition and tagging is achieved over continuous (word or tag)\\nrepresentations, and recurrent neural networks. We reach F1 performance on par\\nwith well-known existing parsers, while having the advantage of speed, thanks\\nto the greedy nature of the parser. We provide a fully functional\\nimplementation of the method described in this paper.',\n",
              " 'Efficient Exact Gradient Update for training Deep Networks with Very\\n  Large Sparse Targets\\nAn important class of problems involves training deep neural networks with\\nsparse prediction targets of very high dimension D. These occur naturally in\\ne.g. neural language models or the learning of word-embeddings, often posed as\\npredicting the probability of next words among a vocabulary of size D (e.g. 200\\n000). Computing the equally large, but typically non-sparse D-dimensional\\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\\nincurs a prohibitive O(Dd) computational cost for each example, as does\\nupdating the D x d output weight matrix and computing the gradient needed for\\nbackpropagation to previous layers. While efficient handling of large sparse\\nnetwork inputs is trivial, the case of large sparse targets is not, and has\\nthus so far been sidestepped with approximate alternatives such as hierarchical\\nsoftmax or sampling-based approximations during training. In this work we\\ndevelop an original algorithmic approach which, for a family of loss functions\\nthat includes squared error and spherical softmax, can compute the exact loss,\\ngradient update for the output weights, and gradient for backpropagation, all\\nin O(d^2) per example instead of O(Dd), remarkably without ever computing the\\nD-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e.\\ntwo orders of magnitude for typical sizes, for that critical part of the\\ncomputations that often dominates the training time in this kind of network\\narchitecture.',\n",
              " \"Discriminative Neural Sentence Modeling by Tree-Based Convolution\\nThis paper proposes a tree-based convolutional neural network (TBCNN) for\\ndiscriminative sentence modeling. Our models leverage either constituency trees\\nor dependency trees of sentences. The tree-based convolution process extracts\\nsentences' structural features, and these features are aggregated by max\\npooling. Such architecture allows short propagation paths between the output\\nlayer and underlying feature detectors, which enables effective structural\\nfeature learning and extraction. We evaluate our models on two tasks: sentiment\\nanalysis and question classification. In both experiments, TBCNN outperforms\\nprevious state-of-the-art results, including existing neural networks and\\ndedicated feature/rule engineering. We also make efforts to visualize the\\ntree-based convolution process, shedding light on how our models work.\",\n",
              " 'Self-Adaptive Hierarchical Sentence Model\\nThe ability to accurately model a sentence at varying stages (e.g.,\\nword-phrase-sentence) plays a central role in natural language processing. As\\nan effort towards this goal we propose a self-adaptive hierarchical sentence\\nmodel (AdaSent). AdaSent effectively forms a hierarchy of representations from\\nwords to phrases and then to sentences through recursive gated local\\ncomposition of adjacent segments. We design a competitive mechanism (through\\ngating networks) to allow the representations of the same sentence to be\\nengaged in a particular learning task (e.g., classification), therefore\\neffectively mitigating the gradient vanishing problem persistent in other\\nrecursive models. Both qualitative and quantitative analysis shows that AdaSent\\ncan automatically form and select the representations suitable for the task at\\nhand during training, yielding superior classification performance over\\ncompetitor models on 5 benchmark data sets.',\n",
              " 'Classifying Relations by Ranking with Convolutional Neural Networks\\nRelation classification is an important semantic processing task for which\\nstate-ofthe-art systems still rely on costly handcrafted features. In this work\\nwe tackle the relation classification task using a convolutional neural network\\nthat performs classification by ranking (CR-CNN). We propose a new pairwise\\nranking loss function that makes it easy to reduce the impact of artificial\\nclasses. We perform experiments using the the SemEval-2010 Task 8 dataset,\\nwhich is designed for the task of classifying the relationship between two\\nnominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art\\nfor this dataset and achieve a F1 of 84.1 without using any costly handcrafted\\nfeatures. Additionally, our experimental results show that: (1) our approach is\\nmore effective than CNN followed by a softmax classifier; (2) omitting the\\nrepresentation of the artificial class Other improves both precision and\\nrecall; and (3) using only word embeddings as input features is enough to\\nachieve state-of-the-art results if we consider only the text between the two\\ntarget nominals.',\n",
              " 'Lexical Translation Model Using a Deep Neural Network Architecture\\nIn this paper we combine the advantages of a model using global source\\nsentence contexts, the Discriminative Word Lexicon, and neural networks. By\\nusing deep neural networks instead of the linear maximum entropy model in the\\nDiscriminative Word Lexicon models, we are able to leverage dependencies\\nbetween different source words due to the non-linearity. Furthermore, the\\nmodels for different target words can share parameters and therefore data\\nsparsity problems are effectively reduced.\\n  By using this approach in a state-of-the-art translation system, we can\\nimprove the performance by up to 0.5 BLEU points for three different language\\npairs on the TED translation task.',\n",
              " 'Visualizing and Understanding Recurrent Networks\\nRecurrent Neural Networks (RNNs), and specifically a variant with Long\\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\\nsuccessful applications in a wide range of machine learning problems that\\ninvolve sequential data. However, while LSTMs provide exceptional results in\\npractice, the source of their performance and their limitations remain rather\\npoorly understood. Using character-level language models as an interpretable\\ntestbed, we aim to bridge this gap by providing an analysis of their\\nrepresentations, predictions and error types. In particular, our experiments\\nreveal the existence of interpretable cells that keep track of long-range\\ndependencies such as line lengths, quotes and brackets. Moreover, our\\ncomparative analysis with finite horizon n-gram models traces the source of the\\nLSTM improvements to long-range structural dependencies. Finally, we provide\\nanalysis of the remaining errors and suggests areas for further study.',\n",
              " 'A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for\\n  Unsupervised Discovery of Linguistic Units and Generation of High Quality\\n  Features\\nThis paper summarizes the work done by the authors for the Zero Resource\\nSpeech Challenge organized in the technical program of Interspeech 2015. The\\ngoal of the challenge is to discover linguistic units directly from unlabeled\\nspeech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work\\nautomatically discovers multiple sets of acoustic tokens from the given corpus.\\nEach acoustic token set is specified by a set of hyperparameters that describe\\nthe model configuration. These sets of acoustic tokens carry different\\ncharacteristics of the given corpus and the language behind thus can be\\nmutually reinforced. The multiple sets of token labels are then used as the\\ntargets of a Multi-target DNN (MDNN) trained on low-level acoustic features.\\nBottleneck features extracted from the MDNN are used as feedback for the MAT\\nand the MDNN itself. We call this iterative system the Multi-layered Acoustic\\nTokenizing Deep Neural Network (MAT-DNN) which generates high quality features\\nfor track 1 of the challenge and acoustic tokens for track 2 of the challenge.',\n",
              " \"Author Identification using Multi-headed Recurrent Neural Networks\\nRecurrent neural networks (RNNs) are very good at modelling the flow of text,\\nbut typically need to be trained on a far larger corpus than is available for\\nthe PAN 2015 Author Identification task. This paper describes a novel approach\\nwhere the output layer of a character-level RNN language model is split into\\nseveral independent predictive sub-models, each representing an author, while\\nthe recurrent layer is shared by all. This allows the recurrent layer to model\\nthe language as a whole without over-fitting, while the outputs select aspects\\nof the underlying model that reflect their author's style. The method proves\\ncompetitive, ranking first in two of the four languages.\",\n",
              " 'A Deep Memory-based Architecture for Sequence-to-Sequence Learning\\nWe propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence\\nlearning, which performs the task through a series of nonlinear transformations\\nfrom the representation of the input sequence (e.g., a Chinese sentence) to the\\nfinal output sequence (e.g., translation to English). Inspired by the recently\\nproposed Neural Turing Machine (Graves et al., 2014), we store the intermediate\\nrepresentations in stacked layers of memories, and use read-write operations on\\nthe memories to realize the nonlinear transformations between the\\nrepresentations. The types of transformations are designed in advance but the\\nparameters are learned from data. Through layer-by-layer transformations,\\nDEEPMEMORY can model complicated relations between sequences necessary for\\napplications such as machine translation between distant languages. The\\narchitecture can be trained with normal back-propagation on sequenceto-sequence\\ndata, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is\\nbroad enough to subsume the state-of-the-art neural translation model in\\n(Bahdanau et al., 2015) as its special case, while significantly improving upon\\nthe model with its deeper architecture. Remarkably, DEEPMEMORY, being purely\\nneural network-based, can achieve performance comparable to the traditional\\nphrase-based machine translation system Moses with a small vocabulary and a\\nmodest parameter size.',\n",
              " \"Ask Me Anything: Dynamic Memory Networks for Natural Language Processing\\nMost tasks in natural language processing can be cast into question answering\\n(QA) problems over language input. We introduce the dynamic memory network\\n(DMN), a neural network architecture which processes input sequences and\\nquestions, forms episodic memories, and generates relevant answers. Questions\\ntrigger an iterative attention process which allows the model to condition its\\nattention on the inputs and the result of previous iterations. These results\\nare then reasoned over in a hierarchical recurrent sequence model to generate\\nanswers. The DMN can be trained end-to-end and obtains state-of-the-art results\\non several types of tasks and datasets: question answering (Facebook's bAbI\\ndataset), text classification for sentiment analysis (Stanford Sentiment\\nTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The\\ntraining for these different tasks relies exclusively on trained word vector\\nrepresentations and input-question-answer triplets.\",\n",
              " 'Improved Deep Speaker Feature Learning for Text-Dependent Speaker\\n  Recognition\\nA deep learning approach has been proposed recently to derive speaker\\nidentifies (d-vector) by a deep neural network (DNN). This approach has been\\napplied to text-dependent speaker recognition tasks and shows reasonable\\nperformance gains when combined with the conventional i-vector approach.\\nAlthough promising, the existing d-vector implementation still can not compete\\nwith the i-vector baseline. This paper presents two improvements for the deep\\nlearning approach: a phonedependent DNN structure to normalize phone variation,\\nand a new scoring approach based on dynamic time warping (DTW). Experiments on\\na text-dependent speaker recognition task demonstrated that the proposed\\nmethods can provide considerable performance improvement over the existing\\nd-vector implementation.',\n",
              " 'Grid Long Short-Term Memory\\nThis paper introduces Grid Long Short-Term Memory, a network of LSTM cells\\narranged in a multidimensional grid that can be applied to vectors, sequences\\nor higher dimensional data such as images. The network differs from existing\\ndeep LSTM architectures in that the cells are connected between network layers\\nas well as along the spatiotemporal dimensions of the data. The network\\nprovides a unified way of using LSTM for both deep and sequential computation.\\nWe apply the model to algorithmic tasks such as 15-digit integer addition and\\nsequence memorization, where it is able to significantly outperform the\\nstandard LSTM. We then give results for two empirical tasks. We find that 2D\\nGrid LSTM achieves 1.47 bits per character on the Wikipedia character\\nprediction benchmark, which is state-of-the-art among neural approaches. In\\naddition, we use the Grid LSTM to define a novel two-dimensional translation\\nmodel, the Reencoder, and show that it outperforms a phrase-based reference\\nsystem on a Chinese-to-English translation task.',\n",
              " 'A Dependency-Based Neural Network for Relation Classification\\nPrevious research on relation classification has verified the effectiveness\\nof using dependency shortest paths or subtrees. In this paper, we further\\nexplore how to make full use of the combination of these dependency\\ninformation. We first propose a new structure, termed augmented dependency path\\n(ADP), which is composed of the shortest dependency path between two entities\\nand the subtrees attached to the shortest path. To exploit the semantic\\nrepresentation behind the ADP structure, we develop dependency-based neural\\nnetworks (DepNN): a recursive neural network designed to model the subtrees,\\nand a convolutional neural network to capture the most important features on\\nthe shortest path. Experiments on the SemEval-2010 dataset show that our\\nproposed method achieves state-of-art results.',\n",
              " 'PTE: Predictive Text Embedding through Large-scale Heterogeneous Text\\n  Networks\\nUnsupervised text embedding methods, such as Skip-gram and Paragraph Vector,\\nhave been attracting increasing attention due to their simplicity, scalability,\\nand effectiveness. However, comparing to sophisticated deep learning\\narchitectures such as convolutional neural networks, these methods usually\\nyield inferior results when applied to particular machine learning tasks. One\\npossible reason is that these text embedding methods learn the representation\\nof text in a fully unsupervised way, without leveraging the labeled information\\navailable for the task. Although the low dimensional representations learned\\nare applicable to many different tasks, they are not particularly tuned for any\\ntask. In this paper, we fill this gap by proposing a semi-supervised\\nrepresentation learning method for text data, which we call the\\n\\\\textit{predictive text embedding} (PTE). Predictive text embedding utilizes\\nboth labeled and unlabeled data to learn the embedding of text. The labeled\\ninformation and different levels of word co-occurrence information are first\\nrepresented as a large-scale heterogeneous text network, which is then embedded\\ninto a low dimensional space through a principled and efficient algorithm. This\\nlow dimensional embedding not only preserves the semantic closeness of words\\nand documents, but also has a strong predictive power for the particular task.\\nCompared to recent supervised approaches based on convolutional neural\\nnetworks, predictive text embedding is comparable or more effective, much more\\nefficient, and has fewer parameters to tune.',\n",
              " 'Relation Classification via Recurrent Neural Network\\nDeep learning has gained much success in sentence-level relation\\nclassification. For example, convolutional neural networks (CNN) have delivered\\ncompetitive performance without much effort on feature engineering as the\\nconventional pattern-based methods. Thus a lot of works have been produced\\nbased on CNN structures. However, a key issue that has not been well addressed\\nby the CNN-based method is the lack of capability to learn temporal features,\\nespecially long-distance dependency between nominal pairs. In this paper, we\\npropose a simple framework based on recurrent neural networks (RNN) and compare\\nit with CNN-based model. To show the limitation of popular used SemEval-2010\\nTask 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al.,\\n2014). Experiments on two different datasets strongly indicates that the\\nRNN-based model can deliver better performance on relation classification, and\\nit is particularly capable of learning long-distance relation patterns. This\\nmakes it suitable for real-world applications where complicated expressions are\\noften involved.',\n",
              " 'Learning from LDA using Deep Neural Networks\\nLatent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian\\nmodel for topic inference. In spite of its great success, inferring the latent\\ntopic distribution with LDA is time-consuming. Motivated by the transfer\\nlearning approach proposed by~\\\\newcite{hinton2015distilling}, we present a\\nnovel method that uses LDA to supervise the training of a deep neural network\\n(DNN), so that the DNN can approximate the costly LDA inference with less\\ncomputation. Our experiments on a document classification task show that a\\nsimple DNN can learn the LDA behavior pretty well, while the inference is\\nspeeded up tens or hundreds of times.',\n",
              " 'Online Representation Learning in Recurrent Neural Language Models\\nWe investigate an extension of continuous online learning in recurrent neural\\nnetwork language models. The model keeps a separate vector representation of\\nthe current unit of text being processed and adaptively adjusts it after each\\nprediction. The initial experiments give promising results, indicating that the\\nmethod is able to increase language modelling accuracy, while also decreasing\\nthe parameters needed to store the model along with the computation required at\\neach step.',\n",
              " \"A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional\\n  Neural Networks for Sentence Classification\\nConvolutional Neural Networks (CNNs) have recently achieved remarkably strong\\nperformance on the practically important task of sentence classification (kim\\n2014, kalchbrenner 2014, johnson 2014). However, these models require\\npractitioners to specify an exact model architecture and set accompanying\\nhyperparameters, including the filter region size, regularization parameters,\\nand so on. It is currently unknown how sensitive model performance is to\\nchanges in these configurations for the task of sentence classification. We\\nthus conduct a sensitivity analysis of one-layer CNNs to explore the effect of\\narchitecture components on model performance; our aim is to distinguish between\\nimportant and comparatively inconsequential design decisions for sentence\\nclassification. We focus on one-layer CNNs (to the exclusion of more complex\\nmodels) due to their comparative simplicity and strong empirical performance,\\nwhich makes it a modern standard baseline method akin to Support Vector Machine\\n(SVMs) and logistic regression. We derive practical advice from our extensive\\nempirical results for those interested in getting the most out of CNNs for\\nsentence classification in real world settings.\",\n",
              " 'Prediction-Adaptation-Correction Recurrent Neural Networks for\\n  Low-Resource Language Speech Recognition\\nIn this paper, we investigate the use of prediction-adaptation-correction\\nrecurrent neural networks (PAC-RNNs) for low-resource speech recognition. A\\nPAC-RNN is comprised of a pair of neural networks in which a {\\\\it correction}\\nnetwork uses auxiliary information given by a {\\\\it prediction} network to help\\nestimate the state probability. The information from the correction network is\\nalso used by the prediction network in a recurrent loop. Our model outperforms\\nother state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.\\nMoreover, transfer learning from a language that is similar to the target\\nlanguage can help improve performance further.',\n",
              " \"Generating Text with Deep Reinforcement Learning\\nWe introduce a novel schema for sequence to sequence learning with a Deep\\nQ-Network (DQN), which decodes the output sequence iteratively. The aim here is\\nto enable the decoder to first tackle easier portions of the sequences, and\\nthen turn to cope with difficult parts. Specifically, in each iteration, an\\nencoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the\\ninput sequence, automatically create features to represent the internal states\\nof and formulate a list of potential actions for the DQN. Take rephrasing a\\nnatural sentence as an example. This list can contain ranked potential words.\\nNext, the DQN learns to make decision on which action (e.g., word) will be\\nselected from the list to modify the current decoded sequence. The newly\\nmodified output sequence is subsequently used as the input to the DQN for the\\nnext decoding iteration. In each iteration, we also bias the reinforcement\\nlearning's attention to explore sequence portions which are previously\\ndifficult to be decoded. For evaluation, the proposed strategy was trained to\\ndecode ten thousands natural sentences. Our experiments indicate that, when\\ncompared to a left-to-right greedy beam search LSTM decoder, the proposed\\nmethod performed competitively well when decoding sentences from the training\\nset, but significantly outperformed the baseline when decoding unseen\\nsentences, in terms of BLEU score obtained.\",\n",
              " 'Detecting Interrogative Utterances with Recurrent Neural Networks\\nIn this paper, we explore different neural network architectures that can\\npredict if a speaker of a given utterance is asking a question or making a\\nstatement. We com- pare the outcomes of regularization methods that are\\npopularly used to train deep neural networks and study how different context\\nfunctions can affect the classification performance. We also compare the\\nefficacy of gated activation functions that are favorably used in recurrent\\nneural networks and study how to combine multimodal inputs. We evaluate our\\nmodels on two multimodal datasets: MSR-Skype and CALLHOME.',\n",
              " 'A Neural Transducer\\nSequence-to-sequence models have achieved impressive results on various\\ntasks. However, they are unsuitable for tasks that require incremental\\npredictions to be made as more data arrives or tasks that have long input\\nsequences and output sequences. This is because they generate an output\\nsequence conditioned on an entire input sequence. In this paper, we present a\\nNeural Transducer that can make incremental predictions as more input arrives,\\nwithout redoing the entire computation. Unlike sequence-to-sequence models, the\\nNeural Transducer computes the next-step distribution conditioned on the\\npartially observed input sequence and the partially generated sequence. At each\\ntime step, the transducer can decide to emit zero to many output symbols. The\\ndata can be processed using an encoder and presented as input to the\\ntransducer. The discrete decision to emit a symbol at every time step makes it\\ndifficult to learn with conventional backpropagation. It is however possible to\\ntrain the transducer by using a dynamic programming algorithm to generate\\ntarget discrete decisions. Our experiments show that the Neural Transducer\\nworks well in settings where it is required to produce output predictions as\\ndata come in. We also find that the Neural Transducer performs well for long\\nsequences even when attention mechanisms are not used.',\n",
              " \"Skip-Thought Memory Networks\\nQuestion Answering (QA) is fundamental to natural language processing in that\\nmost nlp problems can be phrased as QA (Kumar et al., 2015). Current weakly\\nsupervised memory network models that have been proposed so far struggle at\\nanswering questions that involve relations among multiple entities (such as\\nfacebook's bAbi qa5-three-arg-relations in (Weston et al., 2015)). To address\\nthis problem of learning multi-argument multi-hop semantic relations for the\\npurpose of QA, we propose a method that combines the jointly learned long-term\\nread-write memory and attentive inference components of end-to-end memory\\nnetworks (MemN2N) (Sukhbaatar et al., 2015) with distributed sentence vector\\nrepresentations encoded by a Skip-Thought model (Kiros et al., 2015). This\\nchoice to append Skip-Thought Vectors to the existing MemN2N framework is\\nmotivated by the fact that Skip-Thought Vectors have been shown to accurately\\nmodel multi-argument semantic relations (Kiros et al., 2015).\",\n",
              " 'Named Entity Recognition with Bidirectional LSTM-CNNs\\nNamed entity recognition is a challenging task that has traditionally\\nrequired large amounts of knowledge in the form of feature engineering and\\nlexicons to achieve high performance. In this paper, we present a novel neural\\nnetwork architecture that automatically detects word- and character-level\\nfeatures using a hybrid bidirectional LSTM and CNN architecture, eliminating\\nthe need for most feature engineering. We also propose a novel method of\\nencoding partial lexicon matches in neural networks and compare it to existing\\napproaches. Extensive evaluation shows that, given only tokenized text and\\npublicly available word embeddings, our system is competitive on the CoNLL-2003\\ndataset and surpasses the previously reported state of the art performance on\\nthe OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed\\nfrom publicly-available sources, we establish new state of the art performance\\nwith an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing\\nsystems that employ heavy feature engineering, proprietary lexicons, and rich\\nentity linking information.',\n",
              " 'Generating News Headlines with Recurrent Neural Networks\\nWe describe an application of an encoder-decoder recurrent neural network\\nwith LSTM units and attention to generating headlines from the text of news\\narticles. We find that the model is quite effective at concisely paraphrasing\\nnews articles. Furthermore, we study how the neural network decides which input\\nwords to pay attention to, and specifically we identify the function of the\\ndifferent neurons in a simplified attention mechanism. Interestingly, our\\nsimplified attention mechanism performs better that the more complex attention\\nmechanism on a held out set of articles.',\n",
              " 'Words are not Equal: Graded Weighting Model for building Composite\\n  Document Vectors\\nDespite the success of distributional semantics, composing phrases from word\\nvectors remains an important challenge. Several methods have been tried for\\nbenchmark tasks such as sentiment classification, including word vector\\naveraging, matrix-vector approaches based on parsing, and on-the-fly learning\\nof paragraph vectors. Most models usually omit stop words from the composition.\\nInstead of such an yes-no decision, we consider several graded schemes where\\nwords are weighted according to their discriminatory relevance with respect to\\nits use in the document (e.g., idf). Some of these methods (particularly\\ntf-idf) are seen to result in a significant improvement in performance over\\nprior state of the art. Further, combining such approaches into an ensemble\\nbased on alternate classifiers such as the RNN model, results in an 1.6%\\nperformance improvement on the standard IMDB movie review dataset, and a 7.01%\\nimprovement on Amazon product reviews. Since these are language free models and\\ncan be obtained in an unsupervised manner, they are of interest also for\\nunder-resourced languages such as Hindi as well and many more languages. We\\ndemonstrate the language free aspects by showing a gain of 12% for two review\\ndatasets over earlier results, and also release a new larger dataset for future\\ntesting (Singh,2015).',\n",
              " 'Small-footprint Deep Neural Networks with Highway Connections for Speech\\n  Recognition\\nFor speech recognition, deep neural networks (DNNs) have significantly\\nimproved the recognition accuracy in most of benchmark datasets and application\\ndomains. However, compared to the conventional Gaussian mixture models,\\nDNN-based acoustic models usually have much larger number of model parameters,\\nmaking it challenging for their applications in resource constrained platforms,\\ne.g., mobile devices. In this paper, we study the application of the recently\\nproposed highway network to train small-footprint DNNs, which are {\\\\it thinner}\\nand {\\\\it deeper}, and have significantly smaller number of model parameters\\ncompared to conventional DNNs. We investigated this approach on the AMI meeting\\nspeech transcription corpus which has around 70 hours of audio data. The\\nhighway neural networks constantly outperformed their plain DNN counterparts,\\nand the number of model parameters can be reduced significantly without\\nsacrificing the recognition accuracy.',\n",
              " \"Backward and Forward Language Modeling for Constrained Sentence\\n  Generation\\nRecent language models, especially those based on recurrent neural networks\\n(RNNs), make it possible to generate natural language from a learned\\nprobability. Language generation has wide applications including machine\\ntranslation, summarization, question answering, conversation systems, etc.\\nExisting methods typically learn a joint probability of words conditioned on\\nadditional information, which is (either statically or dynamically) fed to\\nRNN's hidden layer. In many applications, we are likely to impose hard\\nconstraints on the generated texts, i.e., a particular word must appear in the\\nsentence. Unfortunately, existing approaches could not solve this problem. In\\nthis paper, we propose a novel backward and forward language model. Provided a\\nspecific word, we use RNNs to generate previous words and future words, either\\nsimultaneously or asynchronously, resulting in two model variants. In this way,\\nthe given word could appear at any position in the sentence. Experimental\\nresults show that the generated texts are comparable to sequential LMs in\\nquality.\",\n",
              " 'Online Keyword Spotting with a Character-Level Recurrent Neural Network\\nIn this paper, we propose a context-aware keyword spotting model employing a\\ncharacter-level recurrent neural network (RNN) for spoken term detection in\\ncontinuous speech. The RNN is end-to-end trained with connectionist temporal\\nclassification (CTC) to generate the probabilities of character and\\nword-boundary labels. There is no need for the phonetic transcription, senone\\nmodeling, or system dictionary in training and testing. Also, keywords can\\neasily be added and modified by editing the text based keyword list without\\nretraining the RNN. Moreover, the unidirectional RNN processes an infinitely\\nlong input audio streams without pre-segmentation and keywords are detected\\nwith low-latency before the utterance is finished. Experimental results show\\nthat the proposed keyword spotter significantly outperforms the deep neural\\nnetwork (DNN) and hidden Markov model (HMM) based keyword-filler model even\\nwith less computations.',\n",
              " 'Domain Specific Author Attribution Based on Feedforward Neural Network\\n  Language Models\\nAuthorship attribution refers to the task of automatically determining the\\nauthor based on a given sample of text. It is a problem with a long history and\\nhas a wide range of application. Building author profiles using language models\\nis one of the most successful methods to automate this task. New language\\nmodeling methods based on neural networks alleviate the curse of dimensionality\\nand usually outperform conventional N-gram methods. However, there have not\\nbeen much research applying them to authorship attribution. In this paper, we\\npresent a novel setup of a Neural Network Language Model (NNLM) and apply it to\\na database of text samples from different authors. We investigate how the NNLM\\nperforms on a task with moderate author set size and relatively limited\\ntraining and test data, and how the topics of the text samples affect the\\naccuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of\\nfitness of a trained language model to the test data. Given 5 random test\\nsentences, it also increases the author classification accuracy by 3.43% on\\naverage, compared with the N-gram methods using SRILM tools. An open source\\nimplementation of our methodology is freely available at\\nhttps://github.com/zge/authorship-attribution/.',\n",
              " 'Segmental Recurrent Neural Networks for End-to-end Speech Recognition\\nWe study the segmental recurrent neural network for end-to-end acoustic\\nmodelling. This model connects the segmental conditional random field (CRF)\\nwith a recurrent neural network (RNN) used for feature extraction. Compared to\\nmost previous CRF-based acoustic models, it does not rely on an external system\\nto provide features or segmentation boundaries. Instead, this model\\nmarginalises out all the possible segmentations, and features are extracted\\nfrom the RNN trained together with the segmental CRF. In essence, this model is\\nself-contained and can be trained end-to-end. In this paper, we discuss\\npractical training and decoding issues as well as the method to speed up the\\ntraining in the context of speech recognition. We performed experiments on the\\nTIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass\\ndecoding --- the best reported result using CRFs, despite the fact that we only\\nused a zeroth-order CRF and without using any language model.',\n",
              " 'How Transferable are Neural Networks in NLP Applications?\\nTransfer learning is aimed to make use of valuable knowledge in a source\\ndomain to help model performance in a target domain. It is particularly\\nimportant to neural networks, which are very likely to be overfitting. In some\\nfields like image processing, many studies have shown the effectiveness of\\nneural network-based transfer learning. For neural NLP, however, existing\\nstudies have only casually applied transfer learning, and conclusions are\\ninconsistent. In this paper, we conduct systematic case studies and provide an\\nilluminating picture on the transferability of neural networks in NLP.',\n",
              " 'Recurrent Neural Network Encoder with Attention for Community Question\\n  Answering\\nWe apply a general recurrent neural network (RNN) encoder framework to\\ncommunity question answering (cQA) tasks. Our approach does not rely on any\\nlinguistic processing, and can be applied to different languages or domains.\\nFurther improvements are observed when we extend the RNN encoders with a neural\\nattention mechanism that encourages reasoning over entire sequences. To deal\\nwith practical issues such as data sparsity and imbalanced labels, we apply\\nvarious techniques such as transfer learning and multitask learning. Our\\nexperiments on the SemEval-2016 cQA task show 10% improvement on a MAP score\\ncompared to an information retrieval-based approach, and achieve comparable\\nperformance to a strong handcrafted feature-based method.',\n",
              " 'Recursive Neural Language Architecture for Tag Prediction\\nWe consider the problem of learning distributed representations for tags from\\ntheir associated content for the task of tag recommendation. Considering\\ntagging information is usually very sparse, effective learning from content and\\ntag association is very crucial and challenging task. Recently, various neural\\nrepresentation learning models such as WSABIE and its variants show promising\\nperformance, mainly due to compact feature representations learned in a\\nsemantic space. However, their capacity is limited by a linear compositional\\napproach for representing tags as sum of equal parts and hurt their\\nperformance. In this work, we propose a neural feedback relevance model for\\nlearning tag representations with weighted feature representations. Our\\nexperiments on two widely used datasets show significant improvement for\\nquality of recommendations over various baselines.',\n",
              " 'On the Compression of Recurrent Neural Networks with an Application to\\n  LVCSR acoustic modeling for Embedded Speech Recognition\\nWe study the problem of compressing recurrent neural networks (RNNs). In\\nparticular, we focus on the compression of RNN acoustic models, which are\\nmotivated by the goal of building compact and accurate speech recognition\\nsystems which can be run efficiently on mobile devices. In this work, we\\npresent a technique for general recurrent model compression that jointly\\ncompresses both recurrent and non-recurrent inter-layer weight matrices. We\\nfind that the proposed technique allows us to reduce the size of our Long\\nShort-Term Memory (LSTM) acoustic model to a third of its original size with\\nnegligible loss in accuracy.',\n",
              " 'Pointing the Unknown Words\\nThe problem of rare and unknown words is an important issue that can\\npotentially influence the performance of many NLP systems, including both the\\ntraditional count-based and the deep learning models. We propose a novel way to\\ndeal with the rare and unseen words for the neural network models using\\nattention. Our model uses two softmax layers in order to predict the next word\\nin conditional language models: one predicts the location of a word in the\\nsource sentence, and the other predicts a word in the shortlist vocabulary. At\\neach time-step, the decision of which softmax layer to use choose adaptively\\nmade by an MLP which is conditioned on the context.~We motivate our work from a\\npsychological evidence that humans naturally have a tendency to point towards\\nobjects in the context or the environment when the name of an object is not\\nknown.~We observe improvements on two tasks, neural machine translation on the\\nEuroparl English to French parallel corpora and text summarization on the\\nGigaword dataset using our proposed model.',\n",
              " 'Learning Multiscale Features Directly From Waveforms\\nDeep learning has dramatically improved the performance of speech recognition\\nsystems through learning hierarchies of features optimized for the task at\\nhand. However, true end-to-end learning, where features are learned directly\\nfrom waveforms, has only recently reached the performance of hand-tailored\\nrepresentations based on the Fourier transform. In this paper, we detail an\\napproach to use convolutional filters to push past the inherent tradeoff of\\ntemporal and frequency resolution that exists for spectral representations. At\\nincreased computational cost, we show that increasing temporal resolution via\\nreduced stride and increasing frequency resolution via additional filters\\ndelivers significant performance improvements. Further, we find more efficient\\nrepresentations by simultaneously learning at multiple scales, leading to an\\noverall decrease in word error rate on a difficult internal speech test set by\\n20.7% relative to networks with the same number of parameters trained on\\nspectrograms.',\n",
              " 'Joint Learning of Sentence Embeddings for Relevance and Entailment\\nWe consider the problem of Recognizing Textual Entailment within an\\nInformation Retrieval context, where we must simultaneously determine the\\nrelevancy as well as degree of entailment for individual pieces of evidence to\\ndetermine a yes/no answer to a binary natural language question.\\n  We compare several variants of neural networks for sentence embeddings in a\\nsetting of decision-making based on evidence of varying relevance. We propose a\\nbasic model to integrate evidence for entailment, show that joint training of\\nthe sentence embeddings to model relevance and entailment is feasible even with\\nno explicit per-evidence supervision, and show the importance of evaluating\\nstrong baselines. We also demonstrate the benefit of carrying over text\\ncomprehension model trained on an unrelated task for our small datasets.\\n  Our research is motivated primarily by a new open dataset we introduce,\\nconsisting of binary questions and news-based evidence snippets. We also apply\\nthe proposed relevance-entailment model on a similar task of ranking\\nmultiple-choice test answers, evaluating it on a preliminary dataset of school\\ntest questions as well as the standard MCTest dataset, where we improve the\\nneural model state-of-art.',\n",
              " 'Deep API Learning\\nDevelopers often wonder how to implement a certain functionality (e.g., how\\nto parse XML files) using APIs. Obtaining an API usage sequence based on an\\nAPI-related natural language query is very helpful in this regard. Given a\\nquery, existing approaches utilize information retrieval models to search for\\nmatching API sequences. These approaches treat queries and APIs as bag-of-words\\n(i.e., keyword matching or word-to-word alignment) and lack a deep\\nunderstanding of the semantics of the query.\\n  We propose DeepAPI, a deep learning based approach to generate API usage\\nsequences for a given natural language query. Instead of a bags-of-words\\nassumption, it learns the sequence of words in a query and the sequence of\\nassociated APIs. DeepAPI adapts a neural language model named RNN\\nEncoder-Decoder. It encodes a word sequence (user query) into a fixed-length\\ncontext vector, and generates an API sequence based on the context vector. We\\nalso augment the RNN Encoder-Decoder by considering the importance of\\nindividual APIs. We empirically evaluate our approach with more than 7 million\\nannotated code snippets collected from GitHub. The results show that our\\napproach generates largely accurate API sequences and outperforms the related\\napproaches.',\n",
              " 'Does Multimodality Help Human and Machine for Translation and Image\\n  Captioning?\\nThis paper presents the systems developed by LIUM and CVC for the WMT16\\nMultimodal Machine Translation challenge. We explored various comparative\\nmethods, namely phrase-based systems and attentional recurrent neural networks\\nmodels trained using monomodal or multimodal data. We also performed a human\\nevaluation in order to estimate the usefulness of multimodal data for human\\nmachine translation and image description generation. Our systems obtained the\\nbest results for both tasks according to the automatic evaluation metrics BLEU\\nand METEOR.',\n",
              " 'Very Deep Convolutional Networks for Text Classification\\nThe dominant approach for many NLP tasks are recurrent neural networks, in\\nparticular LSTMs, and convolutional neural networks. However, these\\narchitectures are rather shallow in comparison to the deep convolutional\\nnetworks which have pushed the state-of-the-art in computer vision. We present\\na new architecture (VDCNN) for text processing which operates directly at the\\ncharacter level and uses only small convolutions and pooling operations. We are\\nable to show that the performance of this model increases with depth: using up\\nto 29 convolutional layers, we report improvements over the state-of-the-art on\\nseveral public text classification tasks. To the best of our knowledge, this is\\nthe first time that very deep convolutional nets have been applied to text\\nprocessing.',\n",
              " 'Improving Recurrent Neural Networks For Sequence Labelling\\nIn this paper we study different types of Recurrent Neural Networks (RNN) for\\nsequence labeling tasks. We propose two new variants of RNNs integrating\\nimprovements for sequence labeling, and we compare them to the more traditional\\nElman and Jordan RNNs. We compare all models, either traditional or new, on\\nfour distinct tasks of sequence labeling: two on Spoken Language Understanding\\n(ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the\\nPenn Treebank (PTB) corpora. The results show that our new variants of RNNs are\\nalways more effective than the others.',\n",
              " 'Sentence Similarity Measures for Fine-Grained Estimation of Topical\\n  Relevance in Learner Essays\\nWe investigate the task of assessing sentence-level prompt relevance in\\nlearner essays. Various systems using word overlap, neural embeddings and\\nneural compositional models are evaluated on two datasets of learner writing.\\nWe propose a new method for sentence-level similarity calculation, which learns\\nto adjust the weights of pre-trained word embeddings for a specific task,\\nachieving substantially higher accuracy compared to other relevant baselines.',\n",
              " \"Deep CNNs along the Time Axis with Intermap Pooling for Robustness to\\n  Spectral Variations\\nConvolutional neural networks (CNNs) with convolutional and pooling\\noperations along the frequency axis have been proposed to attain invariance to\\nfrequency shifts of features. However, this is inappropriate with regard to the\\nfact that acoustic features vary in frequency. In this paper, we contend that\\nconvolution along the time axis is more effective. We also propose the addition\\nof an intermap pooling (IMP) layer to deep CNNs. In this layer, filters in each\\ngroup extract common but spectrally variant features, then the layer pools the\\nfeature maps of each group. As a result, the proposed IMP CNN can achieve\\ninsensitivity to spectral variations characteristic of different speakers and\\nutterances. The effectiveness of the IMP CNN architecture is demonstrated on\\nseveral LVCSR tasks. Even without speaker adaptation techniques, the\\narchitecture achieved a WER of 12.7% on the SWB part of the Hub5'2000\\nevaluation test set, which is competitive with other state-of-the-art methods.\",\n",
              " \"Automatic Text Scoring Using Neural Networks\\nAutomated Text Scoring (ATS) provides a cost-effective and consistent\\nalternative to human marking. However, in order to achieve good performance,\\nthe predictive features of the system need to be manually engineered by human\\nexperts. We introduce a model that forms word representations by learning the\\nextent to which specific words contribute to the text's score. Using Long-Short\\nTerm Memory networks to represent the meaning of texts, we demonstrate that a\\nfully automated framework is able to achieve excellent results over similar\\napproaches. In an attempt to make our results more interpretable, and inspired\\nby recent advances in visualizing neural networks, we introduce a novel method\\nfor identifying the regions of the text that the model has found more\\ndiscriminative.\",\n",
              " 'A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic\\n  Modeling in Speech Recognition\\nWe present a comprehensive study of deep bidirectional long short-term memory\\n(LSTM) recurrent neural network (RNN) based acoustic models for automatic\\nspeech recognition (ASR). We study the effect of size and depth and train\\nmodels of up to 8 layers. We investigate the training aspect and study\\ndifferent variants of optimization methods, batching, truncated\\nbackpropagation, different regularization techniques such as dropout and $L_2$\\nregularization, and different gradient clipping variants.\\n  The major part of the experimental analysis was performed on the Quaero\\ncorpus. Additional experiments also were performed on the Switchboard corpus.\\nOur best LSTM model has a relative improvement in word error rate of over 14\\\\%\\ncompared to our best feed-forward neural network (FFNN) baseline on the Quaero\\ntask. On this task, we get our best result with an 8 layer bidirectional LSTM\\nand we show that a pretraining scheme with layer-wise construction helps for\\ndeep LSTMs.\\n  Finally we compare the training calculation time of many of the presented\\nexperiments in relation with recognition performance.\\n  All the experiments were done with RETURNN, the RWTH extensible training\\nframework for universal recurrent neural networks in combination with RASR, the\\nRWTH ASR toolkit.',\n",
              " 'Sequence-Level Knowledge Distillation\\nNeural machine translation (NMT) offers a novel alternative formulation of\\ntranslation that is potentially simpler than statistical approaches. However to\\nreach competitive performance, NMT models need to be exceedingly large. In this\\npaper we consider applying knowledge distillation approaches (Bucila et al.,\\n2006; Hinton et al., 2015) that have proven successful for reducing the size of\\nneural models in other domains to the problem of NMT. We demonstrate that\\nstandard knowledge distillation applied to word-level prediction can be\\neffective for NMT, and also introduce two novel sequence-level versions of\\nknowledge distillation that further improve performance, and somewhat\\nsurprisingly, seem to eliminate the need for beam search (even when applied on\\nthe original teacher model). Our best student model runs 10 times faster than\\nits state-of-the-art teacher with little loss in performance. It is also\\nsignificantly better than a baseline model trained without knowledge\\ndistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight\\npruning on top of knowledge distillation results in a student model that has 13\\ntimes fewer parameters than the original teacher model, with a decrease of 0.4\\nBLEU.',\n",
              " 'Learning Semantically Coherent and Reusable Kernels in Convolution\\n  Neural Nets for Sentence Classification\\nThe state-of-the-art CNN models give good performance on sentence\\nclassification tasks. The purpose of this work is to empirically study\\ndesirable properties such as semantic coherence, attention mechanism and\\nreusability of CNNs in these tasks. Semantically coherent kernels are\\npreferable as they are a lot more interpretable for explaining the decision of\\nthe learned CNN model. We observe that the learned kernels do not have semantic\\ncoherence. Motivated by this observation, we propose to learn kernels with\\nsemantic coherence using clustering scheme combined with Word2Vec\\nrepresentation and domain knowledge such as SentiWordNet. We suggest a\\ntechnique to visualize attention mechanism of CNNs for decision explanation\\npurpose. Reusable property enables kernels learned on one problem to be used in\\nanother problem. This helps in efficient learning as only a few additional\\ndomain specific filters may have to be learned. We demonstrate the efficacy of\\nour core ideas of learning semantically coherent kernels and leveraging\\nreusable kernels for efficient learning on several benchmark datasets.\\nExperimental results show the usefulness of our approach by achieving\\nperformance close to the state-of-the-art methods but with semantic and\\nreusable properties.',\n",
              " 'RETURNN: The RWTH Extensible Training framework for Universal Recurrent\\n  Neural Networks\\nIn this work we release our extensible and easily configurable neural network\\ntraining software. It provides a rich set of functional layers with a\\nparticular focus on efficient training of recurrent neural network topologies\\non multiple GPUs. The source of the software package is public and freely\\navailable for academic research purposes and can be used as a framework or as a\\nstandalone tool which supports a flexible configuration. The software allows to\\ntrain state-of-the-art deep bidirectional long short-term memory (LSTM) models\\non both one dimensional data like speech or two dimensional data like\\nhandwritten text and was used to develop successful submission systems in\\nseveral evaluation campaigns.',\n",
              " 'Character-Level Language Modeling with Hierarchical Recurrent Neural\\n  Networks\\nRecurrent neural network (RNN) based character-level language models (CLMs)\\nare extremely useful for modeling out-of-vocabulary words by nature. However,\\ntheir performance is generally much worse than the word-level language models\\n(WLMs), since CLMs need to consider longer history of tokens to properly\\npredict the next one. We address this problem by proposing hierarchical RNN\\narchitectures, which consist of multiple modules with different timescales.\\nDespite the multi-timescale structures, the input and output layers operate\\nwith the character-level clock, which allows the existing RNN CLM training\\napproaches to be directly applicable without any modifications. Our CLM models\\nshow better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word\\nBenchmark with only 2% of parameters. Also, we present real-time\\ncharacter-level end-to-end speech recognition examples on the Wall Street\\nJournal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the\\nproposed models results in better recognition accuracies even though the number\\nof parameters are reduced to 30%.',\n",
              " 'Multi-task Recurrent Model for True Multilingual Speech Recognition\\nResearch on multilingual speech recognition remains attractive yet\\nchallenging. Recent studies focus on learning shared structures under the\\nmulti-task paradigm, in particular a feature sharing structure. This approach\\nhas been found effective to improve performance on each individual language.\\nHowever, this approach is only useful when the deployed system supports just\\none language. In a true multilingual scenario where multiple languages are\\nallowed, performance will be significantly reduced due to the competition among\\nlanguages in the decoding space. This paper presents a multi-task recurrent\\nmodel that involves a multilingual speech recognition (ASR) component and a\\nlanguage recognition (LR) component, and the ASR component is informed of the\\nlanguage information by the LR component, leading to a language-aware\\nrecognition. We tested the approach on an English-Chinese bilingual recognition\\ntask. The results show that the proposed multi-task recurrent model can improve\\nperformance of multilingual recognition systems.',\n",
              " 'Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\\n  Recurrent models\\nSentiment Analysis (SA) is an action research area in the digital age. With\\nrapid and constant growth of online social media sites and services, and the\\nincreasing amount of textual data such as - statuses, comments, reviews etc.\\navailable in them, application of automatic SA is on the rise. However, most of\\nthe research works on SA in natural language processing (NLP) are based on\\nEnglish language. Despite being the sixth most widely spoken language in the\\nworld, Bangla still does not have a large and standard dataset. Because of\\nthis, recent research works in Bangla have failed to produce results that can\\nbe both comparable to works done by others and reusable as stepping stones for\\nfuture researchers to progress in this field. Therefore, we first tried to\\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\\ntexts as well, is substantial, post-processed and multiple validated, ready to\\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\\n- binary crossentropy and categorical crossentropy, and also did some\\nexperimental pre-training by using data from one validation to pre-train the\\nother and vice versa. Lastly, we documented the results along with some\\nanalysis on them, which were promising.',\n",
              " 'Attending to Characters in Neural Sequence Labeling Models\\nSequence labeling architectures use word embeddings for capturing similarity,\\nbut suffer when handling previously unseen or rare words. We investigate\\ncharacter-level extensions to such models and propose a novel architecture for\\ncombining alternative word representations. By using an attention mechanism,\\nthe model is able to dynamically decide how much information to use from a\\nword- or character-level component. We evaluated different architectures on a\\nrange of sequence labeling datasets, and character-level extensions were found\\nto improve performance on every benchmark. In addition, the proposed\\nattention-based architecture delivered the best results even with a smaller\\nnumber of trainable parameters.',\n",
              " \"Visualizing and Understanding Curriculum Learning for Long Short-Term\\n  Memory Networks\\nCurriculum Learning emphasizes the order of training instances in a\\ncomputational learning setup. The core hypothesis is that simpler instances\\nshould be learned early as building blocks to learn more complex ones. Despite\\nits usefulness, it is still unknown how exactly the internal representation of\\nmodels are affected by curriculum learning. In this paper, we study the effect\\nof curriculum learning on Long Short-Term Memory (LSTM) networks, which have\\nshown strong competency in many Natural Language Processing (NLP) problems. Our\\nexperiments on sentiment analysis task and a synthetic task similar to sequence\\nprediction tasks in NLP show that curriculum learning has a positive effect on\\nthe LSTM's internal states by biasing the model towards building constructive\\nrepresentations i.e. the internal representation at the previous timesteps are\\nused as building blocks for the final prediction. We also find that smaller\\nmodels significantly improves when they are trained with curriculum learning.\\nLastly, we show that curriculum learning helps more when the amount of training\\ndata is limited.\",\n",
              " 'Dense Prediction on Sequences with Time-Dilated Convolutions for Speech\\n  Recognition\\nIn computer vision pixelwise dense prediction is the task of predicting a\\nlabel for each pixel in the image. Convolutional neural networks achieve good\\nperformance on this task, while being computationally efficient. In this paper\\nwe carry these ideas over to the problem of assigning a sequence of labels to a\\nset of speech frames, a task commonly known as framewise classification. We\\nshow that dense prediction view of framewise classification offers several\\nadvantages and insights, including computational efficiency and the ability to\\napply batch normalization. When doing dense prediction we pay specific\\nattention to strided pooling in time and introduce an asymmetric dilated\\nconvolution, called time-dilated convolution, that allows for efficient and\\nelegant implementation of pooling in time. We show results using time-dilated\\nconvolutions in a very deep VGG-style CNN with batch normalization on the Hub5\\nSwitchboard-2000 benchmark task. With a big n-gram language model, we achieve\\n7.7% WER which is the best single model single-pass performance reported so\\nfar.',\n",
              " 'End-to-End ASR-free Keyword Search from Speech\\nEnd-to-end (E2E) systems have achieved competitive results compared to\\nconventional hybrid hidden Markov model (HMM)-deep neural network based\\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\\nto the lack of dependence on alignments between input acoustic and output\\ngrapheme or HMM state sequence during training. This paper explores the design\\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\\nspeech trained with minimal supervision. Our E2E KWS system consists of three\\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\\nacoustic auto-encoder trained to reconstruct the audio through a\\nfinite-dimensional representation. The second sub-system is a character-level\\nRNN language model using embeddings learned from a convolutional neural\\nnetwork. Since the acoustic and text query embeddings occupy different\\nrepresentation spaces, they are input to a third feed-forward neural network\\nthat predicts whether the query occurs in the acoustic utterance or not. This\\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\\nsystem and trains much faster.',\n",
              " 'Training Language Models Using Target-Propagation\\nWhile Truncated Back-Propagation through Time (BPTT) is the most popular\\napproach to training Recurrent Neural Networks (RNNs), it suffers from being\\ninherently sequential (making parallelization difficult) and from truncating\\ngradient flow between distant time-steps. We investigate whether Target\\nPropagation (TPROP) style approaches can address these shortcomings.\\nUnfortunately, extensive experiments suggest that TPROP generally underperforms\\nBPTT, and we end with an analysis of this phenomenon, and suggestions for\\nfuture work.',\n",
              " 'Deep Voice: Real-time Neural Text-to-Speech\\nWe present Deep Voice, a production-quality text-to-speech system constructed\\nentirely from deep neural networks. Deep Voice lays the groundwork for truly\\nend-to-end neural speech synthesis. The system comprises five major building\\nblocks: a segmentation model for locating phoneme boundaries, a\\ngrapheme-to-phoneme conversion model, a phoneme duration prediction model, a\\nfundamental frequency prediction model, and an audio synthesis model. For the\\nsegmentation model, we propose a novel way of performing phoneme boundary\\ndetection with deep neural networks using connectionist temporal classification\\n(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet\\nthat requires fewer parameters and trains faster than the original. By using a\\nneural network for each component, our system is simpler and more flexible than\\ntraditional text-to-speech systems, where each component requires laborious\\nfeature engineering and extensive domain expertise. Finally, we show that\\ninference with our system can be performed faster than real time and describe\\noptimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x\\nspeedups over existing implementations.',\n",
              " \"Improved Variational Autoencoders for Text Modeling using Dilated\\n  Convolutions\\nRecent work on generative modeling of text has found that variational\\nauto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM\\nlanguage models (Bowman et al., 2015). This negative result is so far poorly\\nunderstood, but has been attributed to the propensity of LSTM decoders to\\nignore conditioning information from the encoder. In this paper, we experiment\\nwith a new type of decoder for VAE: a dilated CNN. By changing the decoder's\\ndilation architecture, we control the effective context from previously\\ngenerated words. In experiments, we find that there is a trade off between the\\ncontextual capacity of the decoder and the amount of encoding information used.\\nWe show that with the right decoder, VAE can outperform LSTM language models.\\nWe demonstrate perplexity gains on two datasets, representing the first\\npositive experimental result on the use VAE for generative modeling of text.\\nFurther, we conduct an in-depth investigation of the use of VAE (with our new\\ndecoding architecture) for semi-supervised and unsupervised labeling tasks,\\ndemonstrating gains over several strong baselines.\",\n",
              " 'Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence\\n  Labelling\\nMost existing sequence labelling models rely on a fixed decomposition of a\\ntarget sequence into a sequence of basic units. These methods suffer from two\\nmajor drawbacks: 1) the set of basic units is fixed, such as the set of words,\\ncharacters or phonemes in speech recognition, and 2) the decomposition of\\ntarget sequences is fixed. These drawbacks usually result in sub-optimal\\nperformance of modeling sequences. In this pa- per, we extend the popular CTC\\nloss criterion to alleviate these limitations, and propose a new loss function\\ncalled Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically\\nlearns the best set of basic units (grams), as well as the most suitable\\ndecomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to\\noutput variable number of characters at each time step, which enables the model\\nto capture longer term dependency and improves the computational efficiency. We\\ndemonstrate that the proposed Gram-CTC improves CTC in terms of both\\nperformance and efficiency on the large vocabulary speech recognition task at\\nmultiple scales of data, and that with Gram-CTC we can outperform the\\nstate-of-the-art on a standard speech benchmark.',\n",
              " \"Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model)\\nWe examine Memory Networks for the task of question answering (QA), under\\ncommon real world scenario where training examples are scarce and under weakly\\nsupervised scenario, that is only extrinsic labels are available for training.\\nWe propose extensions for the Dynamic Memory Network (DMN), specifically within\\nthe attention mechanism, we call the resulting Neural Architecture as Dynamic\\nMemory Tensor Network (DMTN). Ultimately, we see that our proposed extensions\\nresults in over 80% improvement in the number of task passed against the\\nbaselined standard DMN and 20% more task passed compared to state-of-the-art\\nEnd-to-End Memory Network for Facebook's single task weakly trained 1K bAbi\\ndataset.\",\n",
              " 'Simplified End-to-End MMI Training and Voting for ASR\\nA simplified speech recognition system that uses the maximum mutual\\ninformation (MMI) criterion is considered. End-to-end training using gradient\\ndescent is suggested, similarly to the training of connectionist temporal\\nclassification (CTC). We use an MMI criterion with a simple language model in\\nthe training stage, and a standard HMM decoder. Our method compares favorably\\nto CTC in terms of performance, robustness, decoding time, disk footprint and\\nquality of alignments. The good alignments enable the use of a straightforward\\nensemble method, obtained by simply averaging the predictions of several neural\\nnetwork models, that were trained separately end-to-end. The ensemble method\\nyields a considerable reduction in the word error rate.',\n",
              " 'Learning to Generate Reviews and Discovering Sentiment\\nWe explore the properties of byte-level recurrent language models. When given\\nsufficient amounts of capacity, training data, and compute time, the\\nrepresentations learned by these models include disentangled features\\ncorresponding to high-level concepts. Specifically, we find a single unit which\\nperforms sentiment analysis. These representations, learned in an unsupervised\\nmanner, achieve state of the art on the binary subset of the Stanford Sentiment\\nTreebank. They are also very data efficient. When using only a handful of\\nlabeled examples, our approach matches the performance of strong baselines\\ntrained on full datasets. We also demonstrate the sentiment unit has a direct\\ninfluence on the generative process of the model. Simply fixing its value to be\\npositive or negative generates samples with the corresponding positive or\\nnegative sentiment.',\n",
              " 'Semi-supervised Multitask Learning for Sequence Labeling\\nWe propose a sequence labeling framework with a secondary training objective,\\nlearning to predict surrounding words for every word in the dataset. This\\nlanguage modeling objective incentivises the system to learn general-purpose\\npatterns of semantic and syntactic composition, which are also useful for\\nimproving accuracy on different sequence labeling tasks. The architecture was\\nevaluated on a range of datasets, covering the tasks of error detection in\\nlearner texts, named entity recognition, chunking and POS-tagging. The novel\\nlanguage modeling objective provided consistent performance improvements on\\nevery benchmark, without requiring any additional annotated or unannotated\\ndata.',\n",
              " \"Going Wider: Recurrent Neural Network With Parallel Cells\\nRecurrent Neural Network (RNN) has been widely applied for sequence modeling.\\nIn RNN, the hidden states at current step are full connected to those at\\nprevious step, thus the influence from less related features at previous step\\nmay potentially decrease model's learning ability. We propose a simple\\ntechnique called parallel cells (PCs) to enhance the learning ability of\\nRecurrent Neural Network (RNN). In each layer, we run multiple small RNN cells\\nrather than one single large cell. In this paper, we evaluate PCs on 2 tasks.\\nOn language modeling task on PTB (Penn Tree Bank), our model outperforms state\\nof art models by decreasing perplexity from 78.6 to 75.3. On Chinese-English\\ntranslation task, our model increases BLEU score for 0.39 points than baseline\\nmodel.\",\n",
              " 'Phonetic Temporal Neural Model for Language Identification\\nDeep neural models, particularly the LSTM-RNN model, have shown great\\npotential for language identification (LID). However, the use of phonetic\\ninformation has been largely overlooked by most existing neural LID methods,\\nalthough this information has been used very successfully in conventional\\nphonetic LID systems. We present a phonetic temporal neural model for LID,\\nwhich is an LSTM-RNN LID system that accepts phonetic features produced by a\\nphone-discriminative DNN as the input, rather than raw acoustic features. This\\nnew model is similar to traditional phonetic LID methods, but the phonetic\\nknowledge here is much richer: it is at the frame level and involves compacted\\ninformation of all phones. Our experiments conducted on the Babel database and\\nthe AP16-OLR database demonstrate that the temporal phonetic neural approach is\\nvery effective, and significantly outperforms existing acoustic neural models.\\nIt also outperforms the conventional i-vector approach on short utterances and\\nin noisy conditions.',\n",
              " 'Relevance-based Word Embedding\\nLearning a high-dimensional dense representation for vocabulary terms, also\\nknown as a word embedding, has recently attracted much attention in natural\\nlanguage processing and information retrieval tasks. The embedding vectors are\\ntypically learned based on term proximity in a large corpus. This means that\\nthe objective in well-known word embedding algorithms, e.g., word2vec, is to\\naccurately predict adjacent word(s) for a given word or context. However, this\\nobjective is not necessarily equivalent to the goal of many information\\nretrieval (IR) tasks. The primary objective in various IR tasks is to capture\\nrelevance instead of term proximity, syntactic, or even semantic similarity.\\nThis is the motivation for developing unsupervised relevance-based word\\nembedding models that learn word representations based on query-document\\nrelevance information. In this paper, we propose two learning models with\\ndifferent objective functions; one learns a relevance distribution over the\\nvocabulary set for each query, and the other classifies each term as belonging\\nto the relevant or non-relevant class for each query. To train our models, we\\nused over six million unique queries and the top ranked documents retrieved in\\nresponse to each query, which are assumed to be relevant to the query. We\\nextrinsically evaluate our learned word representation models using two IR\\ntasks: query expansion and query classification. Both query expansion\\nexperiments on four TREC collections and query classification experiments on\\nthe KDD Cup 2005 dataset suggest that the relevance-based word embedding models\\nsignificantly outperform state-of-the-art proximity-based embedding models,\\nsuch as word2vec and GloVe.',\n",
              " 'Deriving Neural Architectures from Sequence and Graph Kernels\\nThe design of neural architectures for structured objects is typically guided\\nby experimental insights rather than a formal process. In this work, we appeal\\nto kernels over combinatorial structures, such as sequences and graphs, to\\nderive appropriate neural operations. We introduce a class of deep recurrent\\nneural operations and formally characterize their associated kernel spaces. Our\\nrecurrent modules compare the input to virtual reference objects (cf. filters\\nin CNN) via the kernels. Similar to traditional neural operations, these\\nreference objects are parameterized and directly optimized in end-to-end\\ntraining. We empirically evaluate the proposed class of neural architectures on\\nstandard applications such as language modeling and molecular graph regression,\\nachieving state-of-the-art results across these applications.',\n",
              " 'On Multilingual Training of Neural Dependency Parsers\\nWe show that a recently proposed neural dependency parser can be improved by\\njoint training on multiple languages from the same family. The parser is\\nimplemented as a deep neural network whose only input is orthographic\\nrepresentations of words. In order to successfully parse, the network has to\\ndiscover how linguistically relevant concepts can be inferred from word\\nspellings. We analyze the representations of characters and words that are\\nlearned by the network to establish which properties of languages were\\naccounted for. In particular we show that the parser has approximately learned\\nto associate Latin characters with their Cyrillic counterparts and that it can\\ngroup Polish and Russian words that have a similar grammatical function.\\nFinally, we evaluate the parser on selected languages from the Universal\\nDependencies dataset and show that it is competitive with other recently\\nproposed state-of-the art methods, while having a simple structure.',\n",
              " 'Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks\\nLadder networks are a notable new concept in the field of semi-supervised\\nlearning by showing state-of-the-art results in image recognition tasks while\\nbeing compatible with many existing neural architectures. We present the\\nrecurrent ladder network, a novel modification of the ladder network, for\\nsemi-supervised learning of recurrent neural networks which we evaluate with a\\nphoneme recognition task on the TIMIT corpus. Our results show that the model\\nis able to consistently outperform the baseline and achieve fully-supervised\\nbaseline performance with only 75% of all labels which demonstrates that the\\nmodel is capable of using unsupervised data as an effective regulariser.',\n",
              " 'Adversarially Regularized Autoencoders\\nWhile autoencoders are a key technique in representation learning for\\ncontinuous structures, such as images or wave forms, developing general-purpose\\nautoencoders for discrete structures, such as text sequence or discretized\\nimages, has proven to be more challenging. In particular, discrete inputs make\\nit more difficult to learn a smooth encoder that preserves the complex local\\nrelationships in the input space. In this work, we propose an adversarially\\nregularized autoencoder (ARAE) with the goal of learning more robust\\ndiscrete-space representations. ARAE jointly trains both a rich discrete-space\\nencoder, such as an RNN, and a simpler continuous space generator function,\\nwhile using generative adversarial network (GAN) training to constrain the\\ndistributions to be similar. This method yields a smoother contracted code\\nspace that maps similar inputs to nearby codes, and also an implicit latent\\nvariable GAN model for generation. Experiments on text and discretized images\\ndemonstrate that the GAN model produces clean interpolations and captures the\\nmultimodality of the original space, and that the autoencoder produces improve-\\nments in semi-supervised learning as well as state-of-the-art results in\\nunaligned text style transfer task using only a shared continuous-space\\nrepresentation.',\n",
              " 'Auxiliary Objectives for Neural Error Detection Models\\nWe investigate the utility of different auxiliary objectives and training\\nstrategies within a neural sequence labeling approach to error detection in\\nlearner writing. Auxiliary costs provide the model with additional linguistic\\ninformation, allowing it to learn general-purpose compositional features that\\ncan then be exploited for other objectives. Our experiments show that a joint\\nlearning approach trained with parallel labels on in-domain data improves\\nperformance over the previous best error detection system. While the resulting\\nmodel has the same number of parameters, the additional objectives allow it to\\nbe optimised more efficiently and achieve better performance.',\n",
              " \"An Error-Oriented Approach to Word Embedding Pre-Training\\nWe propose a novel word embedding pre-training approach that exploits writing\\nerrors in learners' scripts. We compare our method to previous models that tune\\nthe embeddings based on script scores and the discrimination between correct\\nand corrupt word contexts in addition to the generic commonly-used embeddings\\npre-trained on large corpora. The comparison is achieved by using the\\naforementioned models to bootstrap a neural network that learns to predict a\\nholistic score for scripts. Furthermore, we investigate augmenting our model\\nwith error corrections and monitor the impact on performance. Our results show\\nthat our error-oriented approach outperforms other comparable ones which is\\nfurther demonstrated when training on more data. Additionally, extending the\\nmodel with corrections provides further performance gains when data sparsity is\\nan issue.\",\n",
              " 'A Continuous Relaxation of Beam Search for End-to-end Training of Neural\\n  Sequence Models\\nBeam search is a desirable choice of test-time decoding algorithm for neural\\nsequence models because it potentially avoids search errors made by simpler\\ngreedy methods. However, typical cross entropy training procedures for these\\nmodels do not directly consider the behaviour of the final decoding method. As\\na result, for cross-entropy trained models, beam decoding can sometimes yield\\nreduced test performance when compared with greedy decoding. In order to train\\nmodels that can more effectively make use of beam search, we propose a new\\ntraining procedure that focuses on the final loss metric (e.g. Hamming loss)\\nevaluated on the output of beam search. While well-defined, this \"direct loss\"\\nobjective is itself discontinuous and thus difficult to optimize. Hence, in our\\napproach, we form a sub-differentiable surrogate objective by introducing a\\nnovel continuous approximation of the beam search decoding procedure. In\\nexperiments, we show that optimizing this new training objective yields\\nsubstantially better results on two sequence tasks (Named Entity Recognition\\nand CCG Supertagging) when compared with both cross entropy trained greedy\\ndecoding and cross entropy trained beam decoding baselines.',\n",
              " 'Regularizing and Optimizing LSTM Language Models\\nRecurrent neural networks (RNNs), such as long short-term memory networks\\n(LSTMs), serve as a fundamental building block for many sequence learning\\ntasks, including machine translation, language modeling, and question\\nanswering. In this paper, we consider the specific problem of word-level\\nlanguage modeling and investigate strategies for regularizing and optimizing\\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\\nwherein the averaging trigger is determined using a non-monotonic condition as\\nopposed to being tuned by the user. Using these and other regularization\\nstrategies, we achieve state-of-the-art word level perplexities on two data\\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\\neffectiveness of a neural cache in conjunction with our proposed model, we\\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\\n52.0 on WikiText-2.',\n",
              " 'Supervised Speech Separation Based on Deep Learning: An Overview\\nSpeech separation is the task of separating target speech from background\\ninterference. Traditionally, speech separation is studied as a signal\\nprocessing problem. A more recent approach formulates speech separation as a\\nsupervised learning problem, where the discriminative patterns of speech,\\nspeakers, and background noise are learned from training data. Over the past\\ndecade, many supervised separation algorithms have been put forward. In\\nparticular, the recent introduction of deep learning to supervised speech\\nseparation has dramatically accelerated progress and boosted separation\\nperformance. This article provides a comprehensive overview of the research on\\ndeep learning based supervised speech separation in the last several years. We\\nfirst introduce the background of speech separation and the formulation of\\nsupervised separation. Then we discuss three main components of supervised\\nseparation: learning machines, training targets, and acoustic features. Much of\\nthe overview is on separation algorithms where we review monaural methods,\\nincluding speech enhancement (speech-nonspeech separation), speaker separation\\n(multi-talker separation), and speech dereverberation, as well as\\nmulti-microphone techniques. The important issue of generalization, unique to\\nsupervised learning, is discussed. This overview provides a historical\\nperspective on how advances are made. In addition, we discuss a number of\\nconceptual issues, including what constitutes the target source.',\n",
              " 'Grasping the Finer Point: A Supervised Similarity Network for Metaphor\\n  Detection\\nThe ubiquity of metaphor in our everyday communication makes it an important\\nproblem for natural language understanding. Yet, the majority of metaphor\\nprocessing systems to date rely on hand-engineered features and there is still\\nno consensus in the field as to which features are optimal for this task. In\\nthis paper, we present the first deep learning architecture designed to capture\\nmetaphorical composition. Our results demonstrate that it outperforms the\\nexisting approaches in the metaphor identification task.',\n",
              " 'Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words\\nDistributed word embeddings have shown superior performances in numerous\\nNatural Language Processing (NLP) tasks. However, their performances vary\\nsignificantly across different tasks, implying that the word embeddings learnt\\nby those methods capture complementary aspects of lexical semantics. Therefore,\\nwe believe that it is important to combine the existing word embeddings to\\nproduce more accurate and complete \\\\emph{meta-embeddings} of words. For this\\npurpose, we propose an unsupervised locally linear meta-embedding learning\\nmethod that takes pre-trained word embeddings as the input, and produces more\\naccurate meta embeddings. Unlike previously proposed meta-embedding learning\\nmethods that learn a global projection over all words in a vocabulary, our\\nproposed method is sensitive to the differences in local neighbourhoods of the\\nindividual source word embeddings. Moreover, we show that vector concatenation,\\na previously proposed highly competitive baseline approach for integrating word\\nembeddings, can be derived as a special case of the proposed method.\\nExperimental results on semantic similarity, word analogy, relation\\nclassification, and short-text classification tasks show that our\\nmeta-embeddings to significantly outperform prior methods in several benchmark\\ndatasets, establishing a new state of the art for meta-embeddings.',\n",
              " 'KeyVec: Key-semantics Preserving Document Representations\\nPrevious studies have demonstrated the empirical success of word embeddings\\nin various applications. In this paper, we investigate the problem of learning\\ndistributed representations for text documents which many machine learning\\nalgorithms take as input for a number of NLP tasks.\\n  We propose a neural network model, KeyVec, which learns document\\nrepresentations with the goal of preserving key semantics of the input text. It\\nenables the learned low-dimensional vectors to retain the topics and important\\ninformation from the documents that will flow to downstream tasks. Our\\nempirical evaluations show the superior quality of KeyVec representations in\\ntwo different document understanding tasks.',\n",
              " 'Exploring Asymmetric Encoder-Decoder Structure for Context-based\\n  Sentence Representation Learning\\nContext information plays an important role in human language understanding,\\nand it is also useful for machines to learn vector representations of language.\\nIn this paper, we explore an asymmetric encoder-decoder structure for\\nunsupervised context-based sentence representation learning. As a result, we\\nbuild an encoder-decoder architecture with an RNN encoder and a CNN decoder. We\\nfurther combine a suite of effective designs to significantly improve model\\nefficiency while also achieving better performance. Our model is trained on two\\ndifferent large unlabeled corpora, and in both cases transferability is\\nevaluated on a set of downstream language understanding tasks. We empirically\\nshow that our model is simple and fast while producing rich sentence\\nrepresentations that excel in downstream tasks.',\n",
              " 'CNN Is All You Need\\nThe Convolution Neural Network (CNN) has demonstrated the unique advantage in\\naudio, image and text learning; recently it has also challenged Recurrent\\nNeural Networks (RNNs) with long short-term memory cells (LSTM) in\\nsequence-to-sequence learning, since the computations involved in CNN are\\neasily parallelizable whereas those involved in RNN are mostly sequential,\\nleading to a performance bottleneck. However, unlike RNN, the native CNN lacks\\nthe history sensitivity required for sequence transformation; therefore\\nenhancing the sequential order awareness, or position-sensitivity, becomes the\\nkey to make CNN the general deep learning model. In this work we introduce an\\nextended CNN model with strengthen position-sensitivity, called PoseNet. A\\nnotable feature of PoseNet is the asymmetric treatment of position information\\nin the encoder and the decoder. Experiments shows that PoseNet allows us to\\nimprove the accuracy of CNN based sequence-to-sequence learning significantly,\\nachieving around 33-36 BLEU scores on the WMT 2014 English-to-German\\ntranslation task, and around 44-46 BLEU scores on the English-to-French\\ntranslation task.',\n",
              " 'Combining Representation Learning with Logic for Language Processing\\nThe current state-of-the-art in many natural language processing and\\nautomated knowledge base completion tasks is held by representation learning\\nmethods which learn distributed vector representations of symbols via\\ngradient-based optimization. They require little or no hand-crafted features,\\nthus avoiding the need for most preprocessing steps and task-specific\\nassumptions. However, in many cases representation learning requires a large\\namount of annotated training data to generalize well to unseen data. Such\\nlabeled training data is provided by human annotators who often use formal\\nlogic as the language for specifying annotations. This thesis investigates\\ndifferent combinations of representation learning methods with logic for\\nreducing the need for annotated training data, and for improving\\ngeneralization.',\n",
              " 'A Note on Topology Preservation in Classification, and the Construction\\n  of a Universal Neuron Grid\\nIt will be shown that according to theorems of K. Menger, every neuron grid\\nif identified with a curve is able to preserve the adopted qualitative\\nstructure of a data space. Furthermore, if this identification is made, the\\nneuron grid structure can always be mapped to a subset of a universal neuron\\ngrid which is constructable in three space dimensions. Conclusions will be\\ndrawn for established neuron grid types as well as neural fields.',\n",
              " 'Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On\\n  Boltzmann Machines\\nOne conjecture in both deep learning and classical connectionist viewpoint is\\nthat the biological brain implements certain kinds of deep networks as its\\nback-end. However, to our knowledge, a detailed correspondence has not yet been\\nset up, which is important if we want to bridge between neuroscience and\\nmachine learning. Recent researches emphasized the biological plausibility of\\nLinear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally\\nplausible settings, the whole network is capable of representing any Boltzmann\\nmachine and performing a semi-stochastic Bayesian inference algorithm lying\\nbetween Gibbs sampling and variational inference.',\n",
              " 'Mapping Temporal Variables into the NeuCube for Improved Pattern\\n  Recognition, Predictive Modelling and Understanding of Stream Data\\nThis paper proposes a new method for an optimized mapping of temporal\\nvariables, describing a temporal stream data, into the recently proposed\\nNeuCube spiking neural network architecture. This optimized mapping extends the\\nuse of the NeuCube, which was initially designed for spatiotemporal brain data,\\nto work on arbitrary stream data and to achieve a better accuracy of temporal\\npattern recognition, a better and earlier event prediction and a better\\nunderstanding of complex temporal stream data through visualization of the\\nNeuCube connectivity. The effect of the new mapping is demonstrated on three\\nbench mark problems. The first one is early prediction of patient sleep stage\\nevent from temporal physiological data. The second one is pattern recognition\\nof dynamic temporal patterns of traffic in the Bay Area of California and the\\nlast one is the Challenge 2012 contest data set. In all cases the use of the\\nproposed mapping leads to an improved accuracy of pattern recognition and event\\nprediction and a better understanding of the data when compared to traditional\\nmachine learning techniques or spiking neural network reservoirs with arbitrary\\nmapping of the variables.',\n",
              " 'An Evolutionary Algorithm to Learn SPARQL Queries for\\n  Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia\\nEfficient usage of the knowledge provided by the Linked Data community is\\noften hindered by the need for domain experts to formulate the right SPARQL\\nqueries to answer questions. For new questions they have to decide which\\ndatasets are suitable and in which terminology and modelling style to phrase\\nthe SPARQL query.\\n  In this work we present an evolutionary algorithm to help with this\\nchallenging task. Given a training list of source-target node-pair examples our\\nalgorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The\\nlearned patterns can be visualised to form the basis for further investigation,\\nor they can be used to predict target nodes for new source nodes.\\n  Amongst others, we apply our algorithm to a dataset of several hundred human\\nassociations (such as \"circle - square\") to find patterns for them in DBpedia.\\nWe show the scalability of the algorithm by running it against a SPARQL\\nendpoint loaded with > 7.9 billion triples. Further, we use the resulting\\nSPARQL queries to mimic human associations with a Mean Average Precision (MAP)\\nof 39.9 % and a Recall@10 of 63.9 %.',\n",
              " 'A Geometric Framework for Convolutional Neural Networks\\nIn this paper, a geometric framework for neural networks is proposed. This\\nframework uses the inner product space structure underlying the parameter set\\nto perform gradient descent not in a component-based form, but in a\\ncoordinate-free manner. Convolutional neural networks are described in this\\nframework in a compact form, with the gradients of standard --- and\\nhigher-order --- loss functions calculated for each layer of the network. This\\napproach can be applied to other network structures and provides a basis on\\nwhich to create new networks.',\n",
              " 'A Novel Representation of Neural Networks\\nDeep Neural Networks (DNNs) have become very popular for prediction in many\\nareas. Their strength is in representation with a high number of parameters\\nthat are commonly learned via gradient descent or similar optimization methods.\\nHowever, the representation is non-standardized, and the gradient calculation\\nmethods are often performed using component-based approaches that break\\nparameters down into scalar units, instead of considering the parameters as\\nwhole entities. In this work, these problems are addressed. Standard notation\\nis used to represent DNNs in a compact framework. Gradients of DNN loss\\nfunctions are calculated directly over the inner product space on which the\\nparameters are defined. This framework is general and is applied to two common\\nnetwork types: the Multilayer Perceptron and the Deep Autoencoder.',\n",
              " 'Converting Cascade-Correlation Neural Nets into Probabilistic Generative\\n  Models\\nHumans are not only adept in recognizing what class an input instance belongs\\nto (i.e., classification task), but perhaps more remarkably, they can imagine\\n(i.e., generate) plausible instances of a desired class with ease, when\\nprompted. Inspired by this, we propose a framework which allows transforming\\nCascade-Correlation Neural Networks (CCNNs) into probabilistic generative\\nmodels, thereby enabling CCNNs to generate samples from a category of interest.\\nCCNNs are a well-known class of deterministic, discriminative NNs, which\\nautonomously construct their topology, and have been successful in giving\\naccounts for a variety of psychological phenomena. Our proposed framework is\\nbased on a Markov Chain Monte Carlo (MCMC) method, called the\\nMetropolis-adjusted Langevin algorithm, which capitalizes on the gradient\\ninformation of the target distribution to direct its explorations towards\\nregions of high probability, thereby achieving good mixing properties. Through\\nextensive simulations, we demonstrate the efficacy of our proposed framework.',\n",
              " \"On the Performance of Network Parallel Training in Artificial Neural\\n  Networks\\nArtificial Neural Networks (ANNs) have received increasing attention in\\nrecent years with applications that span a wide range of disciplines including\\nvital domains such as medicine, network security and autonomous transportation.\\nHowever, neural network architectures are becoming increasingly complex and\\nwith an increasing need to obtain real-time results from such models, it has\\nbecome pivotal to use parallelization as a mechanism for speeding up network\\ntraining and deployment. In this work we propose an implementation of Network\\nParallel Training through Cannon's Algorithm for matrix multiplication. We show\\nthat increasing the number of processes speeds up training until the point\\nwhere process communication costs become prohibitive; this point varies by\\nnetwork complexity. We also show through empirical efficiency calculations that\\nthe speedup obtained is superlinear.\",\n",
              " 'Programmable Agents\\nWe build deep RL agents that execute declarative programs expressed in formal\\nlanguage. The agents learn to ground the terms in this language in their\\nenvironment, and can generalize their behavior at test time to execute new\\nprograms that refer to objects that were not referenced during training. The\\nagents develop disentangled interpretable representations that allow them to\\ngeneralize to a wide variety of zero-shot semantic tasks.',\n",
              " 'Explainable Artificial Intelligence: Understanding, Visualizing and\\n  Interpreting Deep Learning Models\\nWith the availability of large databases and recent improvements in deep\\nlearning methodology, the performance of AI systems is reaching or even\\nexceeding the human level on an increasing number of complex tasks. Impressive\\nexamples of this development can be found in domains such as image\\nclassification, sentiment analysis, speech understanding or strategic game\\nplaying. However, because of their nested non-linear structure, these highly\\nsuccessful machine learning and artificial intelligence models are usually\\napplied in a black box manner, i.e., no information is provided about what\\nexactly makes them arrive at their predictions. Since this lack of transparency\\ncan be a major drawback, e.g., in medical applications, the development of\\nmethods for visualizing, explaining and interpreting deep learning models has\\nrecently attracted increasing attention. This paper summarizes recent\\ndevelopments in this field and makes a plea for more interpretability in\\nartificial intelligence. Furthermore, it presents two approaches to explaining\\npredictions of deep learning models, one method which computes the sensitivity\\nof the prediction with respect to changes in the input and one approach which\\nmeaningfully decomposes the decision in terms of the input variables. These\\nmethods are evaluated on three classification tasks.',\n",
              " 'Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?\\nDeep reinforcement learning has achieved many recent successes, but our\\nunderstanding of its strengths and limitations is hampered by the lack of rich\\nenvironments in which we can fully characterize optimal behavior, and\\ncorrespondingly diagnose individual actions against such a characterization.\\nHere we consider a family of combinatorial games, arising from work of Erdos,\\nSelfridge, and Spencer, and we propose their use as environments for evaluating\\nand comparing different approaches to reinforcement learning. These games have\\na number of appealing features: they are challenging for current learning\\napproaches, but they form (i) a low-dimensional, simply parametrized\\nenvironment where (ii) there is a linear closed form solution for optimal\\nbehavior from any state, and (iii) the difficulty of the game can be tuned by\\nchanging environment parameters in an interpretable way. We use these\\nErdos-Selfridge-Spencer games not only to compare different algorithms, but\\ntest for generalization, make comparisons to supervised learning, analyse\\nmultiagent play, and even develop a self play algorithm.',\n",
              " 'Emergence of grid-like representations by training recurrent neural\\n  networks to perform spatial localization\\nDecades of research on the neural code underlying spatial navigation have\\nrevealed a diverse set of neural response properties. The Entorhinal Cortex\\n(EC) of the mammalian brain contains a rich set of spatial correlates,\\nincluding grid cells which encode space using tessellating patterns. However,\\nthe mechanisms and functional significance of these spatial representations\\nremain largely mysterious. As a new way to understand these neural\\nrepresentations, we trained recurrent neural networks (RNNs) to perform\\nnavigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find\\nthat grid-like spatial response patterns emerge in trained networks, along with\\nunits that exhibit other spatial correlates, including border cells and\\nband-like cells. All these different functional types of neurons have been\\nobserved experimentally. The order of the emergence of grid-like and border\\ncells is also consistent with observations from developmental studies.\\nTogether, our results suggest that grid cells, border cells and others as\\nobserved in EC may be a natural solution for representing space efficiently\\ngiven the predominant recurrent connections in the neural circuits.',\n",
              " 'Dimensionality Reduction and Reconstruction using Mirroring Neural\\n  Networks and Object Recognition based on Reduced Dimension Characteristic\\n  Vector\\nIn this paper, we present a Mirroring Neural Network architecture to perform\\nnon-linear dimensionality reduction and Object Recognition using a reduced\\nlowdimensional characteristic vector. In addition to dimensionality reduction,\\nthe network also reconstructs (mirrors) the original high-dimensional input\\nvector from the reduced low-dimensional data. The Mirroring Neural Network\\narchitecture has more number of processing elements (adalines) in the outer\\nlayers and the least number of elements in the central layer to form a\\nconverging-diverging shape in its configuration. Since this network is able to\\nreconstruct the original image from the output of the innermost layer (which\\ncontains all the information about the input pattern), these outputs can be\\nused as object signature to classify patterns. The network is trained to\\nminimize the discrepancy between actual output and the input by back\\npropagating the mean squared error from the output layer to the input layer.\\nAfter successfully training the network, it can reduce the dimension of input\\nvectors and mirror the patterns fed to it. The Mirroring Neural Network\\narchitecture gave very good results on various test patterns.',\n",
              " 'Parcellation of fMRI Datasets with ICA and PLS-A Data Driven Approach\\nInter-subject parcellation of functional Magnetic Resonance Imaging (fMRI)\\ndata based on a standard General Linear Model (GLM)and spectral clustering was\\nrecently proposed as a means to alleviate the issues associated with spatial\\nnormalization in fMRI. However, for all its appeal, a GLM-based parcellation\\napproach introduces its own biases, in the form of a priori knowledge about the\\nshape of Hemodynamic Response Function (HRF) and task-related signal changes,\\nor about the subject behaviour during the task. In this paper, we introduce a\\ndata-driven version of the spectral clustering parcellation, based on\\nIndependent Component Analysis (ICA) and Partial Least Squares (PLS) instead of\\nthe GLM. First, a number of independent components are automatically selected.\\nSeed voxels are then obtained from the associated ICA maps and we compute the\\nPLS latent variables between the fMRI signal of the seed voxels (which covers\\nregional variations of the HRF) and the principal components of the signal\\nacross all voxels. Finally, we parcellate all subjects data with a spectral\\nclustering of the PLS latent variables. We present results of the application\\nof the proposed method on both single-subject and multi-subject fMRI datasets.\\nPreliminary experimental results, evaluated with intra-parcel variance of GLM\\nt-values and PLS derived t-values, indicate that this data-driven approach\\noffers improvement in terms of parcellation accuracy over GLM based techniques.',\n",
              " 'Iris Codes Classification Using Discriminant and Witness Directions\\nThe main topic discussed in this paper is how to use intelligence for\\nbiometric decision defuzzification. A neural training model is proposed and\\ntested here as a possible solution for dealing with natural fuzzification that\\nappears between the intra- and inter-class distribution of scores computed\\nduring iris recognition tests. It is shown here that the use of proposed neural\\nnetwork support leads to an improvement in the artificial perception of the\\nseparation between the intra- and inter-class score distributions by moving\\nthem away from each other.',\n",
              " 'Algorithms for Image Analysis and Combination of Pattern Classifiers\\n  with Application to Medical Diagnosis\\nMedical Informatics and the application of modern signal processing in the\\nassistance of the diagnostic process in medical imaging is one of the more\\nrecent and active research areas today. This thesis addresses a variety of\\nissues related to the general problem of medical image analysis, specifically\\nin mammography, and presents a series of algorithms and design approaches for\\nall the intermediate levels of a modern system for computer-aided diagnosis\\n(CAD). The diagnostic problem is analyzed with a systematic approach, first\\ndefining the imaging characteristics and features that are relevant to probable\\npathology in mammo-grams. Next, these features are quantified and fused into\\nnew, integrated radio-logical systems that exhibit embedded digital signal\\nprocessing, in order to improve the final result and minimize the radiological\\ndose for the patient. In a higher level, special algorithms are designed for\\ndetecting and encoding these clinically interest-ing imaging features, in order\\nto be used as input to advanced pattern classifiers and machine learning\\nmodels. Finally, these approaches are extended in multi-classifier models under\\nthe scope of Game Theory and optimum collective deci-sion, in order to produce\\nefficient solutions for combining classifiers with minimum computational costs\\nfor advanced diagnostic systems. The material covered in this thesis is related\\nto a total of 18 published papers, 6 in scientific journals and 12 in\\ninternational conferences.',\n",
              " 'Deep Neural Networks are Easily Fooled: High Confidence Predictions for\\n  Unrecognizable Images\\nDeep neural networks (DNNs) have recently been achieving state-of-the-art\\nperformance on a variety of pattern-recognition tasks, most notably visual\\nclassification problems. Given that DNNs are now able to classify objects in\\nimages with near-human-level performance, questions naturally arise as to what\\ndifferences remain between computer and human vision. A recent study revealed\\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\\nlion a library). Here we show a related result: it is easy to produce images\\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\\ncertainty that white noise static is a lion). Specifically, we take\\nconvolutional neural networks trained to perform well on either the ImageNet or\\nMNIST datasets and then find images with evolutionary algorithms or gradient\\nascent that DNNs label with high confidence as belonging to each dataset class.\\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\\nbelieve with near certainty are familiar objects, which we call \"fooling\\nimages\" (more generally, fooling examples). Our results shed light on\\ninteresting differences between human vision and current DNNs, and raise\\nquestions about the generality of DNN computer vision.',\n",
              " 'Homogeneous Spiking Neuromorphic System for Real-World Pattern\\n  Recognition\\nA neuromorphic chip that combines CMOS analog spiking neurons and memristive\\nsynapses offers a promising solution to brain-inspired computing, as it can\\nprovide massive neural network parallelism and density. Previous hybrid analog\\nCMOS-memristor approaches required extensive CMOS circuitry for training, and\\nthus eliminated most of the density advantages gained by the adoption of\\nmemristor synapses. Further, they used different waveforms for pre and\\npost-synaptic spikes that added undesirable circuit overhead. Here we describe\\na hardware architecture that can feature a large number of memristor synapses\\nto learn real-world patterns. We present a versatile CMOS neuron that combines\\nintegrate-and-fire behavior, drives passive memristors and implements\\ncompetitive learning in a compact circuit module, and enables in-situ\\nplasticity in the memristor synapses. We demonstrate handwritten-digits\\nrecognition using the proposed architecture using transistor-level circuit\\nsimulations. As the described neuromorphic architecture is homogeneous, it\\nrealizes a fundamental building block for large-scale energy-efficient\\nbrain-inspired silicon chips that could lead to next-generation cognitive\\ncomputing.',\n",
              " 'Crowd Behavior Analysis: A Review where Physics meets Biology\\nAlthough the traits emerged in a mass gathering are often non-deliberative,\\nthe act of mass impulse may lead to irre- vocable crowd disasters. The two-fold\\nincrease of carnage in crowd since the past two decades has spurred significant\\nadvances in the field of computer vision, towards effective and proactive crowd\\nsurveillance. Computer vision stud- ies related to crowd are observed to\\nresonate with the understanding of the emergent behavior in physics (complex\\nsystems) and biology (animal swarm). These studies, which are inspired by\\nbiology and physics, share surprisingly common insights, and interesting\\ncontradictions. However, this aspect of discussion has not been fully explored.\\nTherefore, this survey provides the readers with a review of the\\nstate-of-the-art methods in crowd behavior analysis from the physics and\\nbiologically inspired perspectives. We provide insights and comprehensive\\ndiscussions for a broader understanding of the underlying prospect of blending\\nphysics and biology studies in computer vision.',\n",
              " 'Can Pretrained Neural Networks Detect Anatomy?\\nConvolutional neural networks demonstrated outstanding empirical results in\\ncomputer vision and speech recognition tasks where labeled training data is\\nabundant. In medical imaging, there is a huge variety of possible imaging\\nmodalities and contrasts, where annotated data is usually very scarce. We\\npresent two approaches to deal with this challenge. A network pretrained in a\\ndifferent domain with abundant data is used as a feature extractor, while a\\nsubsequent classifier is trained on a small target dataset; and a deep\\narchitecture trained with heavy augmentation and equipped with sophisticated\\nregularization methods. We test the approaches on a corpus of X-ray images to\\ndesign an anatomy detection system.',\n",
              " 'Metaheuristic Algorithms for Convolution Neural Network\\nA typical modern optimization technique is usually either heuristic or\\nmetaheuristic. This technique has managed to solve some optimization problems\\nin the research area of science, engineering, and industry. However,\\nimplementation strategy of metaheuristic for accuracy improvement on\\nconvolution neural networks (CNN), a famous deep learning method, is still\\nrarely investigated. Deep learning relates to a type of machine learning\\ntechnique, where its aim is to move closer to the goal of artificial\\nintelligence of creating a machine that could successfully perform any\\nintellectual tasks that can be carried out by a human. In this paper, we\\npropose the implementation strategy of three popular metaheuristic approaches,\\nthat is, simulated annealing, differential evolution, and harmony search, to\\noptimize CNN. The performances of these metaheuristic methods in optimizing CNN\\non classifying MNIST and CIFAR dataset were evaluated and compared.\\nFurthermore, the proposed methods are also compared with the original CNN.\\nAlthough the proposed methods show an increase in the computation time, their\\naccuracy has also been improved (up to 7.14 percent).',\n",
              " 'Hadamard Product for Low-rank Bilinear Pooling\\nBilinear models provide rich representations compared with linear models.\\nThey have been applied in various visual tasks, such as object recognition,\\nsegmentation, and visual question-answering, to get state-of-the-art\\nperformances taking advantage of the expanded representations. However,\\nbilinear representations tend to be high-dimensional, limiting the\\napplicability to computationally complex tasks. We propose low-rank bilinear\\npooling using Hadamard product for an efficient attention mechanism of\\nmultimodal learning. We show that our model outperforms compact bilinear\\npooling in visual question-answering tasks with the state-of-the-art results on\\nthe VQA dataset, having a better parsimonious property.',\n",
              " 'Incremental Network Quantization: Towards Lossless CNNs with\\n  Low-Precision Weights\\nThis paper presents incremental network quantization (INQ), a novel method,\\ntargeting to efficiently convert any pre-trained full-precision convolutional\\nneural network (CNN) model into a low-precision version whose weights are\\nconstrained to be either powers of two or zero. Unlike existing methods which\\nare struggled in noticeable accuracy loss, our INQ has the potential to resolve\\nthis issue, as benefiting from two innovations. On one hand, we introduce three\\ninterdependent operations, namely weight partition, group-wise quantization and\\nre-training. A well-proven measure is employed to divide the weights in each\\nlayer of a pre-trained CNN model into two disjoint groups. The weights in the\\nfirst group are responsible to form a low-precision base, thus they are\\nquantized by a variable-length encoding method. The weights in the other group\\nare responsible to compensate for the accuracy loss from the quantization, thus\\nthey are the ones to be re-trained. On the other hand, these three operations\\nare repeated on the latest re-trained group in an iterative manner until all\\nthe weights are converted into low-precision ones, acting as an incremental\\nnetwork quantization and accuracy enhancement procedure. Extensive experiments\\non the ImageNet classification task using almost all known deep CNN\\narchitectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the\\nefficacy of the proposed method. Specifically, at 5-bit quantization, our\\nmodels have improved accuracy than the 32-bit floating-point references. Taking\\nResNet-18 as an example, we further show that our quantized models with 4-bit,\\n3-bit and 2-bit ternary weights have improved or very similar accuracy against\\nits 32-bit floating-point baseline. Besides, impressive results with the\\ncombination of network pruning and INQ are also reported. The code is available\\nat https://github.com/Zhouaojun/Incremental-Network-Quantization.',\n",
              " \"LesionSeg: Semantic segmentation of skin lesions using Deep\\n  Convolutional Neural Network\\nWe present a method for skin lesion segmentation for the ISIC 2017 Skin\\nLesion Segmentation Challenge. Our approach is based on a Fully Convolutional\\nNetwork architecture which is trained end to end, from scratch, on a limited\\ndataset. Our semantic segmentation architecture utilizes several recent\\ninnovations in particularly in the combined use of (i) use of atrous\\nconvolutions to increase the effective field of view of the network's receptive\\nfield without increasing the number of parameters, (ii) the use of\\nnetwork-in-network $1\\\\times1$ convolution layers to add capacity to the network\\nand (iii) state-of-art super-resolution upsampling of predictions using\\nsubpixel CNN layers. We reported a mean IOU score of 0.642 on the validation\\nset provided by the organisers.\",\n",
              " 'Convolutional Spike Timing Dependent Plasticity based Feature Learning\\n  in Spiking Neural Networks\\nBrain-inspired learning models attempt to mimic the cortical architecture and\\ncomputations performed in the neurons and synapses constituting the human brain\\nto achieve its efficiency in cognitive tasks. In this work, we present\\nconvolutional spike timing dependent plasticity based feature learning with\\nbiologically plausible leaky-integrate-and-fire neurons in Spiking Neural\\nNetworks (SNNs). We use shared weight kernels that are trained to encode\\nrepresentative features underlying the input patterns thereby improving the\\nsparsity as well as the robustness of the learning model. We demonstrate that\\nthe proposed unsupervised learning methodology learns several visual categories\\nfor object recognition with fewer number of examples and outperforms\\ntraditional fully-connected SNN architectures while yielding competitive\\naccuracy. Additionally, we observe that the learning model performs out-of-set\\ngeneralization further making the proposed biologically plausible framework a\\nviable and efficient architecture for future neuromorphic applications.',\n",
              " \"Adversarial Transformation Networks: Learning to Generate Adversarial\\n  Examples\\nMultiple different approaches of generating adversarial examples have been\\nproposed to attack deep neural networks. These approaches involve either\\ndirectly computing gradients with respect to the image pixels, or directly\\nsolving an optimization on the image pixels. In this work, we present a\\nfundamentally new method for generating adversarial examples that is fast to\\nexecute and provides exceptional diversity of output. We efficiently train\\nfeed-forward neural networks in a self-supervised manner to generate\\nadversarial examples against a target network or set of networks. We call such\\na network an Adversarial Transformation Network (ATN). ATNs are trained to\\ngenerate adversarial examples that minimally modify the classifier's outputs\\ngiven the original input, while constraining the new classification to match an\\nadversarial target class. We present methods to train ATNs and analyze their\\neffectiveness targeting a variety of MNIST classifiers as well as the latest\\nstate-of-the-art ImageNet classifier Inception ResNet v2.\",\n",
              " 'Opening the Black Box of Financial AI with CLEAR-Trade: A CLass-Enhanced\\n  Attentive Response Approach for Explaining and Visualizing Deep\\n  Learning-Driven Stock Market Prediction\\nDeep learning has been shown to outperform traditional machine learning\\nalgorithms across a wide range of problem domains. However, current deep\\nlearning algorithms have been criticized as uninterpretable \"black-boxes\" which\\ncannot explain their decision making processes. This is a major shortcoming\\nthat prevents the widespread application of deep learning to domains with\\nregulatory processes such as finance. As such, industries such as finance have\\nto rely on traditional models like decision trees that are much more\\ninterpretable but less effective than deep learning for complex problems. In\\nthis paper, we propose CLEAR-Trade, a novel financial AI visualization\\nframework for deep learning-driven stock market prediction that mitigates the\\ninterpretability issue of deep learning methods. In particular, CLEAR-Trade\\nprovides a effective way to visualize and explain decisions made by deep stock\\nmarket prediction models. We show the efficacy of CLEAR-Trade in enhancing the\\ninterpretability of stock market prediction by conducting experiments based on\\nS&P 500 stock index prediction. The results demonstrate that CLEAR-Trade can\\nprovide significant insight into the decision-making process of deep\\nlearning-driven financial models, particularly for regulatory processes, thus\\nimproving their potential uptake in the financial industry.',\n",
              " 'Fast YOLO: A Fast You Only Look Once System for Real-time Embedded\\n  Object Detection in Video\\nObject detection is considered one of the most challenging problems in this\\nfield of computer vision, as it involves the combination of object\\nclassification and object localization within a scene. Recently, deep neural\\nnetworks (DNNs) have been demonstrated to achieve superior object detection\\nperformance compared to other approaches, with YOLOv2 (an improved You Only\\nLook Once model) being one of the state-of-the-art in DNN-based object\\ndetection methods in terms of both speed and accuracy. Although YOLOv2 can\\nachieve real-time performance on a powerful GPU, it still remains very\\nchallenging for leveraging this approach for real-time object detection in\\nvideo on embedded computing devices with limited computational power and\\nlimited memory. In this paper, we propose a new framework called Fast YOLO, a\\nfast You Only Look Once framework which accelerates YOLOv2 to be able to\\nperform object detection in video on embedded devices in a real-time manner.\\nFirst, we leverage the evolutionary deep intelligence framework to evolve the\\nYOLOv2 network architecture and produce an optimized architecture (referred to\\nas O-YOLOv2 here) that has 2.8X fewer parameters with just a ~2% IOU drop. To\\nfurther reduce power consumption on embedded devices while maintaining\\nperformance, a motion-adaptive inference method is introduced into the proposed\\nFast YOLO framework to reduce the frequency of deep inference with O-YOLOv2\\nbased on temporal motion characteristics. Experimental results show that the\\nproposed Fast YOLO framework can reduce the number of deep inferences by an\\naverage of 38.13%, and an average speedup of ~3.3X for objection detection in\\nvideo compared to the original YOLOv2, leading Fast YOLO to run an average of\\n~18FPS on a Nvidia Jetson TX1 embedded system.',\n",
              " 'NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm\\nNeural networks (NNs) have begun to have a pervasive impact on various\\napplications of machine learning. However, the problem of finding an optimal NN\\narchitecture for large applications has remained open for several decades.\\nConventional approaches search for the optimal NN architecture through\\nextensive trial-and-error. Such a procedure is quite inefficient. In addition,\\nthe generated NN architectures incur substantial redundancy. To address these\\nproblems, we propose an NN synthesis tool (NeST) that automatically generates\\nvery compact architectures for a given dataset. NeST starts with a seed NN\\narchitecture. It iteratively tunes the architecture with gradient-based growth\\nand magnitude-based pruning of neurons and connections. Our experimental\\nresults show that NeST yields accurate yet very compact NNs with a wide range\\nof seed architecture selection. For example, for the LeNet-300-100 (LeNet-5) NN\\narchitecture derived from the MNIST dataset, we reduce network parameters by\\n34.1x (74.3x) and floating-point operations (FLOPs) by 35.8x (43.7x). For the\\nAlexNet NN architecture derived from the ImageNet dataset, we reduce network\\nparameters by 15.7x and FLOPs by 4.6x. All these results are the current\\nstate-of-the-art for these architectures.',\n",
              " 'Analysis of supervised and semi-supervised GrowCut applied to\\n  segmentation of masses in mammography images\\nBreast cancer is already one of the most common form of cancer worldwide.\\nMammography image analysis is still the most effective diagnostic method to\\npromote the early detection of breast cancer. Accurately segmenting tumors in\\ndigital mammography images is important to improve diagnosis capabilities of\\nhealth specialists and avoid misdiagnosis. In this work, we evaluate the\\nfeasibility of applying GrowCut to segment regions of tumor and we propose two\\nGrowCut semi-supervised versions. All the analysis was performed by evaluating\\nthe application of segmentation techniques to a set of images obtained from the\\nMini-MIAS mammography image database. GrowCut segmentation was compared to\\nRegion Growing, Active Contours, Random Walks and Graph Cut techniques.\\nExperiments showed that GrowCut, when compared to the other techniques, was\\nable to acquire better results for the metrics analyzed. Moreover, the proposed\\nsemi-supervised versions of GrowCut was proved to have a clinically\\nsatisfactory quality of segmentation.',\n",
              " 'Empirical Explorations in Training Networks with Discrete Activations\\nWe present extensive experiments training and testing hidden units in deep\\nnetworks that emit only a predefined, static, number of discretized values.\\nThese units provide benefits in real-world deployment in systems in which\\nmemory and/or computation may be limited. Additionally, they are particularly\\nwell suited for use in large recurrent network models that require the\\nmaintenance of large amounts of internal state in memory. Surprisingly, we find\\nthat despite reducing the number of values that can be represented in the\\noutput activations from $2^{32}-2^{64}$ to between 64 and 256, there is little\\nto no degradation in network performance across a variety of different\\nsettings. We investigate simple classification and regression tasks, as well as\\nmemorization and compression problems. We compare the results with more\\nstandard activations, such as tanh and relu. Unlike previous discretization\\nstudies which often concentrate only on binary units, we examine the effects of\\nvarying the number of allowed activation levels. Compared to existing\\napproaches for discretization, the approach presented here is both conceptually\\nand programatically simple, has no stochastic component, and allows the\\ntraining, testing, and usage phases to be treated in exactly the same manner.',\n",
              " 'Regularized Evolution for Image Classifier Architecture Search\\nThe effort devoted to hand-crafting image classifiers has motivated the use\\nof architecture search to discover them automatically. Reinforcement learning\\nand evolution have both shown promise for this purpose. This study employs a\\nregularized version of a popular asynchronous evolutionary algorithm. We\\nrigorously compare it to the non-regularized form and to a highly-successful\\nreinforcement learning baseline. Using the same hardware, compute effort and\\nneural network training code, we conduct repeated experiments side-by-side,\\nexploring different datasets, search spaces and scales. We show regularized\\nevolution consistently produces models with similar or higher accuracy, across\\na variety of contexts without need for re-tuning parameters. In addition,\\nevolution exhibits considerably better performance than reinforcement learning\\nat early search stages, suggesting it may be the better choice when fewer\\ncompute resources are available. This constitutes the first controlled\\ncomparison of the two search algorithms in this context. Finally, we present\\nnew architectures discovered with evolution that we nickname AmoebaNets. These\\nmodels achieve state-of-the-art results for CIFAR-10 (mean test error = 2.13%),\\nmobile-size ImageNet (top-1 accuracy = 75.1% with 5.1 M parameters) and\\nImageNet (top-1 accuracy = 83.1%). This is the first time evolutionary\\nalgorithms produce state-of-the-art image classifiers.',\n",
              " 'Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network\\n  for Real-time Embedded Object Detection\\nObject detection is a major challenge in computer vision, involving both\\nobject classification and object localization within a scene. While deep neural\\nnetworks have been shown in recent years to yield very powerful techniques for\\ntackling the challenge of object detection, one of the biggest challenges with\\nenabling such object detection networks for widespread deployment on embedded\\ndevices is high computational and memory requirements. Recently, there has been\\nan increasing focus in exploring small deep neural network architectures for\\nobject detection that are more suitable for embedded devices, such as Tiny YOLO\\nand SqueezeDet. Inspired by the efficiency of the Fire microarchitecture\\nintroduced in SqueezeNet and the object detection performance of the\\nsingle-shot detection macroarchitecture introduced in SSD, this paper\\nintroduces Tiny SSD, a single-shot detection deep convolutional neural network\\nfor real-time embedded object detection that is composed of a highly optimized,\\nnon-uniform Fire sub-network stack and a non-uniform sub-network stack of\\nhighly optimized SSD-based auxiliary convolutional feature layers designed\\nspecifically to minimize model size while maintaining object detection\\nperformance. The resulting Tiny SSD possess a model size of 2.3MB (~26X smaller\\nthan Tiny YOLO) while still achieving an mAP of 61.3% on VOC 2007 (~4.2% higher\\nthan Tiny YOLO). These experimental results show that very small deep neural\\nnetwork architectures can be designed for real-time object detection that are\\nwell-suited for embedded scenarios.',\n",
              " 'Inferencing Based on Unsupervised Learning of Disentangled\\n  Representations\\nCombining Generative Adversarial Networks (GANs) with encoders that learn to\\nencode data points has shown promising results in learning data representations\\nin an unsupervised way. We propose a framework that combines an encoder and a\\ngenerator to learn disentangled representations which encode meaningful\\ninformation about the data distribution without the need for any labels. While\\ncurrent approaches focus mostly on the generative aspects of GANs, our\\nframework can be used to perform inference on both real and generated data\\npoints. Experiments on several data sets show that the encoder learns\\ninterpretable, disentangled representations which encode descriptive properties\\nand can be used to sample images that exhibit specific characteristics.',\n",
              " 'The Parameter-Less Self-Organizing Map algorithm\\nThe Parameter-Less Self-Organizing Map (PLSOM) is a new neural network\\nalgorithm based on the Self-Organizing Map (SOM). It eliminates the need for a\\nlearning rate and annealing schemes for learning rate and neighbourhood size.\\nWe discuss the relative performance of the PLSOM and the SOM and demonstrate\\nsome tasks in which the SOM fails but the PLSOM performs satisfactory. Finally\\nwe discuss some example applications of the PLSOM and present a proof of\\nordering under certain limited conditions.',\n",
              " 'Simplified firefly algorithm for 2D image key-points search\\nIn order to identify an object, human eyes firstly search the field of view\\nfor points or areas which have particular properties. These properties are used\\nto recognise an image or an object. Then this process could be taken as a model\\nto develop computer algorithms for images identification. This paper proposes\\nthe idea of applying the simplified firefly algorithm to search for key-areas\\nin 2D images. For a set of input test images the proposed version of firefly\\nalgorithm has been examined. Research results are presented and discussed to\\nshow the efficiency of this evolutionary computation method.',\n",
              " \"Deep-Plant: Plant Identification with convolutional neural networks\\nThis paper studies convolutional neural networks (CNN) to learn unsupervised\\nfeature representations for 44 different plant species, collected at the Royal\\nBotanic Gardens, Kew, England. To gain intuition on the chosen features from\\nthe CNN model (opposed to a 'black box' solution), a visualisation technique\\nbased on the deconvolutional networks (DN) is utilized. It is found that\\nvenations of different order have been chosen to uniquely represent each of the\\nplant species. Experimental results using these CNN features with different\\nclassifiers show consistency and superiority compared to the state-of-the art\\nsolutions which rely on hand-crafted features.\",\n",
              " 'Adapting Deep Network Features to Capture Psychological Representations\\nDeep neural networks have become increasingly successful at solving classic\\nperception problems such as object recognition, semantic segmentation, and\\nscene understanding, often reaching or surpassing human-level accuracy. This\\nsuccess is due in part to the ability of DNNs to learn useful representations\\nof high-dimensional inputs, a problem that humans must also solve. We examine\\nthe relationship between the representations learned by these networks and\\nhuman psychological representations recovered from similarity judgments. We\\nfind that deep features learned in service of object classification account for\\na significant amount of the variance in human similarity judgments for a set of\\nanimal images. However, these features do not capture some qualitative\\ndistinctions that are a key part of human representations. To remedy this, we\\ndevelop a method for adapting deep features to align with human similarity\\njudgments, resulting in image representations that can potentially be used to\\nextend the scope of psychological experiments.',\n",
              " 'Large-Scale Evolution of Image Classifiers\\nNeural networks have proven effective at solving difficult problems but\\ndesigning their architectures can be challenging, even for image classification\\nproblems alone. Our goal is to minimize human participation, so we employ\\nevolutionary algorithms to discover such networks automatically. Despite\\nsignificant computational requirements, we show that it is now possible to\\nevolve models with accuracies within the range of those published in the last\\nyear. Specifically, we employ simple evolutionary techniques at unprecedented\\nscales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting\\nfrom trivial initial conditions and reaching accuracies of 94.6% (95.6% for\\nensemble) and 77.0%, respectively. To do this, we use novel and intuitive\\nmutation operators that navigate large search spaces; we stress that no human\\nparticipation is required once evolution starts and that the output is a\\nfully-trained model. Throughout this work, we place special emphasis on the\\nrepeatability of results, the variability in the outcomes and the computational\\nrequirements.',\n",
              " \"A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification\\n  and Domain Adaptation\\nRecently, DNN model compression based on network architecture design, e.g.,\\nSqueezeNet, attracted a lot attention. No accuracy drop on image classification\\nis observed on these extremely compact networks, compared to well-known models.\\nAn emerging question, however, is whether these model compression techniques\\nhurt DNN's learning ability other than classifying images on a single dataset.\\nOur preliminary experiment shows that these compression methods could degrade\\ndomain adaptation (DA) ability, though the classification performance is\\npreserved. Therefore, we propose a new compact network architecture and\\nunsupervised DA method in this paper. The DNN is built on a new basic module\\nConv-M which provides more diverse feature extractors without significantly\\nincreasing parameters. The unified framework of our DA method will\\nsimultaneously learn invariance across domains, reduce divergence of feature\\nrepresentations, and adapt label prediction. Our DNN has 4.1M parameters, which\\nis only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN\\nobtains GoogLeNet-level accuracy both on classification and DA, and our DA\\nmethod slightly outperforms previous competitive ones. Put all together, our DA\\nstrategy based on our DNN achieves state-of-the-art on sixteen of total\\neighteen DA tasks on popular Office-31 and Office-Caltech datasets.\",\n",
              " 'Identifying Spatial Relations in Images using Convolutional Neural\\n  Networks\\nTraditional approaches to building a large scale knowledge graph have usually\\nrelied on extracting information (entities, their properties, and relations\\nbetween them) from unstructured text (e.g. Dbpedia). Recent advances in\\nConvolutional Neural Networks (CNN) allow us to shift our focus to learning\\nentities and relations from images, as they build robust models that require\\nlittle or no pre-processing of the images. In this paper, we present an\\napproach to identify and extract spatial relations (e.g., The girl is standing\\nbehind the table) from images using CNNs. Our research addresses two specific\\nchallenges: providing insight into how spatial relations are learned by the\\nnetwork and which parts of the image are used to predict these relations. We\\nuse the pre-trained network VGGNet to extract features from an image and train\\na Multi-layer Perceptron (MLP) on a set of synthetic images and the sun09\\ndataset to extract spatial relations. The MLP predicts spatial relations\\nwithout a bounding box around the objects or the space in the image depicting\\nthe relation. To understand how the spatial relations are represented in the\\nnetwork, a heatmap is overlayed on the image to show the regions that are\\ndeemed important by the network. Also, we analyze the MLP to show the\\nrelationship between the activation of consistent groups of nodes and the\\nprediction of a spatial relation. We show how the loss of these groups affects\\nthe networks ability to identify relations.',\n",
              " 'Hierarchical Attentive Recurrent Tracking\\nClass-agnostic object tracking is particularly difficult in cluttered\\nenvironments as target specific discriminative models cannot be learned a\\npriori. Inspired by how the human visual cortex employs spatial attention and\\nseparate \"where\" and \"what\" processing pathways to actively suppress irrelevant\\nvisual features, this work develops a hierarchical attentive recurrent model\\nfor single object tracking in videos. The first layer of attention discards the\\nmajority of background by selecting a region containing the object of interest,\\nwhile the subsequent layers tune in on visual features particular to the\\ntracked object. This framework is fully differentiable and can be trained in a\\npurely data driven fashion by gradient methods. To improve training\\nconvergence, we augment the loss function with terms for a number of auxiliary\\ntasks relevant for tracking. Evaluation of the proposed model is performed on\\ntwo datasets: pedestrian tracking on the KTH activity recognition dataset and\\nthe more difficult KITTI object tracking dataset.',\n",
              " 'PSIque: Next Sequence Prediction of Satellite Images using a\\n  Convolutional Sequence-to-Sequence Network\\nPredicting unseen weather phenomena is an important issue for disaster\\nmanagement. In this paper, we suggest a model for a convolutional\\nsequence-to-sequence autoencoder for predicting undiscovered weather situations\\nfrom previous satellite images. We also propose a symmetric skip connection\\nbetween encoder and decoder modules to produce more comprehensive image\\npredictions. To examine our model performance, we conducted experiments for\\neach suggested model to predict future satellite images from historical\\nsatellite images. A specific combination of skip connection and\\nsequence-to-sequence autoencoder was able to generate closest prediction from\\nthe ground truth image.',\n",
              " \"Evaluation of Alzheimer's Disease by Analysis of MR Images using\\n  Multilayer Perceptrons and Kohonen SOM Classifiers as an Alternative to the\\n  ADC Maps\\nAlzheimer's disease is the most common cause of dementia, yet hard to\\ndiagnose precisely without invasive techniques, particularly at the onset of\\nthe disease. This work approaches image analysis and classification of\\nsynthetic multispectral images composed by diffusion-weighted magnetic\\nresonance (MR) cerebral images for the evaluation of cerebrospinal fluid area\\nand measuring the advance of Alzheimer's disease. A clinical 1.5 T MR imaging\\nsystem was used to acquire all images presented. The classification methods are\\nbased on multilayer perceptrons and Kohonen Self-Organized Map classifiers. We\\nassume the classes of interest can be separated by hyperquadrics. Therefore, a\\n2-degree polynomial network is used to classify the original image, generating\\nthe ground truth image. The classification results are used to improve the\\nusual analysis of the apparent diffusion coefficient map.\",\n",
              " 'Neural tuning size is a key factor underlying holistic face processing\\nFaces are a class of visual stimuli with unique significance, for a variety\\nof reasons. They are ubiquitous throughout the course of a person\\'s life, and\\nface recognition is crucial for daily social interaction. Faces are also unlike\\nany other stimulus class in terms of certain physical stimulus characteristics.\\nFurthermore, faces have been empirically found to elicit certain characteristic\\nbehavioral phenomena, which are widely held to be evidence of \"holistic\"\\nprocessing of faces. However, little is known about the neural mechanisms\\nunderlying such holistic face processing. In other words, for the processing of\\nfaces by the primate visual system, the input and output characteristics are\\nrelatively well known, but the internal neural computations are not. The main\\naim of this work is to further the fundamental understanding of what causes the\\nvisual processing of faces to be different from that of objects. In this\\ncomputational modeling work, we show that a single factor - \"neural tuning\\nsize\" - is able to account for three key phenomena that are characteristic of\\nface processing, namely the Composite Face Effect (CFE), Face Inversion Effect\\n(FIE) and Whole-Part Effect (WPE). Our computational proof-of-principle\\nprovides specific neural tuning properties that correspond to the\\npoorly-understood notion of holistic face processing, and connects these neural\\nproperties to psychophysical behavior. Overall, our work provides a unified and\\nparsimonious theoretical account for the disparate empirical data on\\nface-specific processing, deepening the fundamental understanding of face\\nprocessing.',\n",
              " 'Distribution of the search of evolutionary product unit neural networks\\n  for classification\\nThis paper deals with the distributed processing in the search for an optimum\\nclassification model using evolutionary product unit neural networks. For this\\ndistributed search we used a cluster of computers. Our objective is to obtain a\\nmore efficient design than those net architectures which do not use a\\ndistributed process and which thus result in simpler designs. In order to get\\nthe best classification models we use evolutionary algorithms to train and\\ndesign neural networks, which require a very time consuming computation. The\\nreasons behind the need for this distribution are various. It is complicated to\\ntrain this type of nets because of the difficulty entailed in determining their\\narchitecture due to the complex error surface. On the other hand, the use of\\nevolutionary algorithms involves running a great number of tests with different\\nseeds and parameters, thus resulting in a high computational cost',\n",
              " 'Correlation Alignment for Unsupervised Domain Adaptation\\nIn this chapter, we present CORrelation ALignment (CORAL), a simple yet\\neffective method for unsupervised domain adaptation. CORAL minimizes domain\\nshift by aligning the second-order statistics of source and target\\ndistributions, without requiring any target labels. In contrast to subspace\\nmanifold methods, it aligns the original feature distributions of the source\\nand target domains, rather than the bases of lower-dimensional subspaces. It is\\nalso much simpler than other distribution matching methods. CORAL performs\\nremarkably well in extensive evaluations on standard benchmark datasets. We\\nfirst describe a solution that applies a linear transformation to source\\nfeatures to align them with target features before classifier training. For\\nlinear classifiers, we propose to equivalently apply CORAL to the classifier\\nweights, leading to added efficiency when the number of classifiers is small\\nbut the number and dimensionality of target examples are very high. The\\nresulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a\\nlarge margin on standard domain adaptation benchmarks. Finally, we extend CORAL\\nto learn a nonlinear transformation that aligns correlations of layer\\nactivations in deep neural networks (DNNs). The resulting Deep CORAL approach\\nworks seamlessly with DNNs and achieves state-of-the-art performance on\\nstandard benchmark datasets. Our code is available\\nat:~\\\\url{https://github.com/VisionLearningGroup/CORAL}',\n",
              " \"CITlab ARGUS for historical handwritten documents\\nWe describe CITlab's recognition system for the HTRtS competition attached to\\nthe 13. International Conference on Document Analysis and Recognition, ICDAR\\n2015. The task comprises the recognition of historical handwritten documents.\\nThe core algorithms of our system are based on multi-dimensional recurrent\\nneural networks (MDRNN) and connectionist temporal classification (CTC). The\\nsoftware modules behind that as well as the basic utility technologies are\\nessentially powered by PLANET's ARGUS framework for intelligent text\\nrecognition and image processing.\",\n",
              " 'Generalized Haar Filter based Deep Networks for Real-Time Object\\n  Detection in Traffic Scene\\nVision-based object detection is one of the fundamental functions in numerous\\ntraffic scene applications such as self-driving vehicle systems and advance\\ndriver assistance systems (ADAS). However, it is also a challenging task due to\\nthe diversity of traffic scene and the storage, power and computing source\\nlimitations of the platforms for traffic scene applications. This paper\\npresents a generalized Haar filter based deep network which is suitable for the\\nobject detection tasks in traffic scene. In this approach, we first decompose a\\nobject detection task into several easier local regression tasks. Then, we\\nhandle the local regression tasks by using several tiny deep networks which\\nsimultaneously output the bounding boxes, categories and confidence scores of\\ndetected objects. To reduce the consumption of storage and computing resources,\\nthe weights of the deep networks are constrained to the form of generalized\\nHaar filter in training phase. Additionally, we introduce the strategy of\\nsparse windows generation to improve the efficiency of the algorithm. Finally,\\nwe perform several experiments to validate the performance of our proposed\\napproach. Experimental results demonstrate that the proposed approach is both\\nefficient and effective in traffic scene compared with the state-of-the-art.',\n",
              " \"Autoencoder Regularized Network For Driving Style Representation\\n  Learning\\nIn this paper, we study learning generalized driving style representations\\nfrom automobile GPS trip data. We propose a novel Autoencoder Regularized deep\\nneural Network (ARNet) and a trip encoding framework trip2vec to learn drivers'\\ndriving styles directly from GPS records, by combining supervised and\\nunsupervised feature learning in a unified architecture. Experiments on a\\nchallenging driver number estimation problem and the driver identification\\nproblem show that ARNet can learn a good generalized driving style\\nrepresentation: It significantly outperforms existing methods and alternative\\narchitectures by reaching the least estimation error on average (0.68, less\\nthan one driver) and the highest identification accuracy (by at least 3%\\nimprovement) compared with traditional supervised learning methods.\",\n",
              " 'Fashioning with Networks: Neural Style Transfer to Design Clothes\\nConvolutional Neural Networks have been highly successful in performing a\\nhost of computer vision tasks such as object recognition, object detection,\\nimage segmentation and texture synthesis. In 2015, Gatys et. al [7] show how\\nthe style of a painter can be extracted from an image of the painting and\\napplied to another normal photograph, thus recreating the photo in the style of\\nthe painter. The method has been successfully applied to a wide range of images\\nand has since spawned multiple applications and mobile apps. In this paper, the\\nneural style transfer algorithm is applied to fashion so as to synthesize new\\ncustom clothes. We construct an approach to personalize and generate new custom\\nclothes based on a users preference and by learning the users fashion choices\\nfrom a limited set of clothes from their closet. The approach is evaluated by\\nanalyzing the generated images of clothes and how well they align with the\\nusers fashion style.',\n",
              " 'GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from\\n  Remote Sensing Imagery\\nAdvances in remote sensing technologies have made it possible to use\\nhigh-resolution visual data for weather observation and forecasting tasks. We\\npropose the use of multi-layer neural networks for understanding complex\\natmospheric dynamics based on multichannel satellite images. The capability of\\nour model was evaluated by using a linear regression task for single typhoon\\ncoordinates prediction. A specific combination of models and different\\nactivation policies enabled us to obtain an interesting prediction result in\\nthe northeastern hemisphere (ENH).',\n",
              " 'Improving Efficiency in Convolutional Neural Network with Multilinear\\n  Filters\\nThe excellent performance of deep neural networks has enabled us to solve\\nseveral automatization problems, opening an era of autonomous devices. However,\\ncurrent deep net architectures are heavy with millions of parameters and\\nrequire billions of floating point operations. Several works have been\\ndeveloped to compress a pre-trained deep network to reduce memory footprint\\nand, possibly, computation. Instead of compressing a pre-trained network, in\\nthis work, we propose a generic neural network layer structure employing\\nmultilinear projection as the primary feature extractor. The proposed\\narchitecture requires several times less memory as compared to the traditional\\nConvolutional Neural Networks (CNN), while inherits the similar design\\nprinciples of a CNN. In addition, the proposed architecture is equipped with\\ntwo computation schemes that enable computation reduction or scalability.\\nExperimental results show the effectiveness of our compact projection that\\noutperforms traditional CNN, while requiring far fewer parameters.',\n",
              " 'Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided\\n  Diagnosis of Diabetic Retinopathy\\nObjective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown\\nconsiderable promise in recent years as a potential tool for improving clinical\\ndecision support in medical oncology, particularly those based around the\\nconcept of Discovery Radiomics, where radiomic sequencers are discovered\\nthrough the analysis of medical imaging data. One of the main limitations with\\ncurrent CAD approaches is that it is very difficult to gain insight or\\nrationale as to how decisions are made, thus limiting their utility to\\nclinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable\\nCAD system based on the notion of CLass-Enhanced Attentive Response Discovery\\nRadiomics for the purpose of clinical decision support for diabetic\\nretinopathy. Results: In addition to disease grading via the discovered deep\\nradiomic sequencer, the CLEAR-DR system also produces a visual interpretation\\nof the decision-making process to provide better insight and understanding into\\nthe decision-making process of the system. Conclusion: We demonstrate the\\neffectiveness and utility of the proposed CLEAR-DR system of enhancing the\\ninterpretability of diagnostic grading results for the application of diabetic\\nretinopathy grading. Significance: CLEAR-DR can act as a potential powerful\\ntool to address the uninterpretability issue of current CAD systems, thus\\nimproving their utility to clinicians.',\n",
              " 'HP-GAN: Probabilistic 3D human motion prediction via GAN\\nPredicting and understanding human motion dynamics has many applications,\\nsuch as motion synthesis, augmented reality, security, and autonomous vehicles.\\nDue to the recent success of generative adversarial networks (GAN), there has\\nbeen much interest in probabilistic estimation and synthetic data generation\\nusing deep neural network architectures and learning algorithms.\\n  We propose a novel sequence-to-sequence model for probabilistic human motion\\nprediction, trained with a modified version of improved Wasserstein generative\\nadversarial networks (WGAN-GP), in which we use a custom loss function designed\\nfor human motion prediction. Our model, which we call HP-GAN, learns a\\nprobability density function of future human poses conditioned on previous\\nposes. It predicts multiple sequences of possible future human poses, each from\\nthe same input sequence but a different vector z drawn from a random\\ndistribution. Furthermore, to quantify the quality of the non-deterministic\\npredictions, we simultaneously train a motion-quality-assessment model that\\nlearns the probability that a given skeleton sequence is a real human motion.\\n  We test our algorithm on two of the largest skeleton datasets: NTURGB-D and\\nHuman3.6M. We train our model on both single and multiple action types. Its\\npredictive power for long-term motion estimation is demonstrated by generating\\nmultiple plausible futures of more than 30 frames from just 10 frames of input.\\nWe show that most sequences generated from the same input have more than 50\\\\%\\nprobabilities of being judged as a real human sequence. We will release all the\\ncode used in this paper to Github.',\n",
              " 'Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro\\n  Gesture\\nIn the research of the impact of gestures using by a lecturer, one\\nchallenging task is to infer the attention of a group of audiences. Two\\nimportant measurements that can help infer the level of attention are eye\\nmovement data and Electroencephalography (EEG) data. Under the fundamental\\nassumption that a group of people would look at the same place if they all pay\\nattention at the same time, we apply a method, \"Time Warp Edit Distance\", to\\ncalculate the similarity of their eye movement trajectories. Moreover, we also\\ncluster eye movement pattern of audiences based on these pair-wised similarity\\nmetrics. Besides, since we don\\'t have a direct metric for the \"attention\"\\nground truth, a visual assessment would be beneficial to evaluate the\\ngesture-attention relationship. Thus we also implement a visualization tool.',\n",
              " 'Nature vs. Nurture: The Role of Environmental Resources in Evolutionary\\n  Deep Intelligence\\nEvolutionary deep intelligence synthesizes highly efficient deep neural\\nnetworks architectures over successive generations. Inspired by the nature\\nversus nurture debate, we propose a study to examine the role of external\\nfactors on the network synthesis process by varying the availability of\\nsimulated environmental resources. Experimental results were obtained for\\nnetworks synthesized via asexual evolutionary synthesis (1-parent) and sexual\\nevolutionary synthesis (2-parent, 3-parent, and 5-parent) using a 10% subset of\\nthe MNIST dataset. Results show that a lower environmental factor model\\nresulted in a more gradual loss in performance accuracy and decrease in storage\\nsize. This potentially allows significantly reduced storage size with minimal\\nto no drop in performance accuracy, and the best networks were synthesized\\nusing the lowest environmental factor models.',\n",
              " 'A stochastic model of human visual attention with a dynamic Bayesian\\n  network\\nRecent studies in the field of human vision science suggest that the human\\nresponses to the stimuli on a visual display are non-deterministic. People may\\nattend to different locations on the same visual input at the same time. Based\\non this knowledge, we propose a new stochastic model of visual attention by\\nintroducing a dynamic Bayesian network to predict the likelihood of where\\nhumans typically focus on a video scene. The proposed model is composed of a\\ndynamic Bayesian network with 4 layers. Our model provides a framework that\\nsimulates and combines the visual saliency response and the cognitive state of\\na person to estimate the most probable attended regions. Sample-based inference\\nwith Markov chain Monte-Carlo based particle filter and stream processing with\\nmulti-core processors enable us to estimate human visual attention in near real\\ntime. Experimental results have demonstrated that our model performs\\nsignificantly better in predicting human visual attention compared to the\\nprevious deterministic models.',\n",
              " 'Smart Content Recognition from Images Using a Mixture of Convolutional\\n  Neural Networks\\nWith rapid development of the Internet, web contents become huge. Most of the\\nwebsites are publicly available, and anyone can access the contents from\\nanywhere such as workplace, home and even schools. Nevertheless, not all the\\nweb contents are appropriate for all users, especially children. An example of\\nthese contents is pornography images which should be restricted to certain age\\ngroup. Besides, these images are not safe for work (NSFW) in which employees\\nshould not be seen accessing such contents during work. Recently, convolutional\\nneural networks have been successfully applied to many computer vision\\nproblems. Inspired by these successes, we propose a mixture of convolutional\\nneural networks for adult content recognition. Unlike other works, our method\\nis formulated on a weighted sum of multiple deep neural network models. The\\nweights of each CNN models are expressed as a linear regression problem learned\\nusing Ordinary Least Squares (OLS). Experimental results demonstrate that the\\nproposed model outperforms both single CNN model and the average sum of CNN\\nmodels in adult content recognition.',\n",
              " 'Cortical spatio-temporal dimensionality reduction for visual grouping\\nThe visual systems of many mammals, including humans, is able to integrate\\nthe geometric information of visual stimuli and to perform cognitive tasks\\nalready at the first stages of the cortical processing. This is thought to be\\nthe result of a combination of mechanisms, which include feature extraction at\\nsingle cell level and geometric processing by means of cells connectivity. We\\npresent a geometric model of such connectivities in the space of detected\\nfeatures associated to spatio-temporal visual stimuli, and show how they can be\\nused to obtain low-level object segmentation. The main idea is that of defining\\na spectral clustering procedure with anisotropic affinities over datasets\\nconsisting of embeddings of the visual stimuli into higher dimensional spaces.\\nNeural plausibility of the proposed arguments will be discussed.',\n",
              " 'Visual Sentiment Prediction with Deep Convolutional Neural Networks\\nImages have become one of the most popular types of media through which users\\nconvey their emotions within online social networks. Although vast amount of\\nresearch is devoted to sentiment analysis of textual data, there has been very\\nlimited work that focuses on analyzing sentiment of image data. In this work,\\nwe propose a novel visual sentiment prediction framework that performs image\\nunderstanding with Deep Convolutional Neural Networks (CNN). Specifically, the\\nproposed sentiment prediction framework performs transfer learning from a CNN\\nwith millions of parameters, which is pre-trained on large-scale data for\\nobject recognition. Experiments conducted on two real-world datasets from\\nTwitter and Tumblr demonstrate the effectiveness of the proposed visual\\nsentiment analysis framework.',\n",
              " 'Correntropy Maximization via ADMM - Application to Robust Hyperspectral\\n  Unmixing\\nIn hyperspectral images, some spectral bands suffer from low signal-to-noise\\nratio due to noisy acquisition and atmospheric effects, thus requiring robust\\ntechniques for the unmixing problem. This paper presents a robust supervised\\nspectral unmixing approach for hyperspectral images. The robustness is achieved\\nby writing the unmixing problem as the maximization of the correntropy\\ncriterion subject to the most commonly used constraints. Two unmixing problems\\nare derived: the first problem considers the fully-constrained unmixing, with\\nboth the non-negativity and sum-to-one constraints, while the second one deals\\nwith the non-negativity and the sparsity-promoting of the abundances. The\\ncorresponding optimization problems are solved efficiently using an alternating\\ndirection method of multipliers (ADMM) approach. Experiments on synthetic and\\nreal hyperspectral images validate the performance of the proposed algorithms\\nfor different scenarios, demonstrating that the correntropy-based unmixing is\\nrobust to outlier bands.',\n",
              " 'Identifying individual facial expressions by deconstructing a neural\\n  network\\nThis paper focuses on the problem of explaining predictions of psychological\\nattributes such as attractiveness, happiness, confidence and intelligence from\\nface photographs using deep neural networks. Since psychological attribute\\ndatasets typically suffer from small sample sizes, we apply transfer learning\\nwith two base models to avoid overfitting. These models were trained on an age\\nand gender prediction task, respectively. Using a novel explanation method we\\nextract heatmaps that highlight the parts of the image most responsible for the\\nprediction. We further observe that the explanation method provides important\\ninsights into the nature of features of the base model, which allow one to\\nassess the aptitude of the base model for a given transfer learning task.\\nFinally, we observe that the multiclass model is more feature rich than its\\nbinary counterpart. The experimental evaluation is performed on the 2222 images\\nfrom the 10k US faces dataset containing psychological attribute labels as well\\nas on a subset of KDEF images.',\n",
              " 'Object Boundary Detection and Classification with Image-level Labels\\nSemantic boundary and edge detection aims at simultaneously detecting object\\nedge pixels in images and assigning class labels to them. Systematic training\\nof predictors for this task requires the labeling of edges in images which is a\\nparticularly tedious task. We propose a novel strategy for solving this task,\\nwhen pixel-level annotations are not available, performing it in an almost\\nzero-shot manner by relying on conventional whole image neural net classifiers\\nthat were trained using large bounding boxes. Our method performs the following\\ntwo steps at test time. Firstly it predicts the class labels by applying the\\ntrained whole image network to the test images. Secondly, it computes\\npixel-wise scores from the obtained predictions by applying backprop gradients\\nas well as recent visualization algorithms such as deconvolution and layer-wise\\nrelevance propagation. We show that high pixel-wise scores are indicative for\\nthe location of semantic boundaries, which suggests that the semantic boundary\\nproblem can be approached without using edge labels during the training phase.',\n",
              " 'Evolving Spatially Aggregated Features from Satellite Imagery for\\n  Regional Modeling\\nSatellite imagery and remote sensing provide explanatory variables at\\nrelatively high resolutions for modeling geospatial phenomena, yet regional\\nsummaries are often desirable for analysis and actionable insight. In this\\npaper, we propose a novel method of inducing spatial aggregations as a\\ncomponent of the machine learning process, yielding regional model features\\nwhose construction is driven by model prediction performance rather than prior\\nassumptions. Our results demonstrate that Genetic Programming is particularly\\nwell suited to this type of feature construction because it can automatically\\nsynthesize appropriate aggregations, as well as better incorporate them into\\npredictive models compared to other regression methods we tested. In our\\nexperiments we consider a specific problem instance and real-world dataset\\nrelevant to predicting snow properties in high-mountain Asia.',\n",
              " 'Pillar Networks++: Distributed non-parametric deep and wide networks\\nIn recent work, it was shown that combining multi-kernel based support vector\\nmachines (SVMs) can lead to near state-of-the-art performance on an action\\nrecognition dataset (HMDB-51 dataset). This was 0.4\\\\% lower than frameworks\\nthat used hand-crafted features in addition to the deep convolutional feature\\nextractors. In the present work, we show that combining distributed Gaussian\\nProcesses with multi-stream deep convolutional neural networks (CNN) alleviate\\nthe need to augment a neural network with hand-crafted features. In contrast to\\nprior work, we treat each deep neural convolutional network as an expert\\nwherein the individual predictions (and their respective uncertainties) are\\ncombined into a Product of Experts (PoE) framework.',\n",
              " 'Market-Based Reinforcement Learning in Partially Observable Worlds\\nUnlike traditional reinforcement learning (RL), market-based RL is in\\nprinciple applicable to worlds described by partially observable Markov\\nDecision Processes (POMDPs), where an agent needs to learn short-term memories\\nof relevant previous events in order to execute optimal actions. Most previous\\nwork, however, has focused on reactive settings (MDPs) instead of POMDPs. Here\\nwe reimplement a recent approach to market-based RL and for the first time\\nevaluate it in a toy POMDP setting.',\n",
              " 'Controlled hierarchical filtering: Model of neocortical sensory\\n  processing\\nA model of sensory information processing is presented. The model assumes\\nthat learning of internal (hidden) generative models, which can predict the\\nfuture and evaluate the precision of that prediction, is of central importance\\nfor information extraction. Furthermore, the model makes a bridge to\\ngoal-oriented systems and builds upon the structural similarity between the\\narchitecture of a robust controller and that of the hippocampal entorhinal\\nloop. This generative control architecture is mapped to the neocortex and to\\nthe hippocampal entorhinal loop. Implicit memory phenomena; priming and\\nprototype learning are emerging features of the model. Mathematical theorems\\nensure stability and attractive learning properties of the architecture.\\nConnections to reinforcement learning are also established: both the control\\nnetwork, and the network with a hidden model converge to (near) optimal policy\\nunder suitable conditions. Falsifying predictions, including the role of the\\nfeedback connections between neocortical areas are made.',\n",
              " \"When Do Differences Matter? On-Line Feature Extraction Through Cognitive\\n  Economy\\nFor an intelligent agent to be truly autonomous, it must be able to adapt its\\nrepresentation to the requirements of its task as it interacts with the world.\\nMost current approaches to on-line feature extraction are ad hoc; in contrast,\\nthis paper presents an algorithm that bases judgments of state compatibility\\nand state-space abstraction on principled criteria derived from the\\npsychological principle of cognitive economy. The algorithm incorporates an\\nactive form of Q-learning, and partitions continuous state-spaces by merging\\nand splitting Voronoi regions. The experiments illustrate a new methodology for\\ntesting and comparing representations by means of learning curves. Results from\\nthe puck-on-a-hill task demonstrate the algorithm's ability to learn effective\\nrepresentations, superior to those produced by some other, well-known, methods.\",\n",
              " 'Applying Policy Iteration for Training Recurrent Neural Networks\\nRecurrent neural networks are often used for learning time-series data. Based\\non a few assumptions we model this learning task as a minimization problem of a\\nnonlinear least-squares cost function. The special structure of the cost\\nfunction allows us to build a connection to reinforcement learning. We exploit\\nthis connection and derive a convergent, policy iteration-based algorithm.\\nFurthermore, we argue that RNN training can be fit naturally into the\\nreinforcement learning framework.',\n",
              " 'A Neural-Network Technique to Learn Concepts from Electroencephalograms\\nA new technique is presented developed to learn multi-class concepts from\\nclinical electroencephalograms. A desired concept is represented as a neuronal\\ncomputational model consisting of the input, hidden, and output neurons. In\\nthis model the hidden neurons learn independently to classify the\\nelectroencephalogram segments presented by spectral and statistical features.\\nThis technique has been applied to the electroencephalogram data recorded from\\n65 sleeping healthy newborns in order to learn a brain maturation concept of\\nnewborns aged between 35 and 51 weeks. The 39399 and 19670 segments from these\\ndata have been used for learning and testing the concept, respectively. As a\\nresult, the concept has correctly classified 80.1% of the testing segments or\\n87.7% of the 65 records.',\n",
              " \"Empirical learning aided by weak domain knowledge in the form of feature\\n  importance\\nStandard hybrid learners that use domain knowledge require stronger knowledge\\nthat is hard and expensive to acquire. However, weaker domain knowledge can\\nbenefit from prior knowledge while being cost effective. Weak knowledge in the\\nform of feature relative importance (FRI) is presented and explained. Feature\\nrelative importance is a real valued approximation of a feature's importance\\nprovided by experts. Advantage of using this knowledge is demonstrated by IANN,\\na modified multilayer neural network algorithm. IANN is a very simple\\nmodification of standard neural network algorithm but attains significant\\nperformance gains. Experimental results in the field of molecular biology show\\nhigher performance over other empirical learning algorithms including standard\\nbackpropagation and support vector machines. IANN performance is even\\ncomparable to a theory refinement system KBANN that uses stronger domain\\nknowledge. This shows Feature relative importance can improve performance of\\nexisting empirical learning algorithms significantly with minimal effort.\",\n",
              " 'Evolutionary Algorithms for Reinforcement Learning\\nThere are two distinct approaches to solving reinforcement learning problems,\\nnamely, searching in value function space and searching in policy space.\\nTemporal difference methods and evolutionary algorithms are well-known examples\\nof these approaches. Kaelbling, Littman and Moore recently provided an\\ninformative survey of temporal difference methods. This article focuses on the\\napplication of evolutionary algorithms to the reinforcement learning problem,\\nemphasizing alternative policy representations, credit assignment methods, and\\nproblem-specific genetic operators. Strengths and weaknesses of the\\nevolutionary approach to reinforcement learning are presented, along with a\\nsurvey of representative applications.',\n",
              " 'On Training Deep Boltzmann Machines\\nThe deep Boltzmann machine (DBM) has been an important development in the\\nquest for powerful \"deep\" probabilistic models. To date, simultaneous or joint\\ntraining of all layers of the DBM has been largely unsuccessful with existing\\ntraining methods. We introduce a simple regularization scheme that encourages\\nthe weight vectors associated with each hidden unit to have similar norms. We\\ndemonstrate that this regularization can be easily combined with standard\\nstochastic maximum likelihood to yield an effective training strategy for the\\nsimultaneous training of all layers of the deep Boltzmann machine.',\n",
              " 'Memristive fuzzy edge detector\\nFuzzy inference systems always suffer from the lack of efficient structures\\nor platforms for their hardware implementation. In this paper, we tried to\\novercome this problem by proposing new method for the implementation of those\\nfuzzy inference systems which use fuzzy rule base to make inference. To achieve\\nthis goal, we have designed a multi-layer neuro-fuzzy computing system based on\\nthe memristor crossbar structure by introducing some new concepts like fuzzy\\nminterms. Although many applications can be realized through the use of our\\nproposed system, in this study we show how the fuzzy XOR function can be\\nconstructed and how it can be used to extract edges from grayscale images. Our\\nmemristive fuzzy edge detector (implemented in analog form) compared with other\\ncommon edge detectors has this advantage that it can extract edges of any given\\nimage all at once in real-time.',\n",
              " 'Echo State Queueing Network: a new reservoir computing learning tool\\nIn the last decade, a new computational paradigm was introduced in the field\\nof Machine Learning, under the name of Reservoir Computing (RC). RC models are\\nneural networks which a recurrent part (the reservoir) that does not\\nparticipate in the learning process, and the rest of the system where no\\nrecurrence (no neural circuit) occurs. This approach has grown rapidly due to\\nits success in solving learning tasks and other computational applications.\\nSome success was also observed with another recently proposed neural network\\ndesigned using Queueing Theory, the Random Neural Network (RandNN). Both\\napproaches have good properties and identified drawbacks. In this paper, we\\npropose a new RC model called Echo State Queueing Network (ESQN), where we use\\nideas coming from RandNNs for the design of the reservoir. ESQNs consist in\\nESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The\\npaper positions ESQNs in the global Machine Learning area, and provides\\nexamples of their use and performances. We show on largely used benchmarks that\\nESQNs are very accurate tools, and we illustrate how they compare with standard\\nESNs.',\n",
              " 'The Predictron: End-To-End Learning and Planning\\nOne of the key challenges of artificial intelligence is to learn models that\\nare effective in the context of planning. In this document we introduce the\\npredictron architecture. The predictron consists of a fully abstract model,\\nrepresented by a Markov reward process, that can be rolled forward multiple\\n\"imagined\" planning steps. Each forward pass of the predictron accumulates\\ninternal rewards and values over multiple planning depths. The predictron is\\ntrained end-to-end so as to make these accumulated values accurately\\napproximate the true value function. We applied the predictron to procedurally\\ngenerated random mazes and a simulator for the game of pool. The predictron\\nyielded significantly more accurate predictions than conventional deep neural\\nnetwork architectures.',\n",
              " 'Quadratically constrained quadratic programming for classification using\\n  particle swarms and applications\\nParticle swarm optimization is used in several combinatorial optimization\\nproblems. In this work, particle swarms are used to solve quadratic programming\\nproblems with quadratic constraints. The approach of particle swarms is an\\nexample for interior point methods in optimization as an iterative technique.\\nThis approach is novel and deals with classification problems without the use\\nof a traditional classifier. Our method determines the optimal hyperplane or\\nclassification boundary for a data set. In a binary classification problem, we\\nconstrain each class as a cluster, which is enclosed by an ellipsoid. The\\nestimation of the optimal hyperplane between the two clusters is posed as a\\nquadratically constrained quadratic problem. The optimization problem is solved\\nin distributed format using modified particle swarms. Our method has the\\nadvantage of using the direction towards optimal solution rather than searching\\nthe entire feasible region. Our results on the Iris, Pima, Wine, and Thyroid\\ndatasets show that the proposed method works better than a neural network and\\nthe performance is close to that of SVM.',\n",
              " \"Learning to Execute\\nRecurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are\\nwidely used because they are expressive and are easy to train. Our interest\\nlies in empirically evaluating the expressiveness and the learnability of LSTMs\\nin the sequence-to-sequence regime by training them to evaluate short computer\\nprograms, a domain that has traditionally been seen as too complex for neural\\nnetworks. We consider a simple class of programs that can be evaluated with a\\nsingle left-to-right pass using constant memory. Our main result is that LSTMs\\ncan learn to map the character-level representations of such programs to their\\ncorrect outputs. Notably, it was necessary to use curriculum learning, and\\nwhile conventional curriculum learning proved ineffective, we developed a new\\nvariant of curriculum learning that improved our networks' performance in all\\nexperimental conditions. The improved curriculum had a dramatic impact on an\\naddition problem, making it possible to train an LSTM to add two 9-digit\\nnumbers with 99% accuracy.\",\n",
              " 'Bitwise Neural Networks\\nBased on the assumption that there exists a neural network that efficiently\\nrepresents a set of Boolean functions between all binary inputs and outputs, we\\npropose a process for developing and deploying neural networks whose weight\\nparameters, bias terms, input, and intermediate hidden layer output signals,\\nare all binary-valued, and require only basic bit logic for the feedforward\\npass. The proposed Bitwise Neural Network (BNN) is especially suitable for\\nresource-constrained environments, since it replaces either floating or\\nfixed-point arithmetic with significantly more efficient bitwise operations.\\nHence, the BNN requires for less spatial complexity, less memory bandwidth, and\\nless power consumption in hardware. In order to design such networks, we\\npropose to add a few training schemes, such as weight compression and noisy\\nbackpropagation, which result in a bitwise network that performs almost as well\\nas its corresponding real-valued network. We test the proposed network on the\\nMNIST dataset, represented using binary features, and show that BNNs result in\\ncompetitive performance while offering dramatic computational savings.',\n",
              " 'Graying the black box: Understanding DQNs\\nIn recent years there is a growing interest in using deep representations for\\nreinforcement learning. In this paper, we present a methodology and tools to\\nanalyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a\\nnew model, the Semi Aggregated Markov Decision Process (SAMDP), and an\\nalgorithm that learns it automatically. The SAMDP model allows us to identify\\nspatio-temporal abstractions directly from features and may be used as a\\nsub-goal detector in future work. Using our tools we reveal that the features\\nlearned by DQNs aggregate the state space in a hierarchical fashion, explaining\\nits success. Moreover, we are able to understand and describe the policies\\nlearned by DQNs for three different Atari2600 games and suggest ways to\\ninterpret, debug and optimize deep neural networks in reinforcement learning.',\n",
              " 'Evaluation of a Tree-based Pipeline Optimization Tool for Automating\\n  Data Science\\nAs the field of data science continues to grow, there will be an\\never-increasing demand for tools that make machine learning accessible to\\nnon-experts. In this paper, we introduce the concept of tree-based pipeline\\noptimization for automating one of the most tedious parts of machine\\nlearning---pipeline design. We implement an open source Tree-based Pipeline\\nOptimization Tool (TPOT) in Python and demonstrate its effectiveness on a\\nseries of simulated and real-world benchmark data sets. In particular, we show\\nthat TPOT can design machine learning pipelines that provide a significant\\nimprovement over a basic machine learning analysis while requiring little to no\\ninput nor prior knowledge from the user. We also address the tendency for TPOT\\nto design overly complex pipelines by integrating Pareto optimization, which\\nproduces compact pipelines without sacrificing classification accuracy. As\\nsuch, this work represents an important step toward fully automating machine\\nlearning pipeline design.',\n",
              " 'Probabilistic Reasoning via Deep Learning: Neural Association Models\\nIn this paper, we propose a new deep learning approach, called neural\\nassociation model (NAM), for probabilistic reasoning in artificial\\nintelligence. We propose to use neural networks to model association between\\nany two events in a domain. Neural networks take one event as input and compute\\na conditional probability of the other event to model how likely these two\\nevents are to be associated. The actual meaning of the conditional\\nprobabilities varies between applications and depends on how the models are\\ntrained. In this work, as two case studies, we have investigated two NAM\\nstructures, namely deep neural networks (DNN) and relation-modulated neural\\nnets (RMNN), on several probabilistic reasoning tasks in AI, including\\nrecognizing textual entailment, triple classification in multi-relational\\nknowledge bases and commonsense reasoning. Experimental results on several\\npopular datasets derived from WordNet, FreeBase and ConceptNet have all\\ndemonstrated that both DNNs and RMNNs perform equally well and they can\\nsignificantly outperform the conventional methods available for these reasoning\\ntasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer,\\nwhere a pre-trained model can be quickly extended to an unseen relation after\\nobserving only a few training samples. To further prove the effectiveness of\\nthe proposed models, in this work, we have applied NAMs to solving challenging\\nWinograd Schema (WS) problems. Experiments conducted on a set of WS problems\\nprove that the proposed models have the potential for commonsense reasoning.',\n",
              " 'Deep Reinforcement Learning With Macro-Actions\\nDeep reinforcement learning has been shown to be a powerful framework for\\nlearning policies from complex high-dimensional sensory inputs to actions in\\ncomplex tasks, such as the Atari domain. In this paper, we explore output\\nrepresentation modeling in the form of temporal abstraction to improve\\nconvergence and reliability of deep reinforcement learning approaches. We\\nconcentrate on macro-actions, and evaluate these on different Atari 2600 games,\\nwhere we show that they yield significant improvements in learning speed.\\nAdditionally, we show that they can even achieve better scores than DQN. We\\noffer analysis and explanation for both convergence and final results,\\nrevealing a problem deep RL approaches have with sparse reward signals.',\n",
              " 'RETAIN: An Interpretable Predictive Model for Healthcare using Reverse\\n  Time Attention Mechanism\\nAccuracy and interpretability are two dominant features of successful\\npredictive models. Typically, a choice must be made in favor of complex black\\nbox models such as recurrent neural networks (RNN) for accuracy versus less\\naccurate but more interpretable traditional models such as logistic regression.\\nThis tradeoff poses challenges in medicine where both accuracy and\\ninterpretability are important. We addressed this challenge by developing the\\nREverse Time AttentIoN model (RETAIN) for application to Electronic Health\\nRecords (EHR) data. RETAIN achieves high accuracy while remaining clinically\\ninterpretable and is based on a two-level neural attention model that detects\\ninfluential past visits and significant clinical variables within those visits\\n(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR\\ndata in a reverse time order so that recent clinical visits are likely to\\nreceive higher attention. RETAIN was tested on a large health system EHR\\ndataset with 14 million visits completed by 263K patients over an 8 year period\\nand demonstrated predictive accuracy and computational scalability comparable\\nto state-of-the-art methods such as RNN, and ease of interpretability\\ncomparable to traditional models.',\n",
              " 'A High Speed Multi-label Classifier based on Extreme Learning Machines\\nIn this paper a high speed neural network classifier based on extreme\\nlearning machines for multi-label classification problem is proposed and\\ndis-cussed. Multi-label classification is a superset of traditional binary and\\nmulti-class classification problems. The proposed work extends the extreme\\nlearning machine technique to adapt to the multi-label problems. As opposed to\\nthe single-label problem, both the number of labels the sample belongs to, and\\neach of those target labels are to be identified for multi-label classification\\nresulting in in-creased complexity. The proposed high speed multi-label\\nclassifier is applied to six benchmark datasets comprising of different\\napplication areas such as multi-media, text and biology. The training time and\\ntesting time of the classifier are compared with those of the state-of-the-arts\\nmethods. Experimental studies show that for all the six datasets, our proposed\\ntechnique have faster execution speed and better performance, thereby\\noutperforming all the existing multi-label clas-sification methods.',\n",
              " 'An Online Universal Classifier for Binary, Multi-class and Multi-label\\n  Classification\\nClassification involves the learning of the mapping function that associates\\ninput samples to corresponding target label. There are two major categories of\\nclassification problems: Single-label classification and Multi-label\\nclassification. Traditional binary and multi-class classifications are\\nsub-categories of single-label classification. Several classifiers are\\ndeveloped for binary, multi-class and multi-label classification problems, but\\nthere are no classifiers available in the literature capable of performing all\\nthree types of classification. In this paper, a novel online universal\\nclassifier capable of performing all the three types of classification is\\nproposed. Being a high speed online classifier, the proposed technique can be\\napplied to streaming data applications. The performance of the developed\\nclassifier is evaluated using datasets from binary, multi-class and multi-label\\nproblems. The results obtained are compared with state-of-the-art techniques\\nfrom each of the classification types.',\n",
              " 'Adaptive Online Sequential ELM for Concept Drift Tackling\\nA machine learning method needs to adapt to over time changes in the\\nenvironment. Such changes are known as concept drift. In this paper, we propose\\nconcept drift tackling method as an enhancement of Online Sequential Extreme\\nLearning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by\\nadding adaptive capability for classification and regression problem. The\\nscheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme\\nthat works well to handle real drift, virtual drift, and hybrid drift. The\\nAOS-ELM also works well for sudden drift and recurrent context change type. The\\nscheme is a simple unified method implemented in simple lines of code. We\\nevaluated AOS-ELM on regression and classification problem by using concept\\ndrift public data set (SEA and STAGGER) and other public data sets such as\\nMNIST, USPS, and IDS. Experiments show that our method gives higher kappa value\\ncompared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice\\ndoes not need hidden nodes increase, we address some issues related to the\\nincreasing of the hidden nodes such as error condition and rank values. We\\npropose taking the rank of the pseudoinverse matrix as an indicator parameter\\nto detect underfitting condition.',\n",
              " 'Adaptive Convolutional ELM For Concept Drift Handling in Online Stream\\n  Data\\nIn big data era, the data continuously generated and its distribution may\\nkeep changes overtime. These challenges in online stream of data are known as\\nconcept drift. In this paper, we proposed the Adaptive Convolutional ELM method\\n(ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid\\nExtreme Learning Machine (ELM) model plus adaptive capability. This method is\\naimed for concept drift handling. We enhanced the CNN as convolutional\\nhiererchical features representation learner combined with Elastic ELM\\n(E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM\\n(AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1)\\nand matrices concatenation ensembles for concept drift adaptability in ensemble\\nlevel (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works\\nwell in classifier level and ensemble level while most current methods only\\nproposed to work on either one of the levels.\\n  We verified our method in extended MNIST data set and not MNIST data set. We\\nset the experiment to simulate virtual drift, real drift, and hybrid drift\\nevent and we demonstrated how our CNNELM adaptability works. Our proposed\\nmethod works well and gives better accuracy, computation scalability, and\\nconcept drifts adaptability compared to the regular ELM and CNN. Further\\nresearches are still required to study the optimum parameters and to use more\\nvaried image data set.',\n",
              " \"Particle Swarm Optimization for Generating Interpretable Fuzzy\\n  Reinforcement Learning Policies\\nFuzzy controllers are efficient and interpretable system controllers for\\ncontinuous state and action spaces. To date, such controllers have been\\nconstructed manually or trained automatically either using expert-generated\\nproblem-specific cost functions or incorporating detailed knowledge about the\\noptimal control strategy. Both requirements for automatic training processes\\nare not found in most real-world reinforcement learning (RL) problems. In such\\napplications, online learning is often prohibited for safety reasons because\\nonline learning requires exploration of the problem's dynamics during policy\\ntraining. We introduce a fuzzy particle swarm reinforcement learning (FPSRL)\\napproach that can construct fuzzy RL policies solely by training parameters on\\nworld models that simulate real system dynamics. These world models are created\\nby employing an autonomous machine learning technique that uses previously\\ngenerated transition samples of a real system. To the best of our knowledge,\\nthis approach is the first to relate self-organizing fuzzy controllers to\\nmodel-based batch RL. Therefore, FPSRL is intended to solve problems in domains\\nwhere online learning is prohibited, system dynamics are relatively easy to\\nmodel from previously generated default policy transition samples, and it is\\nexpected that a relatively easily interpretable control policy exists. The\\nefficiency of the proposed approach with problems from such domains is\\ndemonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole\\nbalancing, and cart-pole swing-up. Our experimental results demonstrate\\nhigh-performing, interpretable fuzzy policies.\",\n",
              " 'A Growing Long-term Episodic & Semantic Memory\\nThe long-term memory of most connectionist systems lies entirely in the\\nweights of the system. Since the number of weights is typically fixed, this\\nbounds the total amount of knowledge that can be learned and stored. Though\\nthis is not normally a problem for a neural network designed for a specific\\ntask, such a bound is undesirable for a system that continually learns over an\\nopen range of domains. To address this, we describe a lifelong learning system\\nthat leverages a fast, though non-differentiable, content-addressable memory\\nwhich can be exploited to encode both a long history of sequential episodic\\nknowledge and semantic knowledge over many episodes for an unbounded number of\\ndomains. This opens the door for investigation into transfer learning, and\\nleveraging prior knowledge that has been learned over a lifetime of experiences\\nto new domains.',\n",
              " 'Cognitive Discriminative Mappings for Rapid Learning\\nHumans can learn concepts or recognize items from just a handful of examples,\\nwhile machines require many more samples to perform the same task. In this\\npaper, we build a computational model to investigate the possibility of this\\nkind of rapid learning. The proposed method aims to improve the learning task\\nof input from sensory memory by leveraging the information retrieved from\\nlong-term memory. We present a simple and intuitive technique called cognitive\\ndiscriminative mappings (CDM) to explore the cognitive problem. First, CDM\\nseparates and clusters the data instances retrieved from long-term memory into\\ndistinct classes with a discrimination method in working memory when a sensory\\ninput triggers the algorithm. CDM then maps each sensory data instance to be as\\nclose as possible to the median point of the data group with the same class.\\nThe experimental results demonstrate that the CDM approach is effective for\\nlearning the discriminative features of supervised classifications with few\\ntraining sensory input instances.',\n",
              " \"Towards a Mathematical Understanding of the Difficulty in Learning with\\n  Feedforward Neural Networks\\nTraining deep neural networks for solving machine learning problems is one\\ngreat challenge in the field, mainly due to its associated optimisation problem\\nbeing highly non-convex. Recent developments have suggested that many training\\nalgorithms do not suffer from undesired local minima under certain scenario,\\nand consequently led to great efforts in pursuing mathematical explanations for\\nsuch observations. This work provides an alternative mathematical understanding\\nof the challenge from a smooth optimisation perspective. By assuming exact\\nlearning of finite samples, sufficient conditions are identified via a critical\\npoint analysis to ensure any local minimum to be globally minimal as well.\\nFurthermore, a state of the art algorithm, known as the Generalised\\nGauss-Newton (GGN) algorithm, is rigorously revisited as an approximate\\nNewton's algorithm, which shares the property of being locally quadratically\\nconvergent to a global minimum under the condition of exact learning.\",\n",
              " 'An effective algorithm for hyperparameter optimization of neural\\n  networks\\nA major challenge in designing neural network (NN) systems is to determine\\nthe best structure and parameters for the network given the data for the\\nmachine learning problem at hand. Examples of parameters are the number of\\nlayers and nodes, the learning rates, and the dropout rates. Typically, these\\nparameters are chosen based on heuristic rules and manually fine-tuned, which\\nmay be very time-consuming, because evaluating the performance of a single\\nparametrization of the NN may require several hours. This paper addresses the\\nproblem of choosing appropriate parameters for the NN by formulating it as a\\nbox-constrained mathematical optimization problem, and applying a\\nderivative-free optimization tool that automatically and effectively searches\\nthe parameter space. The optimization tool employs a radial basis function\\nmodel of the objective function (the prediction accuracy of the NN) to\\naccelerate the discovery of configurations yielding high accuracy. Candidate\\nconfigurations explored by the algorithm are trained to a small number of\\nepochs, and only the most promising candidates receive full training. The\\nperformance of the proposed methodology is assessed on benchmark sets and in\\nthe context of predicting drug-drug interactions, showing promising results.\\nThe optimization tool used in this paper is open-source.',\n",
              " \"Evolutionary Training of Sparse Artificial Neural Networks: A Network\\n  Science Perspective\\nThrough the success of deep learning, Artificial Neural Networks (ANNs) are\\namong the most used artificial intelligence methods nowadays. ANNs have led to\\nmajor breakthroughs in various domains, such as particle physics, reinforcement\\nlearning, speech recognition, computer vision, and so on. Taking inspiration\\nfrom the network properties of biological neural networks (e.g. sparsity,\\nscale-freeness), we argue that (contrary to general practice) Artificial Neural\\nNetworks (ANN), too, should not have fully-connected layers. We show how ANNs\\nperform perfectly well with sparsely-connected layers. Following a Darwinian\\nevolutionary approach, we propose a novel algorithm which evolves an initial\\nrandom sparse topology (i.e. an Erd\\\\H{o}s-R\\\\'enyi random graph) of two\\nconsecutive layers of neurons into a scale-free topology, during the ANN\\ntraining process. The resulting sparse layers can safely replace the\\ncorresponding fully-connected layers. Our method allows to quadratically reduce\\nthe number of parameters in the fully conencted layers of ANNs, yielding\\nquadratically faster computational times in both phases (i.e. training and\\ninference), at no decrease in accuracy. We demonstrate our claims on two\\npopular ANN types (restricted Boltzmann machine and multi-layer perceptron), on\\ntwo types of tasks (supervised and unsupervised learning), and on 14 benchmark\\ndatasets. We anticipate that our approach will enable ANNs having billions of\\nneurons and evolved topologies to be capable of handling complex real-world\\ntasks that are intractable using state-of-the-art methods.\",\n",
              " 'Attend and Predict: Understanding Gene Regulation by Selective Attention\\n  on Chromatin\\nThe past decade has seen a revolution in genomic technologies that enable a\\nflood of genome-wide profiling of chromatin marks. Recent literature tried to\\nunderstand gene regulation by predicting gene expression from large-scale\\nchromatin measurements. Two fundamental challenges exist for such learning\\ntasks: (1) genome-wide chromatin signals are spatially structured,\\nhigh-dimensional and highly modular; and (2) the core aim is to understand what\\nare the relevant factors and how they work together? Previous studies either\\nfailed to model complex dependencies among input signals or relied on separate\\nfeature analysis to explain the decisions. This paper presents an\\nattention-based deep learning approach; we call AttentiveChrome, that uses a\\nunified architecture to model and to interpret dependencies among chromatin\\nfactors for controlling gene regulation. AttentiveChrome uses a hierarchy of\\nmultiple Long short-term memory (LSTM) modules to encode the input signals and\\nto model how various chromatin marks cooperate automatically. AttentiveChrome\\ntrains two levels of attention jointly with the target prediction, enabling it\\nto attend differentially to relevant marks and to locate important positions\\nper mark. We evaluate the model across 56 different cell types (tasks) in\\nhuman. Not only is the proposed architecture more accurate, but its attention\\nscores also provide a better interpretation than state-of-the-art feature\\nvisualization methods such as saliency map.\\n  Code and data are shared at www.deepchrome.org',\n",
              " 'Parallelizing Linear Recurrent Neural Nets Over Sequence Length\\nRecurrent neural networks (RNNs) are widely used to model sequential data but\\ntheir non-linear dependencies between sequence elements prevent parallelizing\\ntraining over sequence length. We show the training of RNNs with only linear\\nsequential dependencies can be parallelized over the sequence length using the\\nparallel scan algorithm, leading to rapid training on long sequences even with\\nsmall minibatch size. We develop a parallel linear recurrence CUDA kernel and\\nshow that it can be applied to immediately speed up training and inference of\\nseveral state of the art RNN architectures by up to 9x. We abstract recent work\\non linear RNNs into a new framework of linear surrogate RNNs and develop a\\nlinear surrogate model for the long short-term memory unit, the GILR-LSTM, that\\nutilizes parallel linear recurrence. We extend sequence learning to new\\nextremely long sequence regimes that were previously out of reach by\\nsuccessfully training a GILR-LSTM on a synthetic sequence classification task\\nwith a one million timestep dependency.',\n",
              " 'Feature learning in feature-sample networks using multi-objective\\n  optimization\\nData and knowledge representation are fundamental concepts in machine\\nlearning. The quality of the representation impacts the performance of the\\nlearning model directly. Feature learning transforms or enhances raw data to\\nstructures that are effectively exploited by those models. In recent years,\\nseveral works have been using complex networks for data representation and\\nanalysis. However, no feature learning method has been proposed for such\\ncategory of techniques. Here, we present an unsupervised feature learning\\nmechanism that works on datasets with binary features. First, the dataset is\\nmapped into a feature--sample network. Then, a multi-objective optimization\\nprocess selects a set of new vertices to produce an enhanced version of the\\nnetwork. The new features depend on a nonlinear function of a combination of\\npreexisting features. Effectively, the process projects the input data into a\\nhigher-dimensional space. To solve the optimization problem, we design two\\nmetaheuristics based on the lexicographic genetic algorithm and the improved\\nstrength Pareto evolutionary algorithm (SPEA2). We show that the enhanced\\nnetwork contains more information and can be exploited to improve the\\nperformance of machine learning methods. The advantages and disadvantages of\\neach optimization strategy are discussed.',\n",
              " 'Meta-Learning and Universality: Deep Representations and Gradient\\n  Descent can Approximate any Learning Algorithm\\nLearning to learn is a powerful paradigm for enabling models to learn from\\ndata more effectively and efficiently. A popular approach to meta-learning is\\nto train a recurrent model to read in a training dataset as input and output\\nthe parameters of a learned model, or output predictions for new test inputs.\\nAlternatively, a more recent approach to meta-learning aims to acquire deep\\nrepresentations that can be effectively fine-tuned, via standard gradient\\ndescent, to new tasks. In this paper, we consider the meta-learning problem\\nfrom the perspective of universality, formalizing the notion of learning\\nalgorithm approximation and comparing the expressive power of the\\naforementioned recurrent models to the more recent approaches that embed\\ngradient descent into the meta-learner. In particular, we seek to answer the\\nfollowing question: does deep representation combined with standard gradient\\ndescent have sufficient capacity to approximate any learning algorithm? We find\\nthat this is indeed true, and further find, in our experiments, that\\ngradient-based meta-learning consistently leads to learning strategies that\\ngeneralize more widely compared to those represented by recurrent models.',\n",
              " 'Hindsight policy gradients\\nGoal-conditional policies allow reinforcement learning agents to pursue\\nspecific goals during different episodes. In addition to their potential to\\ngeneralize desired behavior to unseen goals, such policies may also help in\\ndefining options for arbitrary subgoals, enabling higher-level planning. While\\ntrying to achieve a specific goal, an agent may also be able to exploit\\ninformation about the degree to which it has achieved alternative goals.\\nReinforcement learning agents have only recently been endowed with such\\ncapacity for hindsight, which is highly valuable in environments with sparse\\nrewards. In this paper, we show how hindsight can be introduced to\\nlikelihood-ratio policy gradient methods, generalizing this capacity to an\\nentire class of highly successful algorithms. Our preliminary experiments\\nsuggest that hindsight may increase the sample efficiency of policy gradient\\nmethods.',\n",
              " 'SquishedNets: Squishing SqueezeNet further for edge device scenarios via\\n  deep evolutionary synthesis\\nWhile deep neural networks have been shown in recent years to outperform\\nother machine learning methods in a wide range of applications, one of the\\nbiggest challenges with enabling deep neural networks for widespread deployment\\non edge devices such as mobile and other consumer devices is high computational\\nand memory requirements. Recently, there has been greater exploration into\\nsmall deep neural network architectures that are more suitable for edge\\ndevices, with one of the most popular architectures being SqueezeNet, with an\\nincredibly small model size of 4.8MB. Taking further advantage of the notion\\nthat many applications of machine learning on edge devices are often\\ncharacterized by a low number of target classes, this study explores the\\nutility of combining architectural modifications and an evolutionary synthesis\\nstrategy for synthesizing even smaller deep neural architectures based on the\\nmore recent SqueezeNet v1.1 macroarchitecture for applications with fewer\\ntarget classes. In particular, architectural modifications are first made to\\nSqueezeNet v1.1 to accommodate for a 10-class ImageNet-10 dataset, and then an\\nevolutionary synthesis strategy is leveraged to synthesize more efficient deep\\nneural networks based on this modified macroarchitecture. The resulting\\nSquishedNets possess model sizes ranging from 2.4MB to 0.95MB (~5.17X smaller\\nthan SqueezeNet v1.1, or 253X smaller than AlexNet). Furthermore, the\\nSquishedNets are still able to achieve accuracies ranging from 81.2% to 77%,\\nand able to process at speeds of 156 images/sec to as much as 256 images/sec on\\na Nvidia Jetson TX1 embedded chip. These preliminary results show that a\\ncombination of architectural modifications and an evolutionary synthesis\\nstrategy can be a useful tool for producing very small deep neural network\\narchitectures that are well-suited for edge device scenarios.',\n",
              " 'Autonomous development and learning in artificial intelligence and\\n  robotics: Scaling up deep learning to human--like learning\\nAutonomous lifelong development and learning is a fundamental capability of\\nhumans, differentiating them from current deep learning systems. However, other\\nbranches of artificial intelligence have designed crucial ingredients towards\\nautonomous learning: curiosity and intrinsic motivation, social learning and\\nnatural interaction with peers, and embodiment. These mechanisms guide\\nexploration and autonomous choice of goals, and integrating them with deep\\nlearning opens stimulating perspectives. Deep learning (DL) approaches made\\ngreat advances in artificial intelligence, but are still far away from human\\nlearning. As argued convincingly by Lake et al., differences include human\\ncapabilities to learn causal models of the world from very little data,\\nleveraging compositional representations and priors like intuitive physics and\\npsychology. However, there are other fundamental differences between current DL\\nsystems and human learning, as well as technical ingredients to fill this gap,\\nthat are either superficially, or not adequately, discussed by Lake et al.\\nThese fundamental mechanisms relate to autonomous development and learning.\\nThey are bound to play a central role in artificial intelligence in the future.\\nCurrent DL systems require engineers to manually specify a task-specific\\nobjective function for every new task, and learn through off-line processing of\\nlarge training databases. On the contrary, humans learn autonomously open-ended\\nrepertoires of skills, deciding for themselves which goals to pursue or value,\\nand which skills to explore, driven by intrinsic motivation/curiosity and\\nsocial learning through natural interaction with peers. Such learning processes\\nare incremental, online, and progressive. Human child development involves a\\nprogressive increase of complexity in a curriculum of learning where skills are\\nexplored, acquired, and built on each other, through particular ordering and\\ntiming. Finally, human learning happens in the physical world, and through\\nbodily and physical experimentation, under severe constraints on energy, time,\\nand computational resources. In the two last decades, the field of\\nDevelopmental and Cognitive Robotics (Cangelosi and Schlesinger, 2015, Asada et\\nal., 2009), in strong interaction with developmental psychology and\\nneuroscience, has achieved significant advances in computational',\n",
              " 'Learning from Scarce Experience\\nSearching the space of policies directly for the optimal policy has been one\\npopular method for solving partially observable reinforcement learning\\nproblems. Typically, with each change of the target policy, its value is\\nestimated from the results of following that very policy. This requires a large\\nnumber of interactions with the environment as different polices are\\nconsidered. We present a family of algorithms based on likelihood ratio\\nestimation that use data gathered when executing one policy (or collection of\\npolicies) to estimate the value of a different policy. The algorithms combine\\nestimation and optimization stages. The former utilizes experience to build a\\nnon-parametric representation of an optimized function. The latter performs\\noptimization on this estimate. We show positive empirical results and provide\\nthe sample complexity bound.',\n",
              " 'Fitness inheritance in the Bayesian optimization algorithm\\nThis paper describes how fitness inheritance can be used to estimate fitness\\nfor a proportion of newly sampled candidate solutions in the Bayesian\\noptimization algorithm (BOA). The goal of estimating fitness for some candidate\\nsolutions is to reduce the number of fitness evaluations for problems where\\nfitness evaluation is expensive. Bayesian networks used in BOA to model\\npromising solutions and generate the new ones are extended to allow not only\\nfor modeling and sampling candidate solutions, but also for estimating their\\nfitness. The results indicate that fitness inheritance is a promising concept\\nin BOA, because population-sizing requirements for building appropriate models\\nof promising solutions lead to good fitness estimates even if only a small\\nproportion of candidate solutions is evaluated using the actual fitness\\nfunction. This can lead to a reduction of the number of actual fitness\\nevaluations by a factor of 30 or more.',\n",
              " 'The Combined Technique for Detection of Artifacts in Clinical\\n  Electroencephalograms of Sleeping Newborns\\nIn this paper we describe a new method combining the polynomial neural\\nnetwork and decision tree techniques in order to derive comprehensible\\nclassification rules from clinical electroencephalograms (EEGs) recorded from\\nsleeping newborns. These EEGs are heavily corrupted by cardiac, eye movement,\\nmuscle and noise artifacts and as a consequence some EEG features are\\nirrelevant to classification problems. Combining the polynomial network and\\ndecision tree techniques, we discover comprehensible classification rules\\nwhilst also attempting to keep their classification error down. This technique\\nis shown to outperform a number of commonly used machine learning technique\\napplied to automatically recognize artifacts in the sleep EEGs.',\n",
              " 'Evolving Classifiers: Methods for Incremental Learning\\nThe ability of a classifier to take on new information and classes by\\nevolving the classifier without it having to be fully retrained is known as\\nincremental learning. Incremental learning has been successfully applied to\\nmany classification problems, where the data is changing and is not all\\navailable at once. In this paper there is a comparison between Learn++, which\\nis one of the most recent incremental learning algorithms, and the new proposed\\nmethod of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has\\nshown good incremental learning capabilities on benchmark datasets on which the\\nnew ILUGA method has been tested. ILUGA has also shown good incremental\\nlearning ability using only a few classifiers and does not suffer from\\ncatastrophic forgetting. The results obtained for ILUGA on the Optical\\nCharacter Recognition (OCR) and Wine datasets are good, with an overall\\naccuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT\\nfor the difficult multi-class OCR dataset.',\n",
              " \"Automatic Pattern Classification by Unsupervised Learning Using\\n  Dimensionality Reduction of Data with Mirroring Neural Networks\\nThis paper proposes an unsupervised learning technique by using Multi-layer\\nMirroring Neural Network and Forgy's clustering algorithm. Multi-layer\\nMirroring Neural Network is a neural network that can be trained with\\ngeneralized data inputs (different categories of image patterns) to perform\\nnon-linear dimensionality reduction and the resultant low-dimensional code is\\nused for unsupervised pattern classification using Forgy's algorithm. By\\nadapting the non-linear activation function (modified sigmoidal function) and\\ninitializing the weights and bias terms to small random values, mirroring of\\nthe input pattern is initiated. In training, the weights and bias terms are\\nchanged in such a way that the input presented is reproduced at the output by\\nback propagating the error. The mirroring neural network is capable of reducing\\nthe input vector to a great degree (approximately 1/30th the original size) and\\nalso able to reconstruct the input pattern at the output layer from this\\nreduced code units. The feature set (output of central hidden layer) extracted\\nfrom this network is fed to Forgy's algorithm, which classify input data\\npatterns into distinguishable classes. In the implementation of Forgy's\\nalgorithm, initial seed points are selected in such a way that they are distant\\nenough to be perfectly grouped into different categories. Thus a new method of\\nunsupervised learning is formulated and demonstrated in this paper. This method\\ngave impressive results when applied to classification of different image\\npatterns.\",\n",
              " 'Improving the Performance of PieceWise Linear Separation Incremental\\n  Algorithms for Practical Hardware Implementations\\nIn this paper we shall review the common problems associated with Piecewise\\nLinear Separation incremental algorithms. This kind of neural models yield poor\\nperformances when dealing with some classification problems, due to the\\nevolving schemes used to construct the resulting networks. So as to avoid this\\nundesirable behavior we shall propose a modification criterion. It is based\\nupon the definition of a function which will provide information about the\\nquality of the network growth process during the learning phase. This function\\nis evaluated periodically as the network structure evolves, and will permit, as\\nwe shall show through exhaustive benchmarks, to considerably improve the\\nperformance(measured in terms of network complexity and generalization\\ncapabilities) offered by the networks generated by these incremental models.',\n",
              " 'A Novel Rough Set Reduct Algorithm for Medical Domain Based on Bee\\n  Colony Optimization\\nFeature selection refers to the problem of selecting relevant features which\\nproduce the most predictive outcome. In particular, feature selection task is\\ninvolved in datasets containing huge number of features. Rough set theory has\\nbeen one of the most successful methods used for feature selection. However,\\nthis method is still not able to find optimal subsets. This paper proposes a\\nnew feature selection method based on Rough set theory hybrid with Bee Colony\\nOptimization (BCO) in an attempt to combat this. This proposed work is applied\\nin the medical domain to find the minimal reducts and experimentally compared\\nwith the Quick Reduct, Entropy Based Reduct, and other hybrid Rough Set methods\\nsuch as Genetic Algorithm (GA), Ant Colony Optimization (ACO) and Particle\\nSwarm Optimization (PSO).',\n",
              " 'Automated Query Learning with Wikipedia and Genetic Programming\\nMost of the existing information retrieval systems are based on bag of words\\nmodel and are not equipped with common world knowledge. Work has been done\\ntowards improving the efficiency of such systems by using intelligent\\nalgorithms to generate search queries, however, not much research has been done\\nin the direction of incorporating human-and-society level knowledge in the\\nqueries. This paper is one of the first attempts where such information is\\nincorporated into the search queries using Wikipedia semantics. The paper\\npresents an essential shift from conventional token based queries to concept\\nbased queries, leading to an enhanced efficiency of information retrieval\\nsystems. To efficiently handle the automated query learning problem, we propose\\nWikipedia-based Evolutionary Semantics (Wiki-ES) framework where concept based\\nqueries are learnt using a co-evolving evolutionary procedure. Learning concept\\nbased queries using an intelligent evolutionary procedure yields significant\\nimprovement in performance which is shown through an extensive study using\\nReuters newswire documents. Comparison of the proposed framework is performed\\nwith other information retrieval systems. Concept based approach has also been\\nimplemented on other information retrieval systems to justify the effectiveness\\nof a transition from token based queries to concept based queries.',\n",
              " \"Scaling Up Estimation of Distribution Algorithms For Continuous\\n  Optimization\\nSince Estimation of Distribution Algorithms (EDA) were proposed, many\\nattempts have been made to improve EDAs' performance in the context of global\\noptimization. So far, the studies or applications of multivariate probabilistic\\nmodel based continuous EDAs are still restricted to rather low dimensional\\nproblems (smaller than 100D). Traditional EDAs have difficulties in solving\\nhigher dimensional problems because of the curse of dimensionality and their\\nrapidly increasing computational cost. However, scaling up continuous EDAs for\\nhigher dimensional optimization is still necessary, which is supported by the\\ndistinctive feature of EDAs: Because a probabilistic model is explicitly\\nestimated, from the learnt model one can discover useful properties or features\\nof the problem. Besides obtaining a good solution, understanding of the problem\\nstructure can be of great benefit, especially for black box optimization. We\\npropose a novel EDA framework with Model Complexity Control (EDA-MCC) to scale\\nup EDAs. By using Weakly dependent variable Identification (WI) and Subspace\\nModeling (SM), EDA-MCC shows significantly better performance than traditional\\nEDAs on high dimensional problems. Moreover, the computational cost and the\\nrequirement of large population sizes can be reduced in EDA-MCC. In addition to\\nbeing able to find a good solution, EDA-MCC can also produce a useful problem\\nstructure characterization. EDA-MCC is the first successful instance of\\nmultivariate model based EDAs that can be effectively applied a general class\\nof up to 500D problems. It also outperforms some newly developed algorithms\\ndesigned specifically for large scale optimization. In order to understand the\\nstrength and weakness of EDA-MCC, we have carried out extensive computational\\nstudies of EDA-MCC. Our results have revealed when EDA-MCC is likely to\\noutperform others on what kind of benchmark functions.\",\n",
              " 'Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA\\nAn automated technique has recently been proposed to transfer learning in the\\nhierarchical Bayesian optimization algorithm (hBOA) based on distance-based\\nstatistics. The technique enables practitioners to improve hBOA efficiency by\\ncollecting statistics from probabilistic models obtained in previous hBOA runs\\nand using the obtained statistics to bias future hBOA runs on similar problems.\\nThe purpose of this paper is threefold: (1) test the technique on several\\nclasses of NP-complete problems, including MAXSAT, spin glasses and minimum\\nvertex cover; (2) demonstrate that the technique is effective even when\\nprevious runs were done on problems of different size; (3) provide empirical\\nevidence that combining transfer learning with other efficiency enhancement\\ntechniques can often yield nearly multiplicative speedups.',\n",
              " 'Discrete Dynamical Genetic Programming in XCS\\nA number of representation schemes have been presented for use within\\nLearning Classifier Systems, ranging from binary encodings to neural networks.\\nThis paper presents results from an investigation into using a discrete\\ndynamical system representation within the XCS Learning Classifier System. In\\nparticular, asynchronous random Boolean networks are used to represent the\\ntraditional condition-action production system rules. It is shown possible to\\nuse self-adaptive, open-ended evolution to design an ensemble of such discrete\\ndynamical systems within XCS to solve a number of well-known test problems.',\n",
              " 'Fuzzy Dynamical Genetic Programming in XCSF\\nA number of representation schemes have been presented for use within\\nLearning Classifier Systems, ranging from binary encodings to Neural Networks,\\nand more recently Dynamical Genetic Programming (DGP). This paper presents\\nresults from an investigation into using a fuzzy DGP representation within the\\nXCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic\\nNetworks are used to represent the traditional condition-action production\\nsystem rules. It is shown possible to use self-adaptive, open-ended evolution\\nto design an ensemble of such fuzzy dynamical systems within XCSF to solve\\nseveral well-known continuous-valued test problems.',\n",
              " 'Learning-Based Procedural Content Generation\\nProcedural content generation (PCG) has recently become one of the hottest\\ntopics in computational intelligence and AI game researches. Among a variety of\\nPCG techniques, search-based approaches overwhelmingly dominate PCG development\\nat present. While SBPCG leads to promising results and successful applications,\\nit poses a number of challenges ranging from representation to evaluation of\\nthe content being generated. In this paper, we present an alternative yet\\ngeneric PCG framework, named learning-based procedure content generation\\n(LBPCG), to provide potential solutions to several challenging problems in\\nexisting PCG techniques. By exploring and exploiting information gained in game\\ndevelopment and public beta test via data-driven learning, our framework can\\ngenerate robust content adaptable to end-user or target players on-line with\\nminimal interruption to their experience. Furthermore, we develop enabling\\ntechniques to implement the various models required in our framework. For a\\nproof of concept, we have developed a prototype based on the classic open\\nsource first-person shooter game, Quake. Simulation results suggest that our\\nframework is promising in generating quality content.',\n",
              " 'Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in\\n  the Othello League\\nN-tuple networks have been successfully used as position evaluation functions\\nfor board games such as Othello or Connect Four. The effectiveness of such\\nnetworks depends on their architecture, which is determined by the placement of\\nconstituent n-tuples, sequences of board locations, providing input to the\\nnetwork. The most popular method of placing n-tuples consists in randomly\\ngenerating a small number of long, snake-shaped board location sequences. In\\ncomparison, we show that learning n-tuple networks is significantly more\\neffective if they involve a large number of systematically placed, short,\\nstraight n-tuples. Moreover, we demonstrate that in order to obtain the best\\nperformance and the steepest learning curve for Othello it is enough to use\\nn-tuples of size just 2, yielding a network consisting of only 288 weights. The\\nbest such network evolved in this study has been evaluated in the online\\nOthello League, obtaining the performance of nearly 96% --- more than any other\\nplayer to date.',\n",
              " 'Towards a Self-Organized Agent-Based Simulation Model for Exploration of\\n  Human Synaptic Connections\\nIn this paper, the early design of our self-organized agent-based simulation\\nmodel for exploration of synaptic connections that faithfully generates what is\\nobserved in natural situation is given. While we take inspiration from\\nneuroscience, our intent is not to create a veridical model of processes in\\nneurodevelopmental biology, nor to represent a real biological system. Instead,\\nour goal is to design a simulation model that learns acting in the same way of\\nhuman nervous system by using findings on human subjects using reflex\\nmethodologies in order to estimate unknown connections.',\n",
              " 'Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural\\n  Network\\nThe paper presents the electronic design and motion planning of a robot based\\non decision making regarding its straight motion and precise turn using\\nArtificial Neural Network (ANN). The ANN helps in learning of robot so that it\\nperforms motion autonomously. The weights calculated are implemented in\\nmicrocontroller. The performance has been tested to be excellent.',\n",
              " 'Learning Bayesian Network Equivalence Classes with Ant Colony\\n  Optimization\\nBayesian networks are a useful tool in the representation of uncertain\\nknowledge. This paper proposes a new algorithm called ACO-E, to learn the\\nstructure of a Bayesian network. It does this by conducting a search through\\nthe space of equivalence classes of Bayesian networks using Ant Colony\\nOptimization (ACO). To this end, two novel extensions of traditional ACO\\ntechniques are proposed and implemented. Firstly, multiple types of moves are\\nallowed. Secondly, moves can be given in terms of indices that are not based on\\nconstruction graph nodes. The results of testing show that ACO-E performs\\nbetter than a greedy search and other state-of-the-art and metaheuristic\\nalgorithms whilst searching in the space of equivalence classes.',\n",
              " 'Probabilistic Neural Programs\\nWe present probabilistic neural programs, a framework for program induction\\nthat permits flexible specification of both a computational model and inference\\nalgorithm while simultaneously enabling the use of deep neural networks.\\nProbabilistic neural programs combine a computation graph for specifying a\\nneural network with an operator for weighted nondeterministic choice. Thus, a\\nprogram describes both a collection of decisions as well as the neural network\\narchitecture used to make each one. We evaluate our approach on a challenging\\ndiagram question answering task where probabilistic neural programs correctly\\nexecute nearly twice as many programs as a baseline model.',\n",
              " \"Cognitive Deep Machine Can Train Itself\\nMachine learning is making substantial progress in diverse applications. The\\nsuccess is mostly due to advances in deep learning. However, deep learning can\\nmake mistakes and its generalization abilities to new tasks are questionable.\\nWe ask when and how one can combine network outputs, when (i) details of the\\nobservations are evaluated by learned deep components and (ii) facts and\\nconfirmation rules are available in knowledge based systems. We show that in\\nlimited contexts the required number of training samples can be low and\\nself-improvement of pre-trained networks in more general context is possible.\\nWe argue that the combination of sparse outlier detection with deep components\\nthat can support each other diminish the fragility of deep methods, an\\nimportant requirement for engineering applications. We argue that supervised\\nlearning of labels may be fully eliminated under certain conditions: a\\ncomponent based architecture together with a knowledge based system can train\\nitself and provide high quality answers. We demonstrate these concepts on the\\nState Farm Distracted Driver Detection benchmark. We argue that the view of the\\nStudy Panel (2016) may overestimate the requirements on `years of focused\\nresearch' and `careful, unique construction' for `AI systems'.\",\n",
              " 'Summary - TerpreT: A Probabilistic Programming Language for Program\\n  Induction\\nWe study machine learning formulations of inductive program synthesis; that\\nis, given input-output examples, synthesize source code that maps inputs to\\ncorresponding outputs. Our key contribution is TerpreT, a domain-specific\\nlanguage for expressing program synthesis problems. A TerpreT model is composed\\nof a specification of a program representation and an interpreter that\\ndescribes how programs map inputs to outputs. The inference task is to observe\\na set of input-output examples and infer the underlying program. From a TerpreT\\nmodel we automatically perform inference using four different back-ends:\\ngradient descent (thus each TerpreT model can be seen as defining a\\ndifferentiable interpreter), linear program (LP) relaxations for graphical\\nmodels, discrete satisfiability solving, and the Sketch program synthesis\\nsystem. TerpreT has two main benefits. First, it enables rapid exploration of a\\nrange of domains, program representations, and interpreter models. Second, it\\nseparates the model specification from the inference algorithm, allowing proper\\ncomparisons between different approaches to inference.\\n  We illustrate the value of TerpreT by developing several interpreter models\\nand performing an extensive empirical comparison between alternative inference\\nalgorithms on a variety of program models. To our knowledge, this is the first\\nwork to compare gradient-based search over program space to traditional\\nsearch-based alternatives. Our key empirical finding is that constraint solvers\\ndominate the gradient descent and LP-based formulations.\\n  This is a workshop summary of a longer report at arXiv:1608.04428',\n",
              " 'Learning in the Machine: Random Backpropagation and the Deep Learning\\n  Channel\\nRandom backpropagation (RBP) is a variant of the backpropagation algorithm\\nfor training neural networks, where the transpose of the forward matrices are\\nreplaced by fixed random matrices in the calculation of the weight updates. It\\nis remarkable both because of its effectiveness, in spite of using random\\nmatrices to communicate error information, and because it completely removes\\nthe taxing requirement of maintaining symmetric weights in a physical neural\\nsystem. To better understand random backpropagation, we first connect it to the\\nnotions of local learning and learning channels. Through this connection, we\\nderive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP\\n(ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their\\ncomputational complexity. We then study their behavior through simulations\\nusing the MNIST and CIFAR-10 bechnmark datasets. These simulations show that\\nmost of these variants work robustly, almost as well as backpropagation, and\\nthat multiplication by the derivatives of the activation functions is\\nimportant. As a follow-up, we study also the low-end of the number of bits\\nrequired to communicate error information over the learning channel. We then\\nprovide partial intuitive explanations for some of the remarkable properties of\\nRBP and its variations. Finally, we prove several mathematical results,\\nincluding the convergence to fixed points of linear chains of arbitrary length,\\nthe convergence to fixed points of linear autoencoders with decorrelated data,\\nthe long-term existence of solutions for linear systems with a single hidden\\nlayer and convergence in special cases, and the convergence to fixed points of\\nnon-linear chains, when the derivative of the activation functions is included.',\n",
              " 'Highway and Residual Networks learn Unrolled Iterative Estimation\\nThe past year saw the introduction of new architectures such as Highway\\nnetworks and Residual networks which, for the first time, enabled the training\\nof feedforward networks with dozens to hundreds of layers using simple gradient\\ndescent. While depth of representation has been posited as a primary reason for\\ntheir success, there are indications that these architectures defy a popular\\nview of deep learning as a hierarchical computation of increasingly abstract\\nfeatures at each layer.\\n  In this report, we argue that this view is incomplete and does not adequately\\nexplain several recent findings. We propose an alternative viewpoint based on\\nunrolled iterative estimation -- a group of successive layers iteratively\\nrefine their estimates of the same features instead of computing an entirely\\nnew representation. We demonstrate that this viewpoint directly leads to the\\nconstruction of Highway and Residual networks. Finally we provide preliminary\\nexperiments to discuss the similarities and differences between the two\\narchitectures.',\n",
              " 'Deep neural heart rate variability analysis\\nDespite of the pain and limited accuracy of blood tests for early recognition\\nof cardiovascular disease, they dominate risk screening and triage. On the\\nother hand, heart rate variability is non-invasive and cheap, but not\\nconsidered accurate enough for clinical practice. Here, we tackle heart beat\\ninterval based classification with deep learning. We introduce an end to end\\ndifferentiable hybrid architecture, consisting of a layer of biological neuron\\nmodels of cardiac dynamics (modified FitzHugh Nagumo neurons) and several\\nlayers of a standard feed-forward neural network. The proposed model is\\nevaluated on ECGs from 474 stable at-risk (coronary artery disease) patients,\\nand 1172 chest pain patients of an emergency department. We show that it can\\nsignificantly outperform models based on traditional heart rate variability\\npredictors, as well as approaching or in some cases outperforming clinical\\nblood tests, based only on 60 seconds of inter-beat intervals.',\n",
              " 'A neural network approach to ordinal regression\\nOrdinal regression is an important type of learning, which has properties of\\nboth classification and regression. Here we describe a simple and effective\\napproach to adapt a traditional neural network to learn ordinal categories. Our\\napproach is a generalization of the perceptron method for ordinal regression.\\nOn several benchmark datasets, our method (NNRank) outperforms a neural network\\nclassification method. Compared with the ordinal regression methods using\\nGaussian processes and support vector machines, NNRank achieves comparable\\nperformance. Moreover, NNRank has the advantages of traditional neural\\nnetworks: learning in both online and batch modes, handling very large training\\ndatasets, and making rapid predictions. These features make NNRank a useful and\\ncomplementary tool for large-scale data processing tasks such as information\\nretrieval, web page ranking, collaborative filtering, and protein ranking in\\nBioinformatics.',\n",
              " 'Computational Model of Music Sight Reading: A Reinforcement Learning\\n  Approach\\nAlthough the Music Sight Reading process has been studied from the cognitive\\npsychology view points, but the computational learning methods like the\\nReinforcement Learning have not yet been used to modeling of such processes. In\\nthis paper, with regards to essential properties of our specific problem, we\\nconsider the value function concept and will indicate that the optimum policy\\ncan be obtained by the method we offer without to be getting involved with\\ncomputing of the complex value functions. Also, we will offer a normative\\nbehavioral model for the interaction of the agent with the musical pitch\\nenvironment and by using a slightly different version of Partially observable\\nMarkov decision processes we will show that our method helps for faster\\nlearning of state-action pairs in our implemented agents.',\n",
              " \"Using Artificial Bee Colony Algorithm for MLP Training on Earthquake\\n  Time Series Data Prediction\\nNowadays, computer scientists have shown the interest in the study of social\\ninsect's behaviour in neural networks area for solving different combinatorial\\nand statistical problems. Chief among these is the Artificial Bee Colony (ABC)\\nalgorithm. This paper investigates the use of ABC algorithm that simulates the\\nintelligent foraging behaviour of a honey bee swarm. Multilayer Perceptron\\n(MLP) trained with the standard back propagation algorithm normally utilises\\ncomputationally intensive training algorithms. One of the crucial problems with\\nthe backpropagation (BP) algorithm is that it can sometimes yield the networks\\nwith suboptimal weights because of the presence of many local optima in the\\nsolution space. To overcome ABC algorithm used in this work to train MLP\\nlearning the complex behaviour of earthquake time series data trained by BP,\\nthe performance of MLP-ABC is benchmarked against MLP training with the\\nstandard BP. The experimental result shows that MLP-ABC performance is better\\nthan MLP-BP for time series data.\",\n",
              " \"Multiple chaotic central pattern generators with learning for legged\\n  locomotion and malfunction compensation\\nAn originally chaotic system can be controlled into various periodic\\ndynamics. When it is implemented into a legged robot's locomotion control as a\\ncentral pattern generator (CPG), sophisticated gait patterns arise so that the\\nrobot can perform various walking behaviors. However, such a single chaotic CPG\\ncontroller has difficulties dealing with leg malfunction. Specifically, in the\\nscenarios presented here, its movement permanently deviates from the desired\\ntrajectory. To address this problem, we extend the single chaotic CPG to\\nmultiple CPGs with learning. The learning mechanism is based on a simulated\\nannealing algorithm. In a normal situation, the CPGs synchronize and their\\ndynamics are identical. With leg malfunction or disability, the CPGs lose\\nsynchronization leading to independent dynamics. In this case, the learning\\nmechanism is applied to automatically adjust the remaining legs' oscillation\\nfrequencies so that the robot adapts its locomotion to deal with the\\nmalfunction. As a consequence, the trajectory produced by the multiple chaotic\\nCPGs resembles the original trajectory far better than the one produced by only\\na single CPG. The performance of the system is evaluated first in a physical\\nsimulation of a quadruped as well as a hexapod robot and finally in a real\\nsix-legged walking machine called AMOSII. The experimental results presented\\nhere reveal that using multiple CPGs with learning is an effective approach for\\nadaptive locomotion generation where, for instance, different body parts have\\nto perform independent movements for malfunction compensation.\",\n",
              " \"Teaching Deep Convolutional Neural Networks to Play Go\\nMastering the game of Go has remained a long standing challenge to the field\\nof AI. Modern computer Go systems rely on processing millions of possible\\nfuture positions to play well, but intuitively a stronger and more 'humanlike'\\nway to play the game would be to rely on pattern recognition abilities rather\\nthen brute force computation. Following this sentiment, we train deep\\nconvolutional neural networks to play Go by training them to predict the moves\\nmade by expert Go players. To solve this problem we introduce a number of novel\\ntechniques, including a method of tying weights in the network to 'hard code'\\nsymmetries that are expect to exist in the target function, and demonstrate in\\nan ablation study they considerably improve performance. Our final networks are\\nable to achieve move prediction accuracies of 41.1% and 44.4% on two different\\nGo datasets, surpassing previous state of the art on this task by significant\\nmargins. Additionally, while previous move prediction programs have not yielded\\nstrong Go playing programs, we show that the networks trained in this work\\nacquired high levels of skill. Our convolutional neural networks can\\nconsistently defeat the well known Go program GNU Go, indicating it is state of\\nthe art among programs that do not use Monte Carlo Tree Search. It is also able\\nto win some games against state of the art Go playing program Fuego while using\\na fraction of the play time. This success at playing Go indicates high level\\nprinciples of the game were learned.\",\n",
              " 'Polyphonic Music Generation by Modeling Temporal Dependencies Using a\\n  RNN-DBN\\nIn this paper, we propose a generic technique to model temporal dependencies\\nand sequences using a combination of a recurrent neural network and a Deep\\nBelief Network. Our technique, RNN-DBN, is an amalgamation of the memory state\\nof the RNN that allows it to provide temporal information and a multi-layer DBN\\nthat helps in high level representation of the data. This makes RNN-DBNs ideal\\nfor sequence generation. Further, the use of a DBN in conjunction with the RNN\\nmakes this model capable of significantly more complex data representation than\\nan RBM. We apply this technique to the task of polyphonic music generation.',\n",
              " 'Massively Parallel Methods for Deep Reinforcement Learning\\nWe present the first massively distributed architecture for deep\\nreinforcement learning. This architecture uses four main components: parallel\\nactors that generate new behaviour; parallel learners that are trained from\\nstored experience; a distributed neural network to represent the value function\\nor behaviour policy; and a distributed store of experience. We used our\\narchitecture to implement the Deep Q-Network algorithm (DQN). Our distributed\\nalgorithm was applied to 49 games from Atari 2600 games from the Arcade\\nLearning Environment, using identical hyperparameters. Our performance\\nsurpassed non-distributed DQN in 41 of the 49 games and also reduced the\\nwall-time required to achieve these results by an order of magnitude on most\\ngames.',\n",
              " 'A genetic algorithm for autonomous navigation in partially observable\\n  domain\\nThe problem of autonomous navigation is one of the basic problems for\\nrobotics. Although, in general, it may be challenging when an autonomous\\nvehicle is placed into partially observable domain. In this paper we consider\\nsimplistic environment model and introduce a navigation algorithm based on\\nLearning Classifier System.',\n",
              " 'Distributed Deep Q-Learning\\nWe propose a distributed deep learning model to successfully learn control\\npolicies directly from high-dimensional sensory input using reinforcement\\nlearning. The model is based on the deep Q-network, a convolutional neural\\nnetwork trained with a variant of Q-learning. Its input is raw pixels and its\\noutput is a value function estimating future rewards from taking an action\\ngiven a system state. To distribute the deep Q-network training, we adapt the\\nDistBelief software framework to the context of efficiently training\\nreinforcement learning agents. As a result, the method is completely\\nasynchronous and scales well with the number of machines. We demonstrate that\\nthe deep Q-network agent, receiving only the pixels and the game score as\\ninputs, was able to achieve reasonable success on a simple game with minimal\\nparameter tuning.',\n",
              " 'Lifted Relational Neural Networks\\nWe propose a method combining relational-logic representations with neural\\nnetwork learning. A general lifted architecture, possibly reflecting some\\nbackground domain knowledge, is described through relational rules which may be\\nhandcrafted or learned. The relational rule-set serves as a template for\\nunfolding possibly deep neural networks whose structures also reflect the\\nstructures of given training or testing relational examples. Different networks\\ncorresponding to different examples share their weights, which co-evolve during\\ntraining by stochastic gradient descent algorithm. The framework allows for\\nhierarchical relational modeling constructs and learning of latent relational\\nconcepts through shared hidden layers weights corresponding to the rules.\\nDiscovery of notable relational concepts and experiments on 78 relational\\nlearning benchmarks demonstrate favorable performance of the method.',\n",
              " \"Giraffe: Using Deep Reinforcement Learning to Play Chess\\nThis report presents Giraffe, a chess engine that uses self-play to discover\\nall its domain-specific knowledge, with minimal hand-crafted knowledge given by\\nthe programmer. Unlike previous attempts using machine learning only to perform\\nparameter-tuning on hand-crafted evaluation functions, Giraffe's learning\\nsystem also performs automatic feature extraction and pattern recognition. The\\ntrained evaluation function performs comparably to the evaluation functions of\\nstate-of-the-art chess engines - all of which containing thousands of lines of\\ncarefully hand-crafted pattern recognizers, tuned over many years by both\\ncomputer chess experts and human chess masters. Giraffe is the most successful\\nattempt thus far at using end-to-end machine learning to play chess.\",\n",
              " 'Attention with Intention for a Neural Network Conversation Model\\nIn a conversation or a dialogue process, attention and intention play\\nintrinsic roles. This paper proposes a neural network based approach that\\nmodels the attention and intention processes. It essentially consists of three\\nrecurrent networks. The encoder network is a word-level model representing\\nsource side sentences. The intention network is a recurrent network that models\\nthe dynamics of the intention process. The decoder network is a recurrent\\nnetwork produces responses to the input from the source side. It is a language\\nmodel that is dependent on the intention and has an attention mechanism to\\nattend to particular source side words, when predicting a symbol in the\\nresponse. The model is trained end-to-end without labeling data. Experiments\\nshow that this model generates natural responses to user inputs.',\n",
              " 'Deep Reinforcement Learning in Parameterized Action Space\\nRecent work has shown that deep neural networks are capable of approximating\\nboth value functions and policies in reinforcement learning domains featuring\\ncontinuous state and action spaces. However, to the best of our knowledge no\\nprevious work has succeeded at using deep neural networks in structured\\n(parameterized) continuous action spaces. To fill this gap, this paper focuses\\non learning within the domain of simulated RoboCup soccer, which features a\\nsmall set of discrete action types, each of which is parameterized with\\ncontinuous variables. The best learned agent can score goals more reliably than\\nthe 2012 RoboCup champion agent. As such, this paper represents a successful\\nextension of deep reinforcement learning to the class of parameterized action\\nspace MDPs.',\n",
              " \"MazeBase: A Sandbox for Learning from Games\\nThis paper introduces MazeBase: an environment for simple 2D games, designed\\nas a sandbox for machine learning approaches to reasoning and planning. Within\\nit, we create 10 simple games embodying a range of algorithmic tasks (e.g.\\nif-then statements or set negation). A variety of neural models (fully\\nconnected, convolutional network, memory network) are deployed via\\nreinforcement learning on these games, with and without a procedurally\\ngenerated curriculum. Despite the tasks' simplicity, the performance of the\\nmodels is far from optimal, suggesting directions for future development. We\\nalso demonstrate the versatility of MazeBase by using it to emulate small\\ncombat scenarios from StarCraft. Models trained on the MazeBase version can be\\ndirectly applied to StarCraft, where they consistently beat the in-game AI.\",\n",
              " 'On Learning to Think: Algorithmic Information Theory for Novel\\n  Combinations of Reinforcement Learning Controllers and Recurrent Neural World\\n  Models\\nThis paper addresses the general problem of reinforcement learning (RL) in\\npartially observable environments. In 2013, our large RL recurrent neural\\nnetworks (RNNs) learned from scratch to drive simulated cars from\\nhigh-dimensional video input. However, real brains are more powerful in many\\nways. In particular, they learn a predictive model of their initially unknown\\nenvironment, and somehow use it for abstract (e.g., hierarchical) planning and\\nreasoning. Guided by algorithmic information theory, we describe RNN-based AIs\\n(RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending\\nsequences of tasks, some of them provided by the user, others invented by the\\nRNNAI itself in a curious, playful fashion, to improve its RNN-based world\\nmodel. Unlike our previous model-building RNN-based RL machines dating back to\\n1990, the RNNAI learns to actively query its model for abstract reasoning and\\nplanning and decision making, essentially \"learning to think.\" The basic ideas\\nof this report can be applied to many other cases where one RNN-like system\\nexploits the algorithmic information content of another. They are taken from a\\ngrant proposal submitted in Fall 2014, and also explain concepts such as\\n\"mirror neurons.\" Experimental results will be described in separate papers.',\n",
              " 'An Empirical Comparison of Neural Architectures for Reinforcement\\n  Learning in Partially Observable Environments\\nThis paper explores the performance of fitted neural Q iteration for\\nreinforcement learning in several partially observable environments, using\\nthree recurrent neural network architectures: Long Short-Term Memory, Gated\\nRecurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of\\nseveral thousands candidate architectures. A variant of fitted Q iteration,\\nbased on Advantage values instead of Q values, is also explored. The results\\nshow that GRU performs significantly better than LSTM and MUT1 for most of the\\nproblems considered, requiring less training episodes and less CPU time before\\nlearning a very good policy. Advantage learning also tends to produce better\\nresults.',\n",
              " \"Predicting Clinical Events by Combining Static and Dynamic Information\\n  Using Recurrent Neural Networks\\nIn clinical data sets we often find static information (e.g. patient gender,\\nblood type, etc.) combined with sequences of data that are recorded during\\nmultiple hospital visits (e.g. medications prescribed, tests performed, etc.).\\nRecurrent Neural Networks (RNNs) have proven to be very successful for\\nmodelling sequences of data in many areas of Machine Learning. In this work we\\npresent an approach based on RNNs, specifically designed for the clinical\\ndomain, that combines static and dynamic information in order to predict future\\nevents. We work with a database collected in the Charit\\\\'{e} Hospital in Berlin\\nthat contains complete information concerning patients that underwent a kidney\\ntransplantation. After the transplantation three main endpoints can occur:\\nrejection of the kidney, loss of the kidney and death of the patient. Our goal\\nis to predict, based on information recorded in the Electronic Health Record of\\neach patient, whether any of those endpoints will occur within the next six or\\ntwelve months after each visit to the clinic. We compared different types of\\nRNNs that we developed for this work, with a model based on a Feedforward\\nNeural Network and a Logistic Regression model. We found that the RNN that we\\ndeveloped based on Gated Recurrent Units provides the best performance for this\\ntask. We also used the same models for a second task, i.e., next event\\nprediction, and found that here the model based on a Feedforward Neural Network\\noutperformed the other models. Our hypothesis is that long-term dependencies\\nare not as relevant in this task.\",\n",
              " 'Weight Normalization: A Simple Reparameterization to Accelerate Training\\n  of Deep Neural Networks\\nWe present weight normalization: a reparameterization of the weight vectors\\nin a neural network that decouples the length of those weight vectors from\\ntheir direction. By reparameterizing the weights in this way we improve the\\nconditioning of the optimization problem and we speed up convergence of\\nstochastic gradient descent. Our reparameterization is inspired by batch\\nnormalization but does not introduce any dependencies between the examples in a\\nminibatch. This means that our method can also be applied successfully to\\nrecurrent models such as LSTMs and to noise-sensitive applications such as deep\\nreinforcement learning or generative models, for which batch normalization is\\nless well suited. Although our method is much simpler, it still provides much\\nof the speed-up of full batch normalization. In addition, the computational\\noverhead of our method is lower, permitting more optimization steps to be taken\\nin the same amount of time. We demonstrate the usefulness of our method on\\napplications in supervised image recognition, generative modelling, and deep\\nreinforcement learning.',\n",
              " 'Bounded Rational Decision-Making in Feedforward Neural Networks\\nBounded rational decision-makers transform sensory input into motor output\\nunder limited computational resources. Mathematically, such decision-makers can\\nbe modeled as information-theoretic channels with limited transmission rate.\\nHere, we apply this formalism for the first time to multilayer feedforward\\nneural networks. We derive synaptic weight update rules for two scenarios,\\nwhere either each neuron is considered as a bounded rational decision-maker or\\nthe network as a whole. In the update rules, bounded rationality translates\\ninto information-theoretically motivated types of regularization in weight\\nspace. In experiments on the MNIST benchmark classification task for\\nhandwritten digits, we show that such information-theoretic regularization\\nsuccessfully prevents overfitting across different architectures and attains\\nresults that are competitive with other recent techniques like dropout,\\ndropconnect and Bayes by backprop, for both ordinary and convolutional neural\\nnetworks.',\n",
              " 'Lie Access Neural Turing Machine\\nFollowing the recent trend in explicit neural memory structures, we present a\\nnew design of an external memory, wherein memories are stored in an Euclidean\\nkey space $\\\\mathbb R^n$. An LSTM controller performs read and write via\\nspecialized read and write heads. It can move a head by either providing a new\\naddress in the key space (aka random access) or moving from its previous\\nposition via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\"\\ninstructions of a traditional Turing Machine are generalized to arbitrary\\nelements of a fixed Lie group action. For this reason, we name this new model\\nthe Lie Access Neural Turing Machine, or LANTM.\\n  We tested two different configurations of LANTM against an LSTM baseline in\\nseveral basic experiments. We found the right configuration of LANTM to\\noutperform the baseline in all of our experiments. In particular, we trained\\nLANTM on addition of $k$-digit numbers for $2 \\\\le k \\\\le 16$, but it was able to\\ngeneralize almost perfectly to $17 \\\\le k \\\\le 32$, all with the number of\\nparameters 2 orders of magnitude below the LSTM baseline.',\n",
              " 'Towards Machine Intelligence\\nThere exists a theory of a single general-purpose learning algorithm which\\ncould explain the principles of its operation. This theory assumes that the\\nbrain has some initial rough architecture, a small library of simple innate\\ncircuits which are prewired at birth and proposes that all significant mental\\nalgorithms can be learned. Given current understanding and observations, this\\npaper reviews and lists the ingredients of such an algorithm from both\\narchitectural and functional perspectives.',\n",
              " 'Dynamic Frame skip Deep Q Network\\nDeep Reinforcement Learning methods have achieved state of the art\\nperformance in learning control policies for the games in the Atari 2600\\ndomain. One of the important parameters in the Arcade Learning Environment\\n(ALE) is the frame skip rate. It decides the granularity at which agents can\\ncontrol game play. A frame skip value of $k$ allows the agent to repeat a\\nselected action $k$ number of times. The current state of the art architectures\\nlike Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of\\na framework with a static frame skip rate, where the action output from the\\nnetwork is repeated for a fixed number of frames regardless of the current\\nstate. In this paper, we propose a new architecture, Dynamic Frame skip Deep\\nQ-Network (DFDQN) which makes the frame skip rate a dynamic learnable\\nparameter. This allows us to choose the number of times an action is to be\\nrepeated based on the current state. We show empirically that such a setting\\nimproves the performance on relatively harder games like Seaquest.',\n",
              " 'Programming with a Differentiable Forth Interpreter\\nGiven that in practice training data is scarce for all but a small set of\\nproblems, a core question is how to incorporate prior knowledge into a model.\\nIn this paper, we consider the case of prior procedural knowledge for neural\\nnetworks, such as knowing how a program should traverse a sequence, but not\\nwhat local actions should be performed at each step. To this end, we present an\\nend-to-end differentiable interpreter for the programming language Forth which\\nenables programmers to write program sketches with slots that can be filled\\nwith behaviour trained from program input-output data. We can optimise this\\nbehaviour directly through gradient descent techniques on user-specified\\nobjectives, and also integrate the program into any larger neural computation\\ngraph. We show empirically that our interpreter is able to effectively leverage\\ndifferent levels of prior program structure and learn complex behaviours such\\nas sequence sorting and addition. When connected to outputs of an LSTM and\\ntrained jointly, our interpreter achieves state-of-the-art accuracy for\\nend-to-end reasoning about quantities expressed in natural language stories.',\n",
              " 'Generative Choreography using Deep Learning\\nRecent advances in deep learning have enabled the extraction of high-level\\nfeatures from raw sensor data which has opened up new possibilities in many\\ndifferent fields, including computer generated choreography. In this paper we\\npresent a system chor-rnn for generating novel choreographic material in the\\nnuanced choreographic language and style of an individual choreographer. It\\nalso shows promising results in producing a higher level compositional\\ncohesion, rather than just generating sequences of movement. At the core of\\nchor-rnn is a deep recurrent neural network trained on raw motion capture data\\nand that can generate new dance sequences for a solo dancer. Chor-rnn can be\\nused for collaborative human-machine choreography or as a creative catalyst,\\nserving as inspiration for a choreographer.',\n",
              " \"Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and\\n  Knowledge\\nWe propose Logic Tensor Networks: a uniform framework for integrating\\nautomatic learning and reasoning. A logic formalism called Real Logic is\\ndefined on a first-order language whereby formulas have truth-value in the\\ninterval [0,1] and semantics defined concretely on the domain of real numbers.\\nLogical constants are interpreted as feature vectors of real numbers. Real\\nLogic promotes a well-founded integration of deductive reasoning on a\\nknowledge-base and efficient data-driven relational machine learning. We show\\nhow Real Logic can be implemented in deep Tensor Neural Networks with the use\\nof Google's tensorflow primitives. The paper concludes with experiments\\napplying Logic Tensor Networks on a simple but representative example of\\nknowledge completion.\",\n",
              " \"Identifying and Harnessing the Building Blocks of Machine Learning\\n  Pipelines for Sensible Initialization of a Data Science Automation Tool\\nAs data science continues to grow in popularity, there will be an increasing\\nneed to make data science tools more scalable, flexible, and accessible. In\\nparticular, automated machine learning (AutoML) systems seek to automate the\\nprocess of designing and optimizing machine learning pipelines. In this\\nchapter, we present a genetic programming-based AutoML system called TPOT that\\noptimizes a series of feature preprocessors and machine learning models with\\nthe goal of maximizing classification accuracy on a supervised classification\\nproblem. Further, we analyze a large database of pipelines that were previously\\nused to solve various supervised classification problems and identify 100 short\\nseries of machine learning operations that appear the most frequently, which we\\ncall the building blocks of machine learning pipelines. We harness these\\nbuilding blocks to initialize TPOT with promising solutions, and find that this\\nsensible initialization method significantly improves TPOT's performance on one\\nbenchmark at no cost of significantly degrading performance on the others.\\nThus, sensible initialization with machine learning pipeline building blocks\\nshows promise for GP-based AutoML systems, and should be further refined in\\nfuture work.\",\n",
              " 'Neuroevolution-Based Inverse Reinforcement Learning\\nThe problem of Learning from Demonstration is targeted at learning to perform\\ntasks based on observed examples. One approach to Learning from Demonstration\\nis Inverse Reinforcement Learning, in which actions are observed to infer\\nrewards. This work combines a feature based state evaluation approach to\\nInverse Reinforcement Learning with neuroevolution, a paradigm for modifying\\nneural networks based on their performance on a given task. Neural networks are\\nused to learn from a demonstrated expert policy and are evolved to generate a\\npolicy similar to the demonstration. The algorithm is discussed and evaluated\\nagainst competitive feature-based Inverse Reinforcement Learning approaches. At\\nthe cost of execution time, neural networks allow for non-linear combinations\\nof features in state evaluations. These valuations may correspond to state\\nvalue or state reward. This results in better correspondence to observed\\nexamples as opposed to using linear combinations. This work also extends\\nexisting work on Bayesian Non-Parametric Feature Construction for Inverse\\nReinforcement Learning by using non-linear combinations of intermediate data to\\nimprove performance. The algorithm is observed to be specifically suitable for\\na linearly solvable non-deterministic Markov Decision Processes in which\\nmultiple rewards are sparsely scattered in state space. A conclusive\\nperformance hierarchy between evaluated algorithms is presented.',\n",
              " 'TerpreT: A Probabilistic Programming Language for Program Induction\\nWe study machine learning formulations of inductive program synthesis; given\\ninput-output examples, we try to synthesize source code that maps inputs to\\ncorresponding outputs. Our aims are to develop new machine learning approaches\\nbased on neural networks and graphical models, and to understand the\\ncapabilities of machine learning techniques relative to traditional\\nalternatives, such as those based on constraint solving from the programming\\nlanguages community.\\n  Our key contribution is the proposal of TerpreT, a domain-specific language\\nfor expressing program synthesis problems. TerpreT is similar to a\\nprobabilistic programming language: a model is composed of a specification of a\\nprogram representation (declarations of random variables) and an interpreter\\ndescribing how programs map inputs to outputs (a model connecting unknowns to\\nobservations). The inference task is to observe a set of input-output examples\\nand infer the underlying program. TerpreT has two main benefits. First, it\\nenables rapid exploration of a range of domains, program representations, and\\ninterpreter models. Second, it separates the model specification from the\\ninference algorithm, allowing like-to-like comparisons between different\\napproaches to inference. From a single TerpreT specification we automatically\\nperform inference using four different back-ends. These are based on gradient\\ndescent, linear program (LP) relaxations for graphical models, discrete\\nsatisfiability solving, and the Sketch program synthesis system.\\n  We illustrate the value of TerpreT by developing several interpreter models\\nand performing an empirical comparison between alternative inference\\nalgorithms. Our key empirical finding is that constraint solvers dominate the\\ngradient descent and LP-based formulations. We conclude with suggestions for\\nthe machine learning community to make progress on program synthesis.',\n",
              " 'Multi-Label Classification Method Based on Extreme Learning Machines\\nIn this paper, an Extreme Learning Machine (ELM) based technique for\\nMulti-label classification problems is proposed and discussed. In multi-label\\nclassification, each of the input data samples belongs to one or more than one\\nclass labels. The traditional binary and multi-class classification problems\\nare the subset of the multi-label problem with the number of labels\\ncorresponding to each sample limited to one. The proposed ELM based multi-label\\nclassification technique is evaluated with six different benchmark multi-label\\ndatasets from different domains such as multimedia, text and biology. A\\ndetailed comparison of the results is made by comparing the proposed method\\nwith the results from nine state of the arts techniques for five different\\nevaluation metrics. The nine methods are chosen from different categories of\\nmulti-label methods. The comparative results shows that the proposed Extreme\\nLearning Machine based multi-label classification technique is a better\\nalternative than the existing state of the art methods for multi-label\\nproblems.',\n",
              " 'A Novel Online Real-time Classifier for Multi-label Data Streams\\nIn this paper, a novel extreme learning machine based online multi-label\\nclassifier for real-time data streams is proposed. Multi-label classification\\nis one of the actively researched machine learning paradigm that has gained\\nmuch attention in the recent years due to its rapidly increasing real world\\napplications. In contrast to traditional binary and multi-class classification,\\nmulti-label classification involves association of each of the input samples\\nwith a set of target labels simultaneously. There are no real-time online\\nneural network based multi-label classifier available in the literature. In\\nthis paper, we exploit the inherent nature of high speed exhibited by the\\nextreme learning machines to develop a novel online real-time classifier for\\nmulti-label data streams. The developed classifier is experimented with\\ndatasets from different application domains for consistency, performance and\\nspeed. The experimental studies show that the proposed method outperforms the\\nexisting state-of-the-art techniques in terms of speed and accuracy and can\\nclassify multi-label data streams in real-time.',\n",
              " 'A Novel Progressive Learning Technique for Multi-class Classification\\nIn this paper, a progressive learning technique for multi-class\\nclassification is proposed. This newly developed learning technique is\\nindependent of the number of class constraints and it can learn new classes\\nwhile still retaining the knowledge of previous classes. Whenever a new class\\n(non-native to the knowledge learnt thus far) is encountered, the neural\\nnetwork structure gets remodeled automatically by facilitating new neurons and\\ninterconnections, and the parameters are calculated in such a way that it\\nretains the knowledge learnt thus far. This technique is suitable for\\nreal-world applications where the number of classes is often unknown and online\\nlearning from real-time data is required. The consistency and the complexity of\\nthe progressive learning technique are analyzed. Several standard datasets are\\nused to evaluate the performance of the developed technique. A comparative\\nstudy shows that the developed technique is superior.',\n",
              " 'A novel online multi-label classifier for high-speed streaming data\\n  applications\\nIn this paper, a high-speed online neural network classifier based on extreme\\nlearning machines for multi-label classification is proposed. In multi-label\\nclassification, each of the input data sample belongs to one or more than one\\nof the target labels. The traditional binary and multi-class classification\\nwhere each sample belongs to only one target class forms the subset of\\nmulti-label classification. Multi-label classification problems are far more\\ncomplex than binary and multi-class classification problems, as both the number\\nof target labels and each of the target labels corresponding to each of the\\ninput samples are to be identified. The proposed work exploits the high-speed\\nnature of the extreme learning machines to achieve real-time multi-label\\nclassification of streaming data. A new threshold-based online sequential\\nlearning algorithm is proposed for high speed and streaming data classification\\nof multi-label problems. The proposed method is experimented with six different\\ndatasets from different application domains such as multimedia, text, and\\nbiology. The hamming loss, accuracy, training time and testing time of the\\nproposed technique is compared with nine different state-of-the-art methods.\\nExperimental studies shows that the proposed technique outperforms the existing\\nmulti-label classifiers in terms of performance and speed.',\n",
              " 'Ternary Neural Networks for Resource-Efficient AI Applications\\nThe computation and storage requirements for Deep Neural Networks (DNNs) are\\nusually high. This issue limits their deployability on ubiquitous computing\\ndevices such as smart phones, wearables and autonomous drones. In this paper,\\nwe propose ternary neural networks (TNNs) in order to make deep learning more\\nresource-efficient. We train these TNNs using a teacher-student approach based\\non a novel, layer-wise greedy methodology. Thanks to our two-stage training\\nprocedure, the teacher network is still able to use state-of-the-art methods\\nsuch as dropout and batch normalization to increase accuracy and reduce\\ntraining time. Using only ternary weights and activations, the student ternary\\nnetwork learns to mimic the behavior of its teacher network without using any\\nmultiplication. Unlike its -1,1 binary counterparts, a ternary neural network\\ninherently prunes the smaller weights by setting them to zero during training.\\nThis makes them sparser and thus more energy-efficient. We design a\\npurpose-built hardware architecture for TNNs and implement it on FPGA and ASIC.\\nWe evaluate TNNs on several benchmark datasets and demonstrate up to 3.1x\\nbetter energy efficiency with respect to the state of the art while also\\nimproving accuracy.',\n",
              " 'Fitted Learning: Models with Awareness of their Limits\\nThough deep learning has pushed the boundaries of classification forward, in\\nrecent years hints of the limits of standard classification have begun to\\nemerge. Problems such as fooling, adding new classes over time, and the need to\\nretrain learning models only for small changes to the original problem all\\npoint to a potential shortcoming in the classic classification regime, where a\\ncomprehensive a priori knowledge of the possible classes or concepts is\\ncritical. Without such knowledge, classifiers misjudge the limits of their\\nknowledge and overgeneralization therefore becomes a serious obstacle to\\nconsistent performance. In response to these challenges, this paper extends the\\nclassic regime by reframing classification instead with the assumption that\\nconcepts present in the training set are only a sample of the hypothetical\\nfinal set of concepts. To bring learning models into this new paradigm, a novel\\nelaboration of standard architectures called the competitive overcomplete\\noutput layer (COOL) neural network is introduced. Experiments demonstrate the\\neffectiveness of COOL by applying it to fooling, separable concept learning,\\none-class neural networks, and standard classification benchmarks. The results\\nsuggest that, unlike conventional classifiers, the amount of generalization in\\nCOOL networks can be tuned to match the problem.',\n",
              " 'Learning to learn with backpropagation of Hebbian plasticity\\nHebbian plasticity is a powerful principle that allows biological brains to\\nlearn from their lifetime experience. By contrast, artificial neural networks\\ntrained with backpropagation generally have fixed connection weights that do\\nnot change once training is complete. While recent methods can endow neural\\nnetworks with long-term memories, Hebbian plasticity is currently not amenable\\nto gradient descent. Here we derive analytical expressions for activity\\ngradients in neural networks with Hebbian plastic connections. Using these\\nexpressions, we can use backpropagation to train not just the baseline weights\\nof the connections, but also their plasticity. As a result, the networks \"learn\\nhow to learn\" in order to solve the problem at hand: the trained networks\\nautomatically perform fast learning of unpredictable environmental features\\nduring their lifetime, expanding the range of solvable problems. We test the\\nalgorithm on various on-line learning tasks, including pattern completion,\\none-shot learning, and reversal learning. The algorithm successfully learns how\\nto learn the relevant associations from one-shot instruction, and fine-tunes\\nthe temporal dynamics of plasticity to allow for continual learning in response\\nto changing environmental parameters. We conclude that backpropagation of\\nHebbian plasticity offers a powerful model for lifelong learning.',\n",
              " 'Learning by Stimulation Avoidance: A Principle to Control Spiking Neural\\n  Networks Dynamics\\nLearning based on networks of real neurons, and by extension biologically\\ninspired models of neural networks, has yet to find general learning rules\\nleading to widespread applications. In this paper, we argue for the existence\\nof a principle allowing to steer the dynamics of a biologically inspired neural\\nnetwork. Using carefully timed external stimulation, the network can be driven\\ntowards a desired dynamical state. We term this principle \"Learning by\\nStimulation Avoidance\" (LSA). We demonstrate through simulation that the\\nminimal sufficient conditions leading to LSA in artificial networks are also\\nsufficient to reproduce learning results similar to those obtained in\\nbiological neurons by Shahaf and Marom [1]. We examine the mechanism\\'s basic\\ndynamics in a reduced network, and demonstrate how it scales up to a network of\\n100 neurons. We show that LSA has a higher explanatory power than existing\\nhypotheses about the response of biological neural networks to external\\nsimulation, and can be used as a learning rule for an embodied application:\\nlearning of wall avoidance by a simulated robot. The surge in popularity of\\nartificial neural networks is mostly directed to disembodied models of neurons\\nwith biologically irrelevant dynamics: to the authors\\' knowledge, this is the\\nfirst work demonstrating sensory-motor learning with random spiking networks\\nthrough pure Hebbian learning.',\n",
              " \"Surprisal-Driven Zoneout\\nWe propose a novel method of regularization for recurrent neural networks\\ncalled suprisal-driven zoneout. In this method, states zoneout (maintain their\\nprevious value rather than updating), when the suprisal (discrepancy between\\nthe last state's prediction and target) is small. Thus regularization is\\nadaptive and input-driven on a per-neuron basis. We demonstrate the\\neffectiveness of this idea by achieving state-of-the-art bits per character of\\n1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to\\nthe best known highly-engineered compression methods.\",\n",
              " 'Neural Architecture Search with Reinforcement Learning\\nNeural networks are powerful and flexible models that work well for many\\ndifficult learning tasks in image, speech and natural language understanding.\\nDespite their success, neural networks are still hard to design. In this paper,\\nwe use a recurrent network to generate the model descriptions of neural\\nnetworks and train this RNN with reinforcement learning to maximize the\\nexpected accuracy of the generated architectures on a validation set. On the\\nCIFAR-10 dataset, our method, starting from scratch, can design a novel network\\narchitecture that rivals the best human-invented architecture in terms of test\\nset accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is\\n0.09 percent better and 1.05x faster than the previous state-of-the-art model\\nthat used a similar architectural scheme. On the Penn Treebank dataset, our\\nmodel can compose a novel recurrent cell that outperforms the widely-used LSTM\\ncell, and other state-of-the-art baselines. Our cell achieves a test set\\nperplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than\\nthe previous state-of-the-art model. The cell can also be transferred to the\\ncharacter language modeling task on PTB and achieves a state-of-the-art\\nperplexity of 1.214.',\n",
              " \"Emergence of foveal image sampling from learning to attend in visual\\n  scenes\\nWe describe a neural attention model with a learnable retinal sampling\\nlattice. The model is trained on a visual search task requiring the\\nclassification of an object embedded in a visual scene amidst background\\ndistractors using the smallest number of fixations. We explore the tiling\\nproperties that emerge in the model's retinal sampling lattice after training.\\nSpecifically, we show that this lattice resembles the eccentricity dependent\\nsampling lattice of the primate retina, with a high resolution region in the\\nfovea surrounded by a low resolution periphery. Furthermore, we find conditions\\nwhere these emergent properties are amplified or eliminated providing clues to\\ntheir function.\",\n",
              " \"Long Timescale Credit Assignment in NeuralNetworks with External Memory\\nCredit assignment in traditional recurrent neural networks usually involves\\nback-propagating through a long chain of tied weight matrices. The length of\\nthis chain scales linearly with the number of time-steps as the same network is\\nrun at each time-step. This creates many problems, such as vanishing gradients,\\nthat have been well studied. In contrast, a NNEM's architecture recurrent\\nactivity doesn't involve a long chain of activity (though some architectures\\nsuch as the NTM do utilize a traditional recurrent architecture as a\\ncontroller). Rather, the externally stored embedding vectors are used at each\\ntime-step, but no messages are passed from previous time-steps. This means that\\nvanishing gradients aren't a problem, as all of the necessary gradient paths\\nare short. However, these paths are extremely numerous (one per embedding\\nvector in memory) and reused for a very long time (until it leaves the memory).\\nThus, the forward-pass information of each memory must be stored for the entire\\nduration of the memory. This is problematic as this additional storage far\\nsurpasses that of the actual memories, to the extent that large memories on\\ninfeasible to back-propagate through in high dimensional settings. One way to\\nget around the need to hold onto forward-pass information is to recalculate the\\nforward-pass whenever gradient information is available. However, if the\\nobservations are too large to store in the domain of interest, direct\\nreinstatement of a forward pass cannot occur. Instead, we rely on a learned\\nautoencoder to reinstate the observation, and then use the embedding network to\\nrecalculate the forward-pass. Since the recalculated embedding vector is\\nunlikely to perfectly match the one stored in memory, we try out 2\\napproximations to utilize error gradient w.r.t. the vector in memory.\",\n",
              " 'Energy Saving Additive Neural Network\\nIn recent years, machine learning techniques based on neural networks for\\nmobile computing become increasingly popular. Classical multi-layer neural\\nnetworks require matrix multiplications at each stage. Multiplication operation\\nis not an energy efficient operation and consequently it drains the battery of\\nthe mobile device. In this paper, we propose a new energy efficient neural\\nnetwork with the universal approximation property over space of Lebesgue\\nintegrable functions. This network, called, additive neural network, is very\\nsuitable for mobile computing. The neural structure is based on a novel vector\\nproduct definition, called ef-operator, that permits a multiplier-free\\nimplementation. In ef-operation, the \"product\" of two real numbers is defined\\nas the sum of their absolute values, with the sign determined by the sign of\\nthe product of the numbers. This \"product\" is used to construct a vector\\nproduct in $R^N$. The vector product induces the $l_1$ norm. The proposed\\nadditive neural network successfully solves the XOR problem. The experiments on\\nMNIST dataset show that the classification performances of the proposed\\nadditive neural networks are very similar to the corresponding multi-layer\\nperceptron and convolutional neural networks (LeNet).',\n",
              " 'Learning to Repeat: Fine Grained Action Repetition for Deep\\n  Reinforcement Learning\\nReinforcement Learning algorithms can learn complex behavioral patterns for\\nsequential decision making tasks wherein an agent interacts with an environment\\nand acquires feedback in the form of rewards sampled from it. Traditionally,\\nsuch algorithms make decisions, i.e., select actions to execute, at every\\nsingle time step of the agent-environment interactions. In this paper, we\\npropose a novel framework, Fine Grained Action Repetition (FiGAR), which\\nenables the agent to decide the action as well as the time scale of repeating\\nit. FiGAR can be used for improving any Deep Reinforcement Learning algorithm\\nwhich maintains an explicit policy estimate by enabling temporal abstractions\\nin the action space. We empirically demonstrate the efficacy of our framework\\nby showing performance improvements on top of three policy search algorithms in\\ndifferent domains: Asynchronous Advantage Actor Critic in the Atari 2600\\ndomain, Trust Region Policy Optimization in Mujoco domain and Deep\\nDeterministic Policy Gradients in the TORCS car racing domain.',\n",
              " \"Survey of reasoning using Neural networks\\nReason and inference require process as well as memory skills by humans.\\nNeural networks are able to process tasks like image recognition (better than\\nhumans) but in memory aspects are still limited (by attention mechanism, size).\\nRecurrent Neural Network (RNN) and it's modified version LSTM are able to solve\\nsmall memory contexts, but as context becomes larger than a threshold, it is\\ndifficult to use them. The Solution is to use large external memory. Still, it\\nposes many challenges like, how to train neural networks for discrete memory\\nrepresentation, how to describe long term dependencies in sequential data etc.\\nMost prominent neural architectures for such tasks are Memory networks:\\ninference components combined with long term memory and Neural Turing Machines:\\nneural networks using external memory resources. Also, additional techniques\\nlike attention mechanism, end to end gradient descent on discrete memory\\nrepresentation are needed to support these solutions. Preliminary results of\\nabove neural architectures on simple algorithms (sorting, copying) and Question\\nAnswering (based on story, dialogs) application are comparable with the state\\nof the art. In this paper, I explain these architectures (in general), the\\nadditional techniques used and the results of their application.\",\n",
              " 'One-Shot Imitation Learning\\nImitation learning has been commonly applied to solve different tasks in\\nisolation. This usually requires either careful feature engineering, or a\\nsignificant number of samples. This is far from what we desire: ideally, robots\\nshould be able to learn from very few demonstrations of any given task, and\\ninstantly generalize to new situations of the same task, without requiring\\ntask-specific engineering. In this paper, we propose a meta-learning framework\\nfor achieving such capability, which we call one-shot imitation learning.\\n  Specifically, we consider the setting where there is a very large set of\\ntasks, and each task has many instantiations. For example, a task could be to\\nstack all blocks on a table into a single tower, another task could be to place\\nall blocks on a table into two-block towers, etc. In each case, different\\ninstances of the task would consist of different sets of blocks with different\\ninitial states. At training time, our algorithm is presented with pairs of\\ndemonstrations for a subset of all tasks. A neural net is trained that takes as\\ninput one demonstration and the current state (which initially is the initial\\nstate of the other demonstration of the pair), and outputs an action with the\\ngoal that the resulting sequence of states and actions matches as closely as\\npossible with the second demonstration. At test time, a demonstration of a\\nsingle instance of a new task is presented, and the neural net is expected to\\nperform well on new instances of this new task. The use of soft attention\\nallows the model to generalize to conditions and tasks unseen in the training\\ndata. We anticipate that by training this model on a much greater variety of\\ntasks and settings, we will obtain a general system that can turn any\\ndemonstrations into robust policies that can accomplish an overwhelming variety\\nof tasks.\\n  Videos available at https://bit.ly/nips2017-oneshot .',\n",
              " 'Deep Learning for Explicitly Modeling Optimization Landscapes\\nIn all but the most trivial optimization problems, the structure of the\\nsolutions exhibit complex interdependencies between the input parameters.\\nDecades of research with stochastic search techniques has shown the benefit of\\nexplicitly modeling the interactions between sets of parameters and the overall\\nquality of the solutions discovered. We demonstrate a novel method, based on\\nlearning deep networks, to model the global landscapes of optimization\\nproblems. To represent the search space concisely and accurately, the deep\\nnetworks must encode information about the underlying parameter interactions\\nand their contributions to the quality of the solution. Once the networks are\\ntrained, the networks are probed to reveal parameter combinations with high\\nexpected performance with respect to the optimization task. These estimates are\\nused to initialize fast, randomized, local search algorithms, which in turn\\nexpose more information about the search space that is subsequently used to\\nrefine the models. We demonstrate the technique on multiple optimization\\nproblems that have arisen in a variety of real-world domains, including:\\npacking, graphics, job scheduling, layout and compression. The problems include\\ncombinatoric search spaces, discontinuous and highly non-linear spaces, and\\nspan binary, higher-cardinality discrete, as well as continuous parameters.\\nStrengths, limitations, and extensions of the approach are extensively\\ndiscussed and demonstrated.',\n",
              " 'Stochastic Neural Networks for Hierarchical Reinforcement Learning\\nDeep reinforcement learning has achieved many impressive results in recent\\nyears. However, tasks with sparse rewards or long horizons continue to pose\\nsignificant challenges. To tackle these important problems, we propose a\\ngeneral framework that first learns useful skills in a pre-training\\nenvironment, and then leverages the acquired skills for learning faster in\\ndownstream tasks. Our approach brings together some of the strengths of\\nintrinsic motivation and hierarchical methods: the learning of useful skill is\\nguided by a single proxy reward, the design of which requires very minimal\\ndomain knowledge about the downstream tasks. Then a high-level policy is\\ntrained on top of these skills, providing a significant improvement of the\\nexploration and allowing to tackle sparse rewards in the downstream tasks. To\\nefficiently pre-train a large span of skills, we use Stochastic Neural Networks\\ncombined with an information-theoretic regularizer. Our experiments show that\\nthis combination is effective in learning a wide span of interpretable skills\\nin a sample-efficient way, and can significantly boost the learning performance\\nuniformly across a wide range of downstream tasks.',\n",
              " 'Batch Reinforcement Learning on the Industrial Benchmark: First\\n  Experiences\\nThe Particle Swarm Optimization Policy (PSO-P) has been recently introduced\\nand proven to produce remarkable results on interacting with academic\\nreinforcement learning benchmarks in an off-policy, batch-based setting. To\\nfurther investigate the properties and feasibility on real-world applications,\\nthis paper investigates PSO-P on the so-called Industrial Benchmark (IB), a\\nnovel reinforcement learning (RL) benchmark that aims at being realistic by\\nincluding a variety of aspects found in industrial applications, like\\ncontinuous state and action spaces, a high dimensional, partially observable\\nstate space, delayed effects, and complex stochasticity. The experimental\\nresults of PSO-P on IB are compared to results of closed-form control policies\\nderived from the model-based Recurrent Control Neural Network (RCNN) and the\\nmodel-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not\\nonly of interest for academic benchmarks, but also for real-world industrial\\napplications, since it also yielded the best performing policy in our IB\\nsetting. Compared to other well established RL techniques, PSO-P produced\\noutstanding results in performance and robustness, requiring only a relatively\\nlow amount of effort in finding adequate parameters or making complex design\\ndecisions.',\n",
              " 'End-to-End Differentiable Proving\\nWe introduce neural networks for end-to-end differentiable proving of queries\\nto knowledge bases by operating on dense vector representations of symbols.\\nThese neural networks are constructed recursively by taking inspiration from\\nthe backward chaining algorithm as used in Prolog. Specifically, we replace\\nsymbolic unification with a differentiable computation on vector\\nrepresentations of symbols using a radial basis function kernel, thereby\\ncombining symbolic reasoning with learning subsymbolic vector representations.\\nBy using gradient descent, the resulting neural network can be trained to infer\\nfacts from a given incomplete knowledge base. It learns to (i) place\\nrepresentations of similar symbols in close proximity in a vector space, (ii)\\nmake use of such similarities to prove queries, (iii) induce logical rules, and\\n(iv) use provided and induced logical rules for multi-hop reasoning. We\\ndemonstrate that this architecture outperforms ComplEx, a state-of-the-art\\nneural link prediction model, on three out of four benchmark knowledge bases\\nwhile at the same time inducing interpretable function-free first-order logic\\nrules.',\n",
              " 'Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\\nWe explore deep reinforcement learning methods for multi-agent domains. We\\nbegin by analyzing the difficulty of traditional algorithms in the multi-agent\\ncase: Q-learning is challenged by an inherent non-stationarity of the\\nenvironment, while policy gradient suffers from a variance that increases as\\nthe number of agents grows. We then present an adaptation of actor-critic\\nmethods that considers action policies of other agents and is able to\\nsuccessfully learn policies that require complex multi-agent coordination.\\nAdditionally, we introduce a training regimen utilizing an ensemble of policies\\nfor each agent that leads to more robust multi-agent policies. We show the\\nstrength of our approach compared to existing methods in cooperative as well as\\ncompetitive scenarios, where agent populations are able to discover various\\nphysical and informational coordination strategies.',\n",
              " \"Getting deep recommenders fit: Bloom embeddings for sparse binary\\n  input/output networks\\nRecommendation algorithms that incorporate techniques from deep learning are\\nbecoming increasingly popular. Due to the structure of the data coming from\\nrecommendation domains (i.e., one-hot-encoded vectors of item preferences),\\nthese algorithms tend to have large input and output dimensionalities that\\ndominate their overall size. This makes them difficult to train, due to the\\nlimited memory of graphical processing units, and difficult to deploy on mobile\\ndevices with limited hardware. To address these difficulties, we propose Bloom\\nembeddings, a compression technique that can be applied to the input and output\\nof neural network models dealing with sparse high-dimensional binary-coded\\ninstances. Bloom embeddings are computationally efficient, and do not seriously\\ncompromise the accuracy of the model up to 1/5 compression ratios. In some\\ncases, they even improve over the original accuracy, with relative increases up\\nto 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4\\nalternative methods, obtaining favorable results. We also discuss a number of\\nfurther advantages of Bloom embeddings, such as 'on-the-fly' constant-time\\noperation, zero or marginal space requirements, training time speedups, or the\\nfact that they do not require any change to the core model architecture or\\ntraining configuration.\",\n",
              " 'Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural\\n  Network and Long-Term Evaluation\\nMonte Carlo tree search (MCTS) is extremely popular in computer Go which\\ndetermines each action by enormous simulations in a broad and deep search tree.\\nHowever, human experts select most actions by pattern analysis and careful\\nevaluation rather than brute search of millions of future nteractions. In this\\npaper, we propose a computer Go system that follows experts way of thinking and\\nplaying. Our system consists of two parts. The first part is a novel deep\\nalternative neural network (DANN) used to generate candidates of next move.\\nCompared with existing deep convolutional neural network (DCNN), DANN inserts\\nrecurrent layer after each convolutional layer and stacks them in an\\nalternative manner. We show such setting can preserve more contexts of local\\nfeatures and its evolutions which are beneficial for move prediction. The\\nsecond part is a long-term evaluation (LTE) module used to provide a reliable\\nevaluation of candidates rather than a single probability from move predictor.\\nThis is consistent with human experts nature of playing since they can foresee\\ntens of steps to give an accurate estimation of candidates. In our system, for\\neach candidate, LTE calculates a cumulative reward after several future\\ninteractions when local variations are settled. Combining criteria from the two\\nparts, our system determines the optimal choice of next move. For more\\ncomprehensive experiments, we introduce a new professional Go dataset (PGD),\\nconsisting of 253233 professional records. Experiments on GoGoD and PGD\\ndatasets show the DANN can substantially improve performance of move prediction\\nover pure DCNN. When combining LTE, our system outperforms most relevant\\napproaches and open engines based on MCTS.',\n",
              " 'Hindsight Experience Replay\\nDealing with sparse rewards is one of the biggest challenges in Reinforcement\\nLearning (RL). We present a novel technique called Hindsight Experience Replay\\nwhich allows sample-efficient learning from rewards which are sparse and binary\\nand therefore avoid the need for complicated reward engineering. It can be\\ncombined with an arbitrary off-policy RL algorithm and may be seen as a form of\\nimplicit curriculum.\\n  We demonstrate our approach on the task of manipulating objects with a\\nrobotic arm. In particular, we run experiments on three different tasks:\\npushing, sliding, and pick-and-place, in each case using only binary rewards\\nindicating whether or not the task is completed. Our ablation studies show that\\nHindsight Experience Replay is a crucial ingredient which makes training\\npossible in these challenging environments. We show that our policies trained\\non a physics simulation can be deployed on a physical robot and successfully\\ncomplete the task.',\n",
              " 'Trial without Error: Towards Safe Reinforcement Learning via Human\\n  Intervention\\nAI systems are increasingly applied to complex tasks that involve interaction\\nwith humans. During training, such systems are potentially dangerous, as they\\nhaven\\'t yet learned to avoid actions that could cause serious harm. How can an\\nAI system explore and learn without making a single mistake that harms humans\\nor otherwise causes serious damage? For model-free reinforcement learning,\\nhaving a human \"in the loop\" and ready to intervene is currently the only way\\nto prevent all catastrophes. We formalize human intervention for RL and show\\nhow to reduce the human labor required by training a supervised learner to\\nimitate the human\\'s intervention decisions. We evaluate this scheme on Atari\\ngames, with a Deep RL agent being overseen by a human for four hours. When the\\nclass of catastrophes is simple, we are able to prevent all catastrophes\\nwithout affecting the agent\\'s learning (whereas an RL baseline fails due to\\ncatastrophic forgetting). However, this scheme is less successful when\\ncatastrophes are more complex: it reduces but does not eliminate catastrophes\\nand the supervised learner fails on adversarial examples found by the agent.\\nExtrapolating to more challenging environments, we show that our implementation\\nwould not scale (due to the infeasible amount of human labor required). We\\noutline extensions of the scheme that are necessary if we are to train\\nmodel-free agents without a single catastrophe.',\n",
              " \"Reverse Curriculum Generation for Reinforcement Learning\\nMany relevant tasks require an agent to reach a certain state, or to\\nmanipulate objects into a desired configuration. For example, we might want a\\nrobot to align and assemble a gear onto an axle or insert and turn a key in a\\nlock. These goal-oriented tasks present a considerable challenge for\\nreinforcement learning, since their natural reward function is sparse and\\nprohibitive amounts of exploration are required to reach the goal and receive\\nsome learning signal. Past approaches tackle these problems by exploiting\\nexpert demonstrations or by manually designing a task-specific reward shaping\\nfunction to guide the learning agent. Instead, we propose a method to learn\\nthese tasks without requiring any prior knowledge other than obtaining a single\\nstate in which the task is achieved. The robot is trained in reverse, gradually\\nlearning to reach the goal from a set of start states increasingly far from the\\ngoal. Our method automatically generates a curriculum of start states that\\nadapts to the agent's performance, leading to efficient training on\\ngoal-oriented tasks. We demonstrate our approach on difficult simulated\\nnavigation and fine-grained manipulation problems, not solvable by\\nstate-of-the-art reinforcement learning methods.\",\n",
              " 'Ideological Sublations: Resolution of Dialectic in Population-based\\n  Optimization\\nA population-based optimization algorithm was designed, inspired by two main\\nthinking modes in philosophy, both based on dialectic concept and\\nthesis-antithesis paradigm. They impose two different kinds of dialectics.\\nIdealistic and materialistic antitheses are formulated as optimization models.\\nBased on the models, the population is coordinated for dialectical\\ninteractions. At the population-based context, the formulated optimization\\nmodels are reduced to a simple detection problem for each thinker (particle).\\nAccording to the assigned thinking mode to each thinker and her/his\\nmeasurements of corresponding dialectic with other candidate particles, they\\ndeterministically decide to interact with a thinker in maximum dialectic with\\ntheir theses. The position of a thinker at maximum dialectic is known as an\\navailable antithesis among the existing solutions. The dialectical interactions\\nat each ideological community are distinguished by meaningful distributions of\\nstep-sizes for each thinking mode. In fact, the thinking modes are regarded as\\nexploration and exploitation elements of the proposed algorithm. The result is\\na delicate balance without any requirement for adjustment of step-size\\ncoefficients. Main parameter of the proposed algorithm is the number of\\nparticles appointed to each thinking modes, or equivalently for each kind of\\nmotions. An additional integer parameter is defined to boost the stability of\\nthe final algorithm in some particular problems. The proposed algorithm is\\nevaluated by a testbed of 12 single-objective continuous benchmark functions.\\nMoreover, its performance and speed were highlighted in sparse reconstruction\\nand antenna selection problems, at the context of compressed sensing and\\nmassive MIMO, respectively. The results indicate fast and efficient performance\\nin comparison with well-known evolutionary algorithms and dedicated\\nstate-of-the-art algorithms.',\n",
              " 'ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural\\n  Projections\\nDeep neural networks have become ubiquitous for applications related to\\nvisual recognition and language understanding tasks. However, it is often\\nprohibitive to use typical neural networks on devices like mobile phones or\\nsmart watches since the model sizes are huge and cannot fit in the limited\\nmemory available on such devices. While these devices could make use of machine\\nlearning models running on high-performance data centers with CPUs or GPUs,\\nthis is not feasible for many applications because data can be privacy\\nsensitive and inference needs to be performed directly \"on\" device.\\n  We introduce a new architecture for training compact neural networks using a\\njoint optimization framework. At its core lies a novel objective that jointly\\ntrains using two different types of networks--a full trainer neural network\\n(using existing architectures like Feed-forward NNs or LSTM RNNs) combined with\\na simpler \"projection\" network that leverages random projections to transform\\ninputs or intermediate representations into bits. The simpler network encodes\\nlightweight and efficient-to-compute operations in bit space with a low memory\\nfootprint. The two networks are trained jointly using backpropagation, where\\nthe projection network learns from the full network similar to apprenticeship\\nlearning. Once trained, the smaller network can be used directly for inference\\nat low memory and computation cost. We demonstrate the effectiveness of the new\\napproach at significantly shrinking the memory requirements of different types\\nof neural networks while preserving good accuracy on visual recognition and\\ntext classification tasks. We also study the question \"how many neural bits are\\nrequired to solve a given task?\" using the new framework and show empirical\\nresults contrasting model predictive capacity (in bits) versus accuracy on\\nseveral datasets.',\n",
              " 'A Flow Model of Neural Networks\\nBased on a natural connection between ResNet and transport equation or its\\ncharacteristic equation, we propose a continuous flow model for both ResNet and\\nplain net. Through this continuous model, a ResNet can be explicitly\\nconstructed as a refinement of a plain net. The flow model provides an\\nalternative perspective to understand phenomena in deep neural networks, such\\nas why it is necessary and sufficient to use 2-layer blocks in ResNets, why\\ndeeper is better, and why ResNets are even deeper, and so on. It also opens a\\ngate to bring in more tools from the huge area of differential equations.',\n",
              " 'Multimodal Content Analysis for Effective Advertisements on YouTube\\nThe rapid advances in e-commerce and Web 2.0 technologies have greatly\\nincreased the impact of commercial advertisements on the general public. As a\\nkey enabling technology, a multitude of recommender systems exists which\\nanalyzes user features and browsing patterns to recommend appealing\\nadvertisements to users. In this work, we seek to study the characteristics or\\nattributes that characterize an effective advertisement and recommend a useful\\nset of features to aid the designing and production processes of commercial\\nadvertisements. We analyze the temporal patterns from multimedia content of\\nadvertisement videos including auditory, visual and textual components, and\\nstudy their individual roles and synergies in the success of an advertisement.\\nThe objective of this work is then to measure the effectiveness of an\\nadvertisement, and to recommend a useful set of features to advertisement\\ndesigners to make it more successful and approachable to users. Our proposed\\nframework employs the signal processing technique of cross modality feature\\nlearning where data streams from different components are employed to train\\nseparate neural network models and are then fused together to learn a shared\\nrepresentation. Subsequently, a neural network model trained on this joint\\nfeature embedding representation is utilized as a classifier to predict\\nadvertisement effectiveness. We validate our approach using subjective ratings\\nfrom a dedicated user study, the sentiment strength of online viewer comments,\\nand a viewer opinion metric of the ratio of the Likes and Views received by\\neach advertisement from an online platform.',\n",
              " 'Overcoming Exploration in Reinforcement Learning with Demonstrations\\nExploration in environments with sparse rewards has been a persistent problem\\nin reinforcement learning (RL). Many tasks are natural to specify with a sparse\\nreward, and manually shaping a reward function can result in suboptimal\\nperformance. However, finding a non-zero reward is exponentially more difficult\\nwith increasing task horizon or action dimensionality. This puts many\\nreal-world tasks out of practical reach of RL methods. In this work, we use\\ndemonstrations to overcome the exploration problem and successfully learn to\\nperform long-horizon, multi-step robotics tasks with continuous control such as\\nstacking blocks with a robot arm. Our method, which builds on top of Deep\\nDeterministic Policy Gradients and Hindsight Experience Replay, provides an\\norder of magnitude of speedup over RL on simulated robotics tasks. It is simple\\nto implement and makes only the additional assumption that we can collect a\\nsmall set of demonstrations. Furthermore, our method is able to solve tasks not\\nsolvable by either RL or behavior cloning alone, and often ends up\\noutperforming the demonstrator policy.',\n",
              " 'Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency\\n  for Sequence Modeling\\nRecurrent neural networks have shown remarkable success in modeling\\nsequences. However low resource situations still adversely affect the\\ngeneralizability of these models. We introduce a new family of models, called\\nLattice Recurrent Units (LRU), to address the challenge of learning deep\\nmulti-layer recurrent models with limited resources. LRU models achieve this\\ngoal by creating distinct (but coupled) flow of information inside the units: a\\nfirst flow along time dimension and a second flow along depth dimension. It\\nalso offers a symmetry in how information can flow horizontally and vertically.\\nWe analyze the effects of decoupling three different components of our LRU\\nmodel: Reset Gate, Update Gate and Projected State. We evaluate this family on\\nnew LRU models on computational convergence rates and statistical efficiency.\\nOur experiments are performed on four publicly-available datasets, comparing\\nwith Grid-LSTM and Recurrent Highway networks. Our results show that LRU has\\nbetter empirical computational convergence rates and statistical efficiency\\nvalues, along with learning more accurate language models.',\n",
              " 'Scalable Recollections for Continual Lifelong Learning\\nGiven the recent success of Deep Learning applied to a variety of single\\ntasks, it is natural to consider more human-realistic settings. Perhaps the\\nmost difficult of these settings is that of continual lifelong learning, where\\nthe model must learn online over a continuous stream of non-stationary data. A\\ncontinual lifelong learning system must have three primary capabilities to\\nsucceed: it must learn and adapt over time, it must not forget what it has\\nlearned, and it must be efficient in both training time and memory. Recent\\ntechniques have focused their efforts largely on the first two capabilities\\nwhile the third capability remains largely unexplored. In this paper, we\\nconsider the problem of efficient and effective storage of experiences over\\nvery large time-frames. In particular we consider the case where typical\\nexperiences are n bits and memories are limited to k bits for k << n. We\\npresent a novel scalable architecture and training algorithm in this\\nchallenging domain and provide an extensive evaluation of its performance. Our\\nresults show that we can achieve considerable gains on top of state-of-the-art\\nmethods such as GEM.',\n",
              " 'Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data\\nThe paper introduces the Hidden Tree Markov Network (HTN), a\\nneuro-probabilistic hybrid fusing the representation power of generative models\\nfor trees with the incremental and discriminative learning capabilities of\\nneural networks. We put forward a modular architecture in which multiple\\ngenerative models of limited complexity are trained to learn structural feature\\ndetectors whose outputs are then combined and integrated by neural layers at a\\nlater stage. In this respect, the model is both deep, thanks to the unfolding\\nof the generative models on the input structures, as well as wide, given the\\npotentially large number of generative modules that can be trained in parallel.\\nExperimental results show that the proposed approach can outperform\\nstate-of-the-art syntactic kernels as well as generative kernels built on the\\nsame probabilistic model as the HTN.',\n",
              " 'Hierarchical Actor-Critic\\nThe ability to learn at different resolutions in time may help overcome one\\nof the main challenges in deep reinforcement learning -- sample efficiency.\\nHierarchical agents that operate at different levels of temporal abstraction\\ncan learn tasks more quickly because they can divide the work of learning\\nbehaviors among multiple policies and can also explore the environment at a\\nhigher level. In this paper, we present a novel approach to hierarchical\\nreinforcement learning called Hierarchical Actor-Critic (HAC) that enables\\nagents to learn to break down problems involving continuous action spaces into\\nsimpler subproblems belonging to different time scales. HAC has two key\\nadvantages over most existing hierarchical learning methods: (i) the potential\\nfor faster learning as agents learn short policies at each level of the\\nhierarchy and (ii) an end-to-end approach. We demonstrate that HAC\\nsignificantly accelerates learning in a series of tasks that require behavior\\nover a relatively long time horizon and involve sparse rewards.',\n",
              " 'Proximodistal Exploration in Motor Learning as an Emergent Property of\\n  Optimization\\nTo harness the complexity of their high-dimensional bodies during\\nsensorimotor development, infants are guided by patterns of freezing and\\nfreeing of degrees of freedom. For instance, when learning to reach, infants\\nfree the degrees of freedom in their arm proximodistally, i.e. from joints that\\nare closer to the body to those that are more distant. Here, we formulate and\\nstudy computationally the hypothesis that such patterns can emerge\\nspontaneously as the result of a family of stochastic optimization processes\\n(evolution strategies with covariance-matrix adaptation), without an innate\\nencoding of a maturational schedule. In particular, we present simulated\\nexperiments with an arm where a computational learner progressively acquires\\nreaching skills through adaptive exploration, and we show that a proximodistal\\norganization appears spontaneously, which we denote PDFF (ProximoDistal\\nFreezing and Freeing of degrees of freedom). We also compare this emergent\\norganization between different arm morphologies -- from human-like to quite\\nunnatural ones -- to study the effect of different kinematic structures on the\\nemergence of PDFF. Keywords: human motor learning; proximo-distal exploration;\\nstochastic optimization; modelling; evolution strategies; cross-entropy\\nmethods; policy search; morphology.}',\n",
              " 'Null Dynamical State Models of Human Cognitive Dysfunction\\nThe hard problem in artificial intelligence asks how the shuffling of\\nsyntactical symbols in a program can lead to systems which experience semantics\\nand qualia. We address this question in three stages. First, we introduce a new\\nclass of human semantic symbols which appears when unexpected and drastic\\nenvironmental change causes humans to become surprised, confused, uncertain,\\nand in extreme cases, unresponsive, passive and dysfunctional. For this class\\nof symbols, pre-learned programs become inoperative so these syntactical\\nprograms cannot be the source of experienced qualia. Second, we model the\\ndysfunctional human response to a radically changed environment as being the\\nnatural response of any learning machine facing novel inputs from well outside\\nits previous training set. In this situation, learning machines are unable to\\nextract information from their input and will typically enter a dynamical state\\ncharacterized by null outputs and a lack of response. This state immediately\\npredicts and explains the characteristics of the semantic experiences of humans\\nin similar circumstances. In the third stage, we consider learning machines\\ntrained to implement multiple functions in simple sequential programs using\\nenvironmental data to specify subroutine names, control flow instructions,\\nmemory calls, and so on. Drastic change in any of these environmental inputs\\ncan again lead to inoperative programs. By examining changes specific to people\\nor locations we can model human cognitive symbols featuring these dependencies,\\nsuch as attachment and grief. Our approach links known dynamical machines\\nstates with human qualia and thus offers new insight into the hard problem of\\nartificial intelligence.',\n",
              " \"Accelerating Deep Learning with Memcomputing\\nRestricted Boltzmann machines (RBMs) and their extensions, called\\n'deep-belief networks', are powerful neural networks that have found\\napplications in the fields of machine learning and big data. The standard way\\nto training these models resorts to an iterative unsupervised procedure based\\non Gibbs sampling, called 'contrastive divergence' (CD), and additional\\nsupervised tuning via back-propagation. However, this procedure has been shown\\nnot to follow any gradient and can lead to suboptimal solutions. In this paper,\\nwe show an efficient alternative to CD by means of simulations of digital\\nmemcomputing machines (DMMs). We test our approach on pattern recognition using\\na modified version of the MNIST data set. DMMs sample effectively the vast\\nphase space given by the model distribution of the RBM, and provide a very good\\napproximation close to the optimum. This efficient search significantly reduces\\nthe number of pretraining iterations necessary to achieve a given level of\\naccuracy, as well as a total performance gain over CD. In fact, the\\nacceleration of pretraining achieved by simulating DMMs is comparable to, in\\nnumber of iterations, the recently reported hardware application of the quantum\\nannealing method on the same network and data set. Notably, however, DMMs\\nperform far better than the reported quantum annealing results in terms of\\nquality of the training. We also compare our method to advances in supervised\\ntraining, like batch-normalization and rectifiers, that work to reduce the\\nadvantage of pretraining. We find that the memcomputing method still maintains\\na quality advantage ($>1\\\\%$ in accuracy, and a $20\\\\%$ reduction in error rate)\\nover these approaches. Furthermore, our method is agnostic about the\\nconnectivity of the network. Therefore, it can be extended to train full\\nBoltzmann machines, and even deep networks at once.\",\n",
              " 'mvn2vec: Preservation and Collaboration in Multi-View Network Embedding\\nMulti-view networks are ubiquitous in real-world applications. In order to\\nextract knowledge or business value, it is of interest to transform such\\nnetworks into representations that are easily machine-actionable. Meanwhile,\\nnetwork embedding has emerged as an effective approach to generate distributed\\nnetwork representations. Therefore, we are motivated to study the problem of\\nmulti-view network embedding, with a focus on the characteristics that are\\nspecific and important in embedding this type of networks. In our practice of\\nembedding real-world multi-view networks, we identify two such characteristics,\\nwhich we refer to as preservation and collaboration. We then explore the\\nfeasibility of achieving better embedding quality by simultaneously modeling\\npreservation and collaboration, and propose the mvn2vec algorithms. With\\nexperiments on a series of synthetic datasets, an internal Snapchat dataset,\\nand two public datasets, we further confirm the presence and importance of\\npreservation and collaboration. These experiments also demonstrate that better\\nembedding can be obtained by simultaneously modeling the two characteristics,\\nwhile not over-complicating the model or requiring additional supervision.',\n",
              " \"Granger-causal Attentive Mixtures of Experts\\nSeveral methods have recently been proposed to detect salient input features\\nfor outputs of neural networks. Those methods offer a qualitative glimpse at\\nfeature importance, but they fall short of providing quantifiable attributions\\nthat can be compared across decisions and measures of the expected quality of\\ntheir explanations. To address these shortcomings, we present an attentive\\nmixture of experts (AME) that couples attentive gating with a Granger-causal\\nobjective to jointly produce accurate predictions as well as measures of\\nfeature importance. We demonstrate the utility of AMEs by determining factors\\ndriving demand for medical prescriptions, comparing predictive features for\\nParkinson's disease and pinpointing discriminatory genes across cancer types.\",\n",
              " 'Memorize or generalize? Searching for a compositional RNN in a haystack\\nNeural networks are very powerful learning systems, but they do not readily\\ngeneralize from one task to the other. This is partly due to the fact that they\\ndo not learn in a compositional way, that is, by discovering skills that are\\nshared by different tasks, and recombining them to solve new problems. In this\\npaper, we explore the compositional generalization capabilities of recurrent\\nneural networks (RNNs). We first propose the lookup table composition domain as\\na simple setup to test compositional behaviour and show that it is\\ntheoretically possible for a standard RNN to learn to behave compositionally in\\nthis domain when trained with standard gradient descent and provided with\\nadditional supervision. We then remove this additional supervision and perform\\na search over a large number of model initializations to investigate the\\nproportion of RNNs that can still converge to a compositional solution. We\\ndiscover that a small but non-negligible proportion of RNNs do reach partial\\ncompositional solutions even without special architectural constraints. This\\nsuggests that a combination of gradient descent and evolutionary strategies\\ndirectly favouring the minority models that developed more compositional\\napproaches might suffice to lead standard RNNs towards compositional solutions.',\n",
              " 'Continual Reinforcement Learning with Complex Synapses\\nUnlike humans, who are capable of continual learning over their lifetimes,\\nartificial neural networks have long been known to suffer from a phenomenon\\nknown as catastrophic forgetting, whereby new learning can lead to abrupt\\nerasure of previously acquired knowledge. Whereas in a neural network the\\nparameters are typically modelled as scalar values, an individual synapse in\\nthe brain comprises a complex network of interacting biochemical components\\nthat evolve at different timescales. In this paper, we show that by equipping\\ntabular and deep reinforcement learning agents with a synaptic model that\\nincorporates this biological complexity (Benna & Fusi, 2016), catastrophic\\nforgetting can be mitigated at multiple timescales. In particular, we find that\\nas well as enabling continual learning across sequential training of two simple\\ntasks, it can also be used to overcome within-task forgetting by reducing the\\nneed for an experience replay database.',\n",
              " 'Meta-Reinforcement Learning of Structured Exploration Strategies\\nExploration is a fundamental challenge in reinforcement learning (RL). Many\\nof the current exploration methods for deep RL use task-agnostic objectives,\\nsuch as information gain or bonuses based on state visitation. However, many\\npractical applications of RL involve learning more than a single task, and\\nprior tasks can be used to inform how exploration should be performed in new\\ntasks. In this work, we explore how prior tasks can inform an agent about how\\nto explore effectively in new situations. We introduce a novel gradient-based\\nfast adaptation algorithm -- model agnostic exploration with structured noise\\n(MAESN) -- to learn exploration strategies from prior experience. The prior\\nexperience is used both to initialize a policy and to acquire a latent\\nexploration space that can inject structured stochasticity into a policy,\\nproducing exploration strategies that are informed by prior knowledge and are\\nmore effective than random action-space noise. We show that MAESN is more\\neffective at learning exploration strategies when compared to prior meta-RL\\nmethods, RL without learned exploration strategies, and task-agnostic\\nexploration methods. We evaluate our method on a variety of simulated tasks:\\nlocomotion with a wheeled robot, locomotion with a quadrupedal walker, and\\nobject manipulation.',\n",
              " 'Approximation Algorithms for Cascading Prediction Models\\nWe present an approximation algorithm that takes a pool of pre-trained models\\nas input and produces from it a cascaded model with similar accuracy but lower\\naverage-case cost. Applied to state-of-the-art ImageNet classification models,\\nthis yields up to a 2x reduction in floating point multiplications, and up to a\\n6x reduction in average-case memory I/O. The auto-generated cascades exhibit\\nintuitive properties, such as using lower-resolution input for easier images\\nand requiring higher prediction confidence when using a computationally cheaper\\nmodel.',\n",
              " 'Coloring black boxes: visualization of neural network decisions\\nNeural networks are commonly regarded as black boxes performing\\nincomprehensible functions. For classification problems networks provide maps\\nfrom high dimensional feature space to K-dimensional image space. Images of\\ntraining vector are projected on polygon vertices, providing visualization of\\nnetwork function. Such visualization may show the dynamics of learning, allow\\nfor comparison of different networks, display training vectors around which\\npotential problems may arise, show differences due to regularization and\\noptimization procedures, investigate stability of network classification under\\nperturbation of original vectors, and place new data sample in relation to\\ntraining data, allowing for estimation of confidence in classification of a\\ngiven sample. An illustrative example for the three-class Wine data and\\nfive-class Satimage data is described. The visualization method proposed here\\nis applicable to any black box system that provides continuous outputs.',\n",
              " 'Relational Neural Expectation Maximization: Unsupervised Discovery of\\n  Objects and their Interactions\\nCommon-sense physical reasoning is an essential ingredient for any\\nintelligent agent operating in the real-world. For example, it can be used to\\nsimulate the environment, or to infer the state of parts of the world that are\\ncurrently unobserved. In order to match real-world conditions this causal\\nknowledge must be learned without access to supervised data. To address this\\nproblem we present a novel method that learns to discover objects and model\\ntheir physical interactions from raw visual images in a purely\\n\\\\emph{unsupervised} fashion. It incorporates prior knowledge about the\\ncompositional nature of human perception to factor interactions between\\nobject-pairs and learn efficiently. On videos of bouncing balls we show the\\nsuperior modelling capabilities of our method compared to other unsupervised\\nneural approaches that do not incorporate such prior knowledge. We demonstrate\\nits ability to handle occlusion and show that it can extrapolate learned\\nknowledge to scenes with different numbers of objects.',\n",
              " 'A Bayesian Model for Activities Recommendation and Event Structure\\n  Optimization Using Visitors Tracking\\nIn events that are composed by many activities, there is a problem that\\ninvolves retrieve and management the information of visitors that are visiting\\nthe activities. This management is crucial to find some activities that are\\ndrawing attention of visitors; identify an ideal positioning for activities;\\nwhich path is more frequented by visitors. In this work, these features are\\nstudied using Complex Network theory. For the beginning, an artificial database\\nwas generated to study the mentioned features. Secondly, this work shows a\\nmethod to optimize the event structure that is better than a random method and\\na recommendation system that achieves ~95% of accuracy.',\n",
              " 'The Lottery Ticket Hypothesis: Training Pruned Neural Networks\\nRecent work on neural network pruning indicates that, at training time,\\nneural networks need to be significantly larger in size than is necessary to\\nrepresent the eventual functions that they learn. This paper articulates a new\\nhypothesis to explain this phenomenon. This conjecture, which we term the\\n\"lottery ticket hypothesis,\" proposes that successful training depends on lucky\\nrandom initialization of a smaller subcomponent of the network. Larger networks\\nhave more of these \"lottery tickets,\" meaning they are more likely to luck out\\nwith a subcomponent initialized in a configuration amenable to successful\\noptimization.\\n  This paper conducts a series of experiments with XOR and MNIST that support\\nthe lottery ticket hypothesis. In particular, we identify these\\nfortuitously-initialized subcomponents by pruning low-magnitude weights from\\ntrained networks. We then demonstrate that these subcomponents can be\\nsuccessfully retrained in isolation so long as the subnetworks are given the\\nsame initializations as they had at the beginning of the training process.\\nInitialized as such, these small networks reliably converge successfully, often\\nfaster than the original network at the same level of accuracy. However, when\\nthese subcomponents are randomly reinitialized or rearranged, they perform\\nworse than the original network. In other words, large networks that train\\nsuccessfully contain small subnetworks with initializations conducive to\\noptimization.\\n  The lottery ticket hypothesis and its connection to pruning are a step toward\\ndeveloping architectures, initializations, and training strategies that make it\\npossible to solve the same problems with much smaller networks.',\n",
              " 'Learning recurrent dynamics in spiking networks\\nSpiking activity of neurons engaged in learning and performing a task show\\ncomplex spatiotemporal dynamics. While the output of recurrent network models\\ncan learn to perform various tasks, the possible range of recurrent dynamics\\nthat emerge after learning remains unknown. Here we show that modifying the\\nrecurrent connectivity with a recursive least squares algorithm provides\\nsufficient flexibility for synaptic and spiking rate dynamics of spiking\\nnetworks to produce a wide range of spatiotemporal activity. We apply the\\ntraining method to learn arbitrary firing patterns, stabilize irregular spiking\\nactivity of a balanced network, and reproduce the heterogeneous spiking rate\\npatterns of cortical neurons engaged in motor planning and movement. We\\nidentify sufficient conditions for successful learning, characterize two types\\nof learning errors, and assess the network capacity. Our findings show that\\nsynaptically-coupled recurrent spiking networks possess a vast computational\\ncapability that can support the diverse activity patterns in the brain.',\n",
              " \"Principal Graphs and Manifolds\\nIn many physical, statistical, biological and other investigations it is\\ndesirable to approximate a system of points by objects of lower dimension\\nand/or complexity. For this purpose, Karl Pearson invented principal component\\nanalysis in 1901 and found 'lines and planes of closest fit to system of\\npoints'. The famous k-means algorithm solves the approximation problem too, but\\nby finite sets instead of lines and planes. This chapter gives a brief\\npractical introduction into the methods of construction of general principal\\nobjects, i.e. objects embedded in the 'middle' of the multidimensional data\\nset. As a basis, the unifying framework of mean squared distance approximation\\nof finite datasets is selected. Principal graphs and manifolds are constructed\\nas generalisations of principal components and k-means principal points. For\\nthis purpose, the family of expectation/maximisation algorithms with nearest\\ngeneralisations is presented. Construction of principal graphs with controlled\\ncomplexity is based on the graph grammar approach.\",\n",
              " 'Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint\\nDeep Belief Networks (DBN) have been successfully applied on popular machine\\nlearning tasks. Specifically, when applied on hand-written digit recognition,\\nDBNs have achieved approximate accuracy rates of 98.8%. In an effort to\\noptimize the data representation achieved by the DBN and maximize their\\ndescriptive power, recent advances have focused on inducing sparse constraints\\nat each layer of the DBN. In this paper we present a theoretical approach for\\nsparse constraints in the DBN using the mixed norm for both non-overlapping and\\noverlapping groups. We explore how these constraints affect the classification\\naccuracy for digit recognition in three different datasets (MNIST, USPS, RIMES)\\nand provide initial estimations of their usefulness by altering different\\nparameters such as the group size and overlap percentage.',\n",
              " 'Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary\\n  Independent Stochastic Neurons\\nIn this paper, a simple, general method of adding auxiliary stochastic\\nneurons to a multi-layer perceptron is proposed. It is shown that the proposed\\nmethod is a generalization of recently successful methods of dropout (Hinton et\\nal., 2012), explicit noise injection (Vincent et al., 2010; Bishop, 1995) and\\nsemantic hashing (Salakhutdinov & Hinton, 2009). Under the proposed framework,\\nan extension of dropout which allows using separate dropping probabilities for\\ndifferent hidden neurons, or layers, is found to be available. The use of\\ndifferent dropping probabilities for hidden layers separately is empirically\\ninvestigated.',\n",
              " 'Locally Imposing Function for Generalized Constraint Neural Networks - A\\n  Study on Equality Constraints\\nThis work is a further study on the Generalized Constraint Neural Network\\n(GCNN) model [1], [2]. Two challenges are encountered in the study, that is, to\\nembed any type of prior information and to select its imposing schemes. The\\nwork focuses on the second challenge and studies a new constraint imposing\\nscheme for equality constraints. A new method called locally imposing function\\n(LIF) is proposed to provide a local correction to the GCNN prediction\\nfunction, which therefore falls within Locally Imposing Scheme (LIS). In\\ncomparison, the conventional Lagrange multiplier method is considered as\\nGlobally Imposing Scheme (GIS) because its added constraint term exhibits a\\nglobal impact to its objective function. Two advantages are gained from LIS\\nover GIS. First, LIS enables constraints to fire locally and explicitly in the\\ndomain only where they need on the prediction function. Second, constraints can\\nbe implemented within a network setting directly. We attempt to interpret\\nseveral constraint methods graphically from a viewpoint of the locality\\nprinciple. Numerical examples confirm the advantages of the proposed method. In\\nsolving boundary value problems with Dirichlet and Neumann constraints, the\\nGCNN model with LIF is possible to achieve an exact satisfaction of the\\nconstraints.',\n",
              " 'Evolution of Covariance Functions for Gaussian Process Regression using\\n  Genetic Programming\\nIn this contribution we describe an approach to evolve composite covariance\\nfunctions for Gaussian processes using genetic programming. A critical aspect\\nof Gaussian processes and similar kernel-based models such as SVM is, that the\\ncovariance function should be adapted to the modeled data. Frequently, the\\nsquared exponential covariance function is used as a default. However, this can\\nlead to a misspecified model, which does not fit the data well. In the proposed\\napproach we use a grammar for the composition of covariance functions and\\ngenetic programming to search over the space of sentences that can be derived\\nfrom the grammar. We tested the proposed approach on synthetic data from\\ntwo-dimensional test functions, and on the Mauna Loa CO2 time series. The\\nresults show, that our approach is feasible, finding covariance functions that\\nperform much better than a default covariance function. For the CO2 data set a\\ncomposite covariance function is found, that matches the performance of a\\nhand-tuned covariance function.',\n",
              " \"Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image\\n  Statistics\\nWe present a theoretical analysis of Gaussian-binary restricted Boltzmann\\nmachines (GRBMs) from the perspective of density models. The key aspect of this\\nanalysis is to show that GRBMs can be formulated as a constrained mixture of\\nGaussians, which gives a much better insight into the model's capabilities and\\nlimitations. We show that GRBMs are capable of learning meaningful features\\nboth in a two-dimensional blind source separation task and in modeling natural\\nimages. Further, we show that reported difficulties in training GRBMs are due\\nto the failure of the training algorithm rather than the model itself. Based on\\nour analysis we are able to propose several training recipes, which allowed\\nsuccessful and fast training in our experiments. Finally, we discuss the\\nrelationship of GRBMs to several modifications that have been proposed to\\nimprove the model.\",\n",
              " 'Training Restricted Boltzmann Machine by Perturbation\\nA new approach to maximum likelihood learning of discrete graphical models\\nand RBM in particular is introduced. Our method, Perturb and Descend (PD) is\\ninspired by two ideas (I) perturb and MAP method for sampling (II) learning by\\nContrastive Divergence minimization. In contrast to perturb and MAP, PD\\nleverages training data to learn the models that do not allow efficient MAP\\nestimation. During the learning, to produce a sample from the current model, we\\nstart from a training data and descend in the energy landscape of the\\n\"perturbed model\", for a fixed number of steps, or until a local optima is\\nreached. For RBM, this involves linear calculations and thresholding which can\\nbe very fast. Furthermore we show that the amount of perturbation is closely\\nrelated to the temperature parameter and it can regularize the model by\\nproducing robust features resulting in sparse hidden layer activation.',\n",
              " 'Multilayer bootstrap networks\\nMultilayer bootstrap network builds a gradually narrowed multilayer nonlinear\\nnetwork from bottom up for unsupervised nonlinear dimensionality reduction.\\nEach layer of the network is a nonparametric density estimator. It consists of\\na group of k-centroids clusterings. Each clustering randomly selects data\\npoints with randomly selected features as its centroids, and learns a one-hot\\nencoder by one-nearest-neighbor optimization. Geometrically, the nonparametric\\ndensity estimator at each layer projects the input data space to a\\nuniformly-distributed discrete feature space, where the similarity of two data\\npoints in the discrete feature space is measured by the number of the nearest\\ncentroids they share in common. The multilayer network gradually reduces the\\nnonlinear variations of data from bottom up by building a vast number of\\nhierarchical trees implicitly on the original data space. Theoretically, the\\nestimation error caused by the nonparametric density estimator is proportional\\nto the correlation between the clusterings, both of which are reduced by the\\nrandomization steps.',\n",
              " 'Invariant backpropagation: how to train a transformation-invariant\\n  neural network\\nIn many classification problems a classifier should be robust to small\\nvariations in the input vector. This is a desired property not only for\\nparticular transformations, such as translation and rotation in image\\nclassification problems, but also for all others for which the change is small\\nenough to retain the object perceptually indistinguishable. We propose two\\nextensions of the backpropagation algorithm that train a neural network to be\\nrobust to variations in the feature vector. While the first of them enforces\\nrobustness of the loss function to all variations, the second method trains the\\npredictions to be robust to a particular variation which changes the loss\\nfunction the most. The second methods demonstrates better results, but is\\nslightly slower. We analytically compare the proposed algorithm with two the\\nmost similar approaches (Tangent BP and Adversarial Training), and propose\\ntheir fast versions. In the experimental part we perform comparison of all\\nalgorithms in terms of classification accuracy and robustness to noise on MNIST\\nand CIFAR-10 datasets. Additionally we analyze how the performance of the\\nproposed algorithm depends on the dataset size and data augmentation.',\n",
              " 'Shared latent subspace modelling within Gaussian-Binary Restricted\\n  Boltzmann Machines for NIST i-Vector Challenge 2014\\nThis paper presents a novel approach to speaker subspace modelling based on\\nGaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is\\nbased on the idea of shared factors as in the Probabilistic Linear Discriminant\\nAnalysis (PLDA). GRBM hidden layer is divided into speaker and channel factors,\\nherein the speaker factor is shared over all vectors of the speaker. Then\\nMaximum Likelihood Parameter Estimation (MLE) for proposed model is introduced.\\nVarious new scoring techniques for speaker verification using GRBM are\\nproposed. The results for NIST i-vector Challenge 2014 dataset are presented.',\n",
              " 'A Neural Transfer Function for a Smooth and Differentiable Transition\\n  Between Additive and Multiplicative Interactions\\nExisting approaches to combine both additive and multiplicative neural units\\neither use a fixed assignment of operations or require discrete optimization to\\ndetermine what function a neuron should perform. This leads either to an\\ninefficient distribution of computational resources or an extensive increase in\\nthe computational complexity of the training procedure.\\n  We present a novel, parameterizable transfer function based on the\\nmathematical concept of non-integer functional iteration that allows the\\noperation each neuron performs to be smoothly and, most importantly,\\ndifferentiablely adjusted between addition and multiplication. This allows the\\ndecision between addition and multiplication to be integrated into the standard\\nbackpropagation training procedure.',\n",
              " 'A Probabilistic Framework for Deep Learning\\nWe develop a probabilistic framework for deep learning based on the Deep\\nRendering Mixture Model (DRMM), a new generative probabilistic model that\\nexplicitly capture variations in data due to latent task nuisance variables. We\\ndemonstrate that max-sum inference in the DRMM yields an algorithm that exactly\\nreproduces the operations in deep convolutional neural networks (DCNs),\\nproviding a first principles derivation. Our framework provides new insights\\ninto the successes and shortcomings of DCNs as well as a principled route to\\ntheir improvement. DRMM training via the Expectation-Maximization (EM)\\nalgorithm is a powerful alternative to DCN back-propagation, and initial\\ntraining results are promising. Classification based on the DRMM and other\\nvariants outperforms DCNs in supervised digit classification, training 2-3x\\nfaster while achieving similar accuracy. Moreover, the DRMM is applicable to\\nsemi-supervised and unsupervised learning tasks, achieving results that are\\nstate-of-the-art in several categories on the MNIST benchmark and comparable to\\nstate of the art on the CIFAR10 benchmark.',\n",
              " 'Neurogenesis Deep Learning\\nNeural machine learning methods, such as deep neural networks (DNN), have\\nachieved remarkable success in a number of complex data processing tasks. These\\nmethods have arguably had their strongest impact on tasks such as image and\\naudio processing - data processing domains in which humans have long held clear\\nadvantages over conventional algorithms. In contrast to biological neural\\nsystems, which are capable of learning continuously, deep artificial networks\\nhave a limited ability for incorporating new information in an already trained\\nnetwork. As a result, methods for continuous learning are potentially highly\\nimpactful in enabling the application of deep networks to dynamic data sets.\\nHere, inspired by the process of adult neurogenesis in the hippocampus, we\\nexplore the potential for adding new neurons to deep layers of artificial\\nneural networks in order to facilitate their acquisition of novel information\\nwhile preserving previously trained data representations. Our results on the\\nMNIST handwritten digit dataset and the NIST SD 19 dataset, which includes\\nlower and upper case letters and digits, demonstrate that neurogenesis is well\\nsuited for addressing the stability-plasticity dilemma that has long challenged\\nadaptive machine learning algorithms.',\n",
              " \"Deep learning for neuroimaging: a validation study\\nDeep learning methods have recently made notable advances in the tasks of\\nclassification and representation learning. These tasks are important for brain\\nimaging and neuroscience discovery, making the methods attractive for porting\\nto a neuroimager's toolbox. Success of these methods is, in part, explained by\\nthe flexibility of deep learning models. However, this flexibility makes the\\nprocess of porting to new areas a difficult parameter optimization problem. In\\nthis work we demonstrate our results (and feasible parameter ranges) in\\napplication of deep learning methods to structural and functional brain imaging\\ndata. We also describe a novel constraint-based approach to visualizing high\\ndimensional data. We use it to analyze the effect of parameter choices on data\\ntransformations. Our results show that deep learning methods are able to learn\\nphysiologically important representations and detect latent relations in\\nneuroimaging data.\",\n",
              " 'Improving Deep Neural Networks with Probabilistic Maxout Units\\nWe present a probabilistic variant of the recently introduced maxout unit.\\nThe success of deep neural networks utilizing maxout can partly be attributed\\nto favorable performance under dropout, when compared to rectified linear\\nunits. It however also depends on the fact that each maxout unit performs a\\npooling operation over a group of linear transformations and is thus partially\\ninvariant to changes in its input. Starting from this observation we ask the\\nquestion: Can the desirable properties of maxout units be preserved while\\nimproving their invariance properties ? We argue that our probabilistic maxout\\n(probout) units successfully achieve this balance. We quantitatively verify\\nthis claim and report classification performance matching or exceeding the\\ncurrent state of the art on three challenging image classification benchmarks\\n(CIFAR-10, CIFAR-100 and SVHN).',\n",
              " 'How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need?\\nIn numerous applicative contexts, data are too rich and too complex to be\\nrepresented by numerical vectors. A general approach to extend machine learning\\nand data mining techniques to such data is to really on a dissimilarity or on a\\nkernel that measures how different or similar two objects are. This approach\\nhas been used to define several variants of the Self Organizing Map (SOM). This\\npaper reviews those variants in using a common set of notations in order to\\noutline differences and similarities between them. It discusses the advantages\\nand drawbacks of the variants, as well as the actual relevance of the\\ndissimilarity/kernel SOM for practical applications.',\n",
              " 'Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures\\nModel-based methods and deep neural networks have both been tremendously\\nsuccessful paradigms in machine learning. In model-based methods, problem\\ndomain knowledge can be built into the constraints of the model, typically at\\nthe expense of difficulties during inference. In contrast, deterministic deep\\nneural networks are constructed in such a way that inference is\\nstraightforward, but their architectures are generic and it is unclear how to\\nincorporate knowledge. This work aims to obtain the advantages of both\\napproaches. To do so, we start with a model-based approach and an associated\\ninference algorithm, and \\\\emph{unfold} the inference iterations as layers in a\\ndeep network. Rather than optimizing the original model, we \\\\emph{untie} the\\nmodel parameters across layers, in order to create a more powerful network. The\\nresulting architecture can be trained discriminatively to perform accurate\\ninference within a fixed network size. We show how this framework allows us to\\ninterpret conventional networks as mean-field inference in Markov random\\nfields, and to obtain new architectures by instead using belief propagation as\\nthe inference algorithm. We then show its application to a non-negative matrix\\nfactorization model that incorporates the problem-domain knowledge that sound\\nsources are additive. Deep unfolding of this model yields a new kind of\\nnon-negative deep neural network, that can be trained using a multiplicative\\nbackpropagation-style update algorithm. We present speech enhancement\\nexperiments showing that our approach is competitive with conventional neural\\nnetworks despite using far fewer parameters.',\n",
              " 'Learning deep dynamical models from image pixels\\nModeling dynamical systems is important in many disciplines, e.g., control,\\nrobotics, or neurotechnology. Commonly the state of these systems is not\\ndirectly observed, but only available through noisy and potentially\\nhigh-dimensional observations. In these cases, system identification, i.e.,\\nfinding the measurement mapping and the transition mapping (system dynamics) in\\nlatent space can be challenging. For linear system dynamics and measurement\\nmappings efficient solutions for system identification are available. However,\\nin practical applications, the linearity assumptions does not hold, requiring\\nnon-linear system identification techniques. If additionally the observations\\nare high-dimensional (e.g., images), non-linear system identification is\\ninherently hard. To address the problem of non-linear system identification\\nfrom high-dimensional observations, we combine recent advances in deep learning\\nand system identification. In particular, we jointly learn a low-dimensional\\nembedding of the observation by means of deep auto-encoders and a predictive\\ntransition model in this low-dimensional space. We demonstrate that our model\\nenables learning good predictive models of dynamical systems from pixel\\ninformation only.',\n",
              " 'From neural PCA to deep unsupervised learning\\nA network supporting deep unsupervised learning is presented. The network is\\nan autoencoder with lateral shortcut connections from the encoder to decoder at\\neach level of the hierarchy. The lateral shortcut connections allow the higher\\nlevels of the hierarchy to focus on abstract invariant features. While standard\\nautoencoders are analogous to latent variable models with a single layer of\\nstochastic variables, the proposed network is analogous to hierarchical latent\\nvariables models. Learning combines denoising autoencoder and denoising sources\\nseparation frameworks. Each layer of the network contributes to the cost\\nfunction a term which measures the distance of the representations produced by\\nthe encoder and the decoder. Since training signals originate from all levels\\nof the network, all layers can learn efficiently even in deep networks. The\\nspeedup offered by cost terms from higher levels of the hierarchy and the\\nability to learn invariant features are demonstrated in experiments.',\n",
              " 'Qualitatively characterizing neural network optimization problems\\nTraining neural networks involves solving large-scale non-convex optimization\\nproblems. This task has long been believed to be extremely difficult, with fear\\nof local minima and other obstacles motivating a variety of schemes to improve\\noptimization, such as unsupervised pretraining. However, modern neural networks\\nare able to achieve negligible training error on complex tasks, using only\\ndirect training with stochastic gradient descent. We introduce a simple\\nanalysis technique to look for evidence that such networks are overcoming local\\noptima. We find that, in fact, on a straight path from initialization to\\nsolution, a variety of state of the art neural networks never encounter any\\nsignificant obstacles.',\n",
              " 'Why does Deep Learning work? - A perspective from Group Theory\\nWhy does Deep Learning work? What representations does it capture? How do\\nhigher-order representations emerge? We study these questions from the\\nperspective of group theory, thereby opening a new approach towards a theory of\\nDeep learning.\\n  One factor behind the recent resurgence of the subject is a key algorithmic\\nstep called pre-training: first search for a good generative model for the\\ninput samples, and repeat the process one layer at a time. We show deeper\\nimplications of this simple principle, by establishing a connection with the\\ninterplay of orbits and stabilizers of group actions. Although the neural\\nnetworks themselves may not form groups, we show the existence of {\\\\em shadow}\\ngroups whose elements serve as close approximations.\\n  Over the shadow groups, the pre-training step, originally introduced as a\\nmechanism to better initialize a network, becomes equivalent to a search for\\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\\\em\\nsimplest}. Which explains why a deep learning network learns simple features\\nfirst. Next, we show how the same principle, when repeated in the deeper\\nlayers, can capture higher order representations, and why representation\\ncomplexity increases as the layers get deeper.',\n",
              " 'ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient\\nStochastic gradient algorithms have been the main focus of large-scale\\nlearning problems and they led to important successes in machine learning. The\\nconvergence of SGD depends on the careful choice of learning rate and the\\namount of the noise in stochastic estimates of the gradients. In this paper, we\\npropose a new adaptive learning rate algorithm, which utilizes curvature\\ninformation for automatically tuning the learning rates. The information about\\nthe element-wise curvature of the loss function is estimated from the local\\nstatistics of the stochastic first order gradients. We further propose a new\\nvariance reduction technique to speed up the convergence. In our preliminary\\nexperiments with deep neural networks, we obtained better performance compared\\nto the popular stochastic gradient algorithms.',\n",
              " 'A Unified Perspective on Multi-Domain and Multi-Task Learning\\nIn this paper, we provide a new neural-network based perspective on\\nmulti-task learning (MTL) and multi-domain learning (MDL). By introducing the\\nconcept of a semantic descriptor, this framework unifies MDL and MTL as well as\\nencompassing various classic and recent MTL/MDL algorithms by interpreting them\\nas different ways of constructing semantic descriptors. Our interpretation\\nprovides an alternative pipeline for zero-shot learning (ZSL), where a model\\nfor a novel class can be constructed without training data. Moreover, it leads\\nto a new and practically relevant problem setting of zero-shot domain\\nadaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model\\nfor an unseen domain can be generated by its semantic descriptor. Experiments\\nacross this range of problems demonstrate that our framework outperforms a\\nvariety of alternatives.',\n",
              " 'A Neural Network Anomaly Detector Using the Random Cluster Model\\nThe random cluster model is used to define an upper bound on a distance\\nmeasure as a function of the number of data points to be classified and the\\nexpected value of the number of classes to form in a hybrid K-means and\\nregression classification methodology, with the intent of detecting anomalies.\\nConditions are given for the identification of classes which contain anomalies\\nand individual anomalies within identified classes. A neural network model\\ndescribes the decision region-separating surface for offline storage and recall\\nin any new anomaly detection.',\n",
              " 'A Group Theoretic Perspective on Unsupervised Deep Learning\\nWhy does Deep Learning work? What representations does it capture? How do\\nhigher-order representations emerge? We study these questions from the\\nperspective of group theory, thereby opening a new approach towards a theory of\\nDeep learning.\\n  One factor behind the recent resurgence of the subject is a key algorithmic\\nstep called {\\\\em pretraining}: first search for a good generative model for the\\ninput samples, and repeat the process one layer at a time. We show deeper\\nimplications of this simple principle, by establishing a connection with the\\ninterplay of orbits and stabilizers of group actions. Although the neural\\nnetworks themselves may not form groups, we show the existence of {\\\\em shadow}\\ngroups whose elements serve as close approximations.\\n  Over the shadow groups, the pre-training step, originally introduced as a\\nmechanism to better initialize a network, becomes equivalent to a search for\\nfeatures with minimal orbits. Intuitively, these features are in a way the {\\\\em\\nsimplest}. Which explains why a deep learning network learns simple features\\nfirst. Next, we show how the same principle, when repeated in the deeper\\nlayers, can capture higher order representations, and why representation\\ncomplexity increases as the layers get deeper.',\n",
              " 'A Generative Model for Deep Convolutional Learning\\nA generative model is developed for deep (multi-layered) convolutional\\ndictionary learning. A novel probabilistic pooling operation is integrated into\\nthe deep model, yielding efficient bottom-up (pretraining) and top-down\\n(refinement) probabilistic learning. Experimental results demonstrate powerful\\ncapabilities of the model to learn multi-layer features from images, and\\nexcellent classification results are obtained on the MNIST and Caltech 101\\ndatasets.',\n",
              " 'Knowledge Transfer Pre-training\\nPre-training is crucial for learning deep neural networks. Most of existing\\npre-training methods train simple models (e.g., restricted Boltzmann machines)\\nand then stack them layer by layer to form the deep structure. This layer-wise\\npre-training has found strong theoretical foundation and broad empirical\\nsupport. However, it is not easy to employ such method to pre-train models\\nwithout a clear multi-layer structure,e.g., recurrent neural networks (RNNs).\\nThis paper presents a new pre-training approach based on knowledge transfer\\nlearning. In contrast to the layer-wise approach which trains model components\\nincrementally, the new approach trains the entire model as a whole but with an\\neasier objective function. This is achieved by utilizing soft targets produced\\nby a prior trained model (teacher model). Compared to the conventional\\nlayer-wise methods, this new method does not care about the model structure, so\\ncan be used to pre-train very complex models. Experiments on a speech\\nrecognition task demonstrated that with this approach, complex RNNs can be well\\ntrained with a weaker deep neural network (DNN) model. Furthermore, the new\\nmethod can be combined with conventional layer-wise pre-training to deliver\\nadditional gains.',\n",
              " 'Stacked What-Where Auto-encoders\\nWe present a novel architecture, the \"stacked what-where auto-encoders\"\\n(SWWAE), which integrates discriminative and generative pathways and provides a\\nunified approach to supervised, semi-supervised and unsupervised learning\\nwithout relying on sampling during training. An instantiation of SWWAE uses a\\nconvolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and\\nemploys a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the\\nreconstruction. The objective function includes reconstruction terms that\\ninduce the hidden states in the Deconvnet to be similar to those of the\\nConvnet. Each pooling layer produces two sets of variables: the \"what\" which\\nare fed to the next layer, and its complementary variable \"where\" that are fed\\nto the corresponding layer in the generative decoder.',\n",
              " 'Training recurrent networks online without backtracking\\nWe introduce the \"NoBackTrack\" algorithm to train the parameters of dynamical\\nsystems such as recurrent neural networks. This algorithm works in an online,\\nmemoryless setting, thus requiring no backpropagation through time, and is\\nscalable, avoiding the large computational and memory cost of maintaining the\\nfull gradient of the current state with respect to the parameters.\\n  The algorithm essentially maintains, at each time, a single search direction\\nin parameter space. The evolution of this search direction is partly stochastic\\nand is constructed in such a way to provide, at every time, an unbiased random\\nestimate of the gradient of the loss function with respect to the parameters.\\nBecause the gradient estimate is unbiased, on average over time the parameter\\nis updated as it should.\\n  The resulting gradient estimate can then be fed to a lightweight Kalman-like\\nfilter to yield an improved algorithm. For recurrent neural networks, the\\nresulting algorithms scale linearly with the number of parameters.\\n  Small-scale experiments confirm the suitability of the approach, showing that\\nthe stochastic approximation of the gradient introduced in the algorithm is not\\ndetrimental to learning. In particular, the Kalman-like version of NoBackTrack\\nis superior to backpropagation through time (BPTT) when the time span of\\ndependencies in the data is longer than the truncation span for BPTT.',\n",
              " 'Deep clustering: Discriminative embeddings for segmentation and\\n  separation\\nWe address the problem of acoustic source separation in a deep learning\\nframework we call \"deep clustering.\" Rather than directly estimating signals or\\nmasking functions, we train a deep network to produce spectrogram embeddings\\nthat are discriminative for partition labels given in training data. Previous\\ndeep network approaches provide great advantages in terms of learning power and\\nspeed, but previously it has been unclear how to use them to separate signals\\nin a class-independent way. In contrast, spectral clustering approaches are\\nflexible with respect to the classes and number of items to be segmented, but\\nit has been unclear how to leverage the learning power and speed of deep\\nnetworks. To obtain the best of both worlds, we use an objective function that\\nto train embeddings that yield a low-rank approximation to an ideal pairwise\\naffinity matrix, in a class-independent way. This avoids the high cost of\\nspectral factorization and instead produces compact clusters that are amenable\\nto simple clustering methods. The segmentations are therefore implicitly\\nencoded in the embeddings, and can be \"decoded\" by clustering. Preliminary\\nexperiments show that the proposed method can separate speech: when trained on\\nspectrogram features containing mixtures of two speakers, and tested on\\nmixtures of a held-out set of speakers, it can infer masking functions that\\nimprove signal quality by around 6dB. We show that the model can generalize to\\nthree-speaker mixtures despite training only on two-speaker mixtures. The\\nframework can be used without class labels, and therefore has the potential to\\nbe trained on a diverse set of sound types, and to generalize to novel sources.\\nWe hope that future work will lead to segmentation of arbitrary sounds, with\\nextensions to microphone array methods as well as image segmentation and other\\ndomains.',\n",
              " 'Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural\\n  Networks\\nSeveral popular graph embedding techniques for representation learning and\\ndimensionality reduction rely on performing computationally expensive\\neigendecompositions to derive a nonlinear transformation of the input data\\nspace. The resulting eigenvectors encode the embedding coordinates for the\\ntraining samples only, and so the embedding of novel data samples requires\\nfurther costly computation. In this paper, we present a method for the\\nout-of-sample extension of graph embeddings using deep neural networks (DNN) to\\nparametrically approximate these nonlinear maps. Compared with traditional\\nnonparametric out-of-sample extension methods, we demonstrate that the DNNs can\\ngeneralize with equal or better fidelity and require orders of magnitude less\\ncomputation at test time. Moreover, we find that unsupervised pretraining of\\nthe DNNs improves optimization for larger network sizes, thus removing\\nsensitivity to model selection.',\n",
              " 'Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A\\n  Systematic Study\\nThis paper presents Rudra, a parameter server based distributed computing\\nframework tuned for training large-scale deep neural networks. Using variants\\nof the asynchronous stochastic gradient descent algorithm we study the impact\\nof synchronization protocol, stale gradient updates, minibatch size, learning\\nrates, and number of learners on runtime performance and model accuracy. We\\nintroduce a new learning rate modulation strategy to counter the effect of\\nstale gradients and propose a new synchronization protocol that can effectively\\nbound the staleness in gradients, improve runtime performance and achieve good\\nmodel accuracy. Our empirical investigation reveals a principled approach for\\ndistributed training of neural networks: the mini-batch size per learner should\\nbe reduced as more learners are added to the system to preserve the model\\naccuracy. We validate this approach using commonly-used image classification\\nbenchmarks: CIFAR10 and ImageNet.',\n",
              " 'Convolutional Networks on Graphs for Learning Molecular Fingerprints\\nWe introduce a convolutional neural network that operates directly on graphs.\\nThese networks allow end-to-end learning of prediction pipelines whose inputs\\nare graphs of arbitrary size and shape. The architecture we present generalizes\\nstandard molecular feature extraction methods based on circular fingerprints.\\nWe show that these data-driven features are more interpretable, and have better\\npredictive performance on a variety of tasks.',\n",
              " 'Population-Contrastive-Divergence: Does Consistency help with RBM\\n  training?\\nEstimating the log-likelihood gradient with respect to the parameters of a\\nRestricted Boltzmann Machine (RBM) typically requires sampling using Markov\\nChain Monte Carlo (MCMC) techniques. To save computation time, the Markov\\nchains are only run for a small number of steps, which leads to a biased\\nestimate. This bias can cause RBM training algorithms such as Contrastive\\nDivergence (CD) learning to deteriorate. We adopt the idea behind Population\\nMonte Carlo (PMC) methods to devise a new RBM training algorithm termed\\nPopulation-Contrastive-Divergence (pop-CD). Compared to CD, it leads to a\\nconsistent estimate and may have a significantly lower bias. Its computational\\noverhead is negligible compared to CD. However, the variance of the gradient\\nestimate increases. We experimentally show that pop-CD can significantly\\noutperform CD. In many cases, we observed a smaller bias and achieved higher\\nlog-likelihood values. However, when the RBM distribution has many hidden\\nneurons, the consistent estimate of pop-CD may still have a considerable bias\\nand the variance of the gradient estimate requires a smaller learning rate.\\nThus, despite its superior theoretical properties, it is not advisable to use\\npop-CD in its current form on large problems.',\n",
              " \"AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction\\n  in Structure-based Drug Discovery\\nDeep convolutional neural networks comprise a subclass of deep neural\\nnetworks (DNN) with a constrained architecture that leverages the spatial and\\ntemporal structure of the domain they model. Convolutional networks achieve the\\nbest predictive performance in areas such as speech and image recognition by\\nhierarchically composing simple local features into complex models. Although\\nDNNs have been used in drug discovery for QSAR and ligand-based bioactivity\\npredictions, none of these models have benefited from this powerful\\nconvolutional architecture. This paper introduces AtomNet, the first\\nstructure-based, deep convolutional neural network designed to predict the\\nbioactivity of small molecules for drug discovery applications. We demonstrate\\nhow to apply the convolutional concepts of feature locality and hierarchical\\ncomposition to the modeling of bioactivity and chemical interactions. In\\nfurther contrast to existing DNN techniques, we show that AtomNet's application\\nof local convolutional filters to structural target information successfully\\npredicts new active molecules for targets with no previously known modulators.\\nFinally, we show that AtomNet outperforms previous docking approaches on a\\ndiverse set of benchmarks by a large margin, achieving an AUC greater than 0.9\\non 57.8% of the targets in the DUDE benchmark.\",\n",
              " 'Distillation as a Defense to Adversarial Perturbations against Deep\\n  Neural Networks\\nDeep learning algorithms have been shown to perform extremely well on many\\nclassical machine learning problems. However, recent studies have shown that\\ndeep learning, like other machine learning techniques, is vulnerable to\\nadversarial samples: inputs crafted to force a deep neural network (DNN) to\\nprovide adversary-selected outputs. Such attacks can seriously undermine the\\nsecurity of the system supported by the DNN, sometimes with devastating\\nconsequences. For example, autonomous vehicles can be crashed, illicit or\\nillegal content can bypass content filters, or biometric authentication systems\\ncan be manipulated to allow improper access. In this work, we introduce a\\ndefensive mechanism called defensive distillation to reduce the effectiveness\\nof adversarial samples on DNNs. We analytically investigate the\\ngeneralizability and robustness properties granted by the use of defensive\\ndistillation when training DNNs. We also empirically study the effectiveness of\\nour defense mechanisms on two DNNs placed in adversarial settings. The study\\nshows that defensive distillation can reduce effectiveness of sample creation\\nfrom 95% to less than 0.5% on a studied DNN. Such dramatic gains can be\\nexplained by the fact that distillation leads gradients used in adversarial\\nsample creation to be reduced by a factor of 10^30. We also find that\\ndistillation increases the average minimum number of features that need to be\\nmodified to create adversarial samples by about 800% on one of the DNNs we\\ntested.',\n",
              " 'The Variational Gaussian Process\\nVariational inference is a powerful tool for approximate inference, and it\\nhas been recently applied for representation learning with deep generative\\nmodels. We develop the variational Gaussian process (VGP), a Bayesian\\nnonparametric variational family, which adapts its shape to match complex\\nposterior distributions. The VGP generates approximate posterior samples by\\ngenerating latent inputs and warping them through random non-linear mappings;\\nthe distribution over random mappings is learned during inference, enabling the\\ntransformed outputs to adapt to varying complexity. We prove a universal\\napproximation theorem for the VGP, demonstrating its representative power for\\nlearning any model. For inference we present a variational objective inspired\\nby auto-encoders and perform black box inference over a wide class of models.\\nThe VGP achieves new state-of-the-art results for unsupervised learning,\\ninferring models such as the deep latent Gaussian model and the recently\\nproposed DRAW.',\n",
              " 'Partial Reinitialisation for Optimisers\\nHeuristic optimisers which search for an optimal configuration of variables\\nrelative to an objective function often get stuck in local optima where the\\nalgorithm is unable to find further improvement. The standard approach to\\ncircumvent this problem involves periodically restarting the algorithm from\\nrandom initial configurations when no further improvement can be found. We\\npropose a method of partial reinitialization, whereby, in an attempt to find a\\nbetter solution, only sub-sets of variables are re-initialised rather than the\\nwhole configuration. Much of the information gained from previous runs is hence\\nretained. This leads to significant improvements in the quality of the solution\\nfound in a given time for a variety of optimisation problems in machine\\nlearning.',\n",
              " 'Efficient Representation of Low-Dimensional Manifolds using Deep\\n  Networks\\nWe consider the ability of deep neural networks to represent data that lies\\nnear a low-dimensional manifold in a high-dimensional space. We show that deep\\nnetworks can efficiently extract the intrinsic, low-dimensional coordinates of\\nsuch data. We first show that the first two layers of a deep network can\\nexactly embed points lying on a monotonic chain, a special type of piecewise\\nlinear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,\\nthe network can do this using an almost optimal number of parameters. We also\\nshow that this network projects nearby points onto the manifold and then embeds\\nthem with little error. We then extend these results to more general manifolds.',\n",
              " 'Enhanced perceptrons using contrastive biclusters\\nPerceptrons are neuronal devices capable of fully discriminating linearly\\nseparable classes. Although straightforward to implement and train, their\\napplicability is usually hindered by non-trivial requirements imposed by\\nreal-world classification problems. Therefore, several approaches, such as\\nkernel perceptrons, have been conceived to counteract such difficulties. In\\nthis paper, we investigate an enhanced perceptron model based on the notion of\\ncontrastive biclusters. From this perspective, a good discriminative bicluster\\ncomprises a subset of data instances belonging to one class that show high\\ncoherence across a subset of features and high differentiation from nearest\\ninstances of the other class under the same features (referred to as its\\ncontrastive bicluster). Upon each local subspace associated with a pair of\\ncontrastive biclusters a perceptron is trained and the model with highest area\\nunder the receiver operating characteristic curve (AUC) value is selected as\\nthe final classifier. Experiments conducted on a range of data sets, including\\nthose related to a difficult biosignal classification problem, show that the\\nproposed variant can be indeed very useful, prevailing in most of the cases\\nupon standard and kernel perceptrons in terms of accuracy and AUC measures.',\n",
              " 'Alternating optimization method based on nonnegative matrix\\n  factorizations for deep neural networks\\nThe backpropagation algorithm for calculating gradients has been widely used\\nin computation of weights for deep neural networks (DNNs). This method requires\\nderivatives of objective functions and has some difficulties finding\\nappropriate parameters such as learning rate. In this paper, we propose a novel\\napproach for computing weight matrices of fully-connected DNNs by using two\\ntypes of semi-nonnegative matrix factorizations (semi-NMFs). In this method,\\noptimization processes are performed by calculating weight matrices\\nalternately, and backpropagation (BP) is not used. We also present a method to\\ncalculate stacked autoencoder using a NMF. The output results of the\\nautoencoder are used as pre-training data for DNNs. The experimental results\\nshow that our method using three types of NMFs attains similar error rates to\\nthe conventional DNNs with BP.',\n",
              " \"Robust Large Margin Deep Neural Networks\\nThe generalization error of deep neural networks via their classification\\nmargin is studied in this work. Our approach is based on the Jacobian matrix of\\na deep neural network and can be applied to networks with arbitrary\\nnon-linearities and pooling layers, and to networks with different\\narchitectures such as feed forward networks and residual networks. Our analysis\\nleads to the conclusion that a bounded spectral norm of the network's Jacobian\\nmatrix in the neighbourhood of the training samples is crucial for a deep\\nneural network of arbitrary depth and width to generalize well. This is a\\nsignificant improvement over the current bounds in the literature, which imply\\nthat the generalization error grows with either the width or the depth of the\\nnetwork. Moreover, it shows that the recently proposed batch normalization and\\nweight normalization re-parametrizations enjoy good generalization properties,\\nand leads to a novel network regularizer based on the network's Jacobian\\nmatrix. The analysis is supported with experimental results on the MNIST,\\nCIFAR-10, LaRED and ImageNet datasets.\",\n",
              " 'No bad local minima: Data independent training error guarantees for\\n  multilayer neural networks\\nWe use smoothed analysis techniques to provide guarantees on the training\\nloss of Multilayer Neural Networks (MNNs) at differentiable local minima.\\nSpecifically, we examine MNNs with piecewise linear activation functions,\\nquadratic loss and a single output, under mild over-parametrization. We prove\\nthat for a MNN with one hidden layer, the training error is zero at every\\ndifferentiable local minimum, for almost every dataset and dropout-like noise\\nrealization. We then extend these results to the case of more than one hidden\\nlayer. Our theoretical guarantees assume essentially nothing on the training\\ndata, and are verified numerically. These results suggest why the highly\\nnon-convex loss of such MNNs can be easily optimized using local updates (e.g.,\\nstochastic gradient descent), as observed empirically.',\n",
              " 'Learning Structured Sparsity in Deep Neural Networks\\nHigh demand for computation resources severely hinders deployment of\\nlarge-scale Deep Neural Networks (DNN) in resource constrained devices. In this\\nwork, we propose a Structured Sparsity Learning (SSL) method to regularize the\\nstructures (i.e., filters, channels, filter shapes, and layer depth) of DNNs.\\nSSL can: (1) learn a compact structure from a bigger DNN to reduce computation\\ncost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently\\naccelerate the DNNs evaluation. Experimental results show that SSL achieves on\\naverage 5.1x and 3.1x speedups of convolutional layer computation of AlexNet\\nagainst CPU and GPU, respectively, with off-the-shelf libraries. These speedups\\nare about twice speedups of non-structured sparsity; (3) regularize the DNN\\nstructure to improve classification accuracy. The results show that for\\nCIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual\\nNetwork (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%,\\nwhich is still slightly higher than that of original ResNet with 32 layers. For\\nAlexNet, structure regularization by SSL also reduces the error by around ~1%.\\nOpen source code is in https://github.com/wenwei202/caffe/tree/scnn',\n",
              " 'Depth-Width Tradeoffs in Approximating Natural Functions with Neural\\n  Networks\\nWe provide several new depth-based separation results for feed-forward neural\\nnetworks, proving that various types of simple and natural functions can be\\nbetter approximated using deeper networks than shallower ones, even if the\\nshallower networks are much larger. This includes indicators of balls and\\nellipses; non-linear functions which are radial with respect to the $L_1$ norm;\\nand smooth non-linear functions. We also show that these gaps can be observed\\nexperimentally: Increasing the depth indeed allows better learning than\\nincreasing width, when training neural networks to learn an indicator of a unit\\nball.',\n",
              " 'Tensor Switching Networks\\nWe present a novel neural network algorithm, the Tensor Switching (TS)\\nnetwork, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to\\ntensor-valued hidden units. The TS network copies its entire input vector to\\ndifferent locations in an expanded representation, with the location determined\\nby its hidden unit activity. In this way, even a simple linear readout from the\\nTS representation can implement a highly expressive deep-network-like function.\\nThe TS network hence avoids the vanishing gradient problem by construction, at\\nthe cost of larger representation size. We develop several methods to train the\\nTS network, including equivalent kernels for infinitely wide and deep TS\\nnetworks, a one-pass linear learning algorithm, and two\\nbackpropagation-inspired representation learning algorithms. Our experimental\\nresults demonstrate that the TS network is indeed more expressive and\\nconsistently learns faster than standard ReLU networks.',\n",
              " 'Survey of Expressivity in Deep Neural Networks\\nWe survey results on neural network expressivity described in \"On the\\nExpressive Power of Deep Neural Networks\". The paper motivates and develops\\nthree natural measures of expressiveness, which all display an exponential\\ndependence on the depth of the network. In fact, all of these measures are\\nrelated to a fourth quantity, trajectory length. This quantity grows\\nexponentially in the depth of the network, and is responsible for the depth\\nsensitivity observed. These results translate to consequences for networks\\nduring and after training. They suggest that parameters earlier in a network\\nhave greater influence on its expressive power -- in particular, given a layer,\\nits influence on expressivity is determined by the remaining depth of the\\nnetwork after that layer. This is verified with experiments on MNIST and\\nCIFAR-10. We also explore the effect of training on the input-output map, and\\nfind that it trades off between the stability and expressivity.',\n",
              " 'Precise Recovery of Latent Vectors from Generative Adversarial Networks\\nGenerative adversarial networks (GANs) transform latent vectors into visually\\nplausible images. It is generally thought that the original GAN formulation\\ngives no out-of-the-box method to reverse the mapping, projecting images back\\ninto latent space. We introduce a simple, gradient-based technique called\\nstochastic clipping. In experiments, for images generated by the GAN, we\\nprecisely recover their latent vector pre-images 100% of the time. Additional\\nexperiments demonstrate that this method is robust to noise. Finally, we show\\nthat even for unseen images, our method appears to recover unique encodings.',\n",
              " 'Predicting Surgery Duration with Neural Heteroscedastic Regression\\nScheduling surgeries is a challenging task due to the fundamental uncertainty\\nof the clinical environment, as well as the risks and costs associated with\\nunder- and over-booking. We investigate neural regression algorithms to\\nestimate the parameters of surgery case durations, focusing on the issue of\\nheteroscedasticity. We seek to simultaneously estimate the duration of each\\nsurgery, as well as a surgery-specific notion of our uncertainty about its\\nduration. Estimating this uncertainty can lead to more nuanced and effective\\nscheduling strategies, as we are able to schedule surgeries more efficiently\\nwhile allowing an informed and case-specific margin of error. Using surgery\\nrecords %from the UC San Diego Health System, from a large United States health\\nsystem we demonstrate potential improvements on the order of 20% (in terms of\\nminutes overbooked) compared to current scheduling techniques. Moreover, we\\ndemonstrate that surgery durations are indeed heteroscedastic. We show that\\nmodels that estimate case-specific uncertainty better fit the data (log\\nlikelihood). Additionally, we show that the heteroscedastic predictions can\\nmore optimally trade off between over and under-booking minutes, especially\\nwhen idle minutes and scheduling collisions confer disparate costs.',\n",
              " 'Depth Creates No Bad Local Minima\\nIn deep learning, \\\\textit{depth}, as well as \\\\textit{nonlinearity}, create\\nnon-convex loss surfaces. Then, does depth alone create bad local minima? In\\nthis paper, we prove that without nonlinearity, depth alone does not create bad\\nlocal minima, although it induces non-convex loss surface. Using this insight,\\nwe greatly simplify a recently proposed proof to show that all of the local\\nminima of feedforward deep linear neural networks are global minima. Our\\ntheoretical results generalize previous results with fewer assumptions, and\\nthis analysis provides a method to show similar results beyond square loss in\\ndeep linear models.',\n",
              " 'Deep Semi-Random Features for Nonlinear Function Approximation\\nWe propose semi-random features for nonlinear function approximation. The\\nflexibility of semi-random feature lies between the fully adjustable units in\\ndeep learning and the random features used in kernel methods. For one hidden\\nlayer models with semi-random features, we prove with no unrealistic\\nassumptions that the model classes contain an arbitrarily good function as the\\nwidth increases (universality), and despite non-convexity, we can find such a\\ngood function (optimization theory) that generalizes to unseen new data\\n(generalization bound). For deep models, with no unrealistic assumptions, we\\nprove universal approximation ability, a lower bound on approximation error, a\\npartial optimization guarantee, and a generalization bound. Depending on the\\nproblems, the generalization bound of deep semi-random features can be\\nexponentially better than the known bounds of deep ReLU nets; our\\ngeneralization error bound can be independent of the depth, the number of\\ntrainable weights as well as the input dimensionality. In experiments, we show\\nthat semi-random features can match the performance of neural networks by using\\nslightly more units, and it outperforms random features by using significantly\\nfewer units. Moreover, we introduce a new implicit ensemble method by using\\nsemi-random features.',\n",
              " 'Curriculum Dropout\\nDropout is a very effective way of regularizing neural networks.\\nStochastically \"dropping out\" units with a certain probability discourages\\nover-specific co-adaptations of feature detectors, preventing overfitting and\\nimproving network generalization. Besides, Dropout can be interpreted as an\\napproximate model aggregation technique, where an exponential number of smaller\\nnetworks are averaged in order to get a more powerful ensemble. In this paper,\\nwe show that using a fixed dropout probability during training is a suboptimal\\nchoice. We thus propose a time scheduling for the probability of retaining\\nneurons in the network. This induces an adaptive regularization scheme that\\nsmoothly increases the difficulty of the optimization problem. This idea of\\n\"starting easy\" and adaptively increasing the difficulty of the learning\\nproblem has its roots in curriculum learning and allows one to train better\\nmodels. Indeed, we prove that our optimization strategy implements a very\\ngeneral curriculum scheme, by gradually adding noise to both the input and\\nintermediate feature representations within the network architecture.\\nExperiments on seven image classification datasets and different network\\narchitectures show that our method, named Curriculum Dropout, frequently yields\\nto better generalization and, at worst, performs just as well as the standard\\nDropout method.',\n",
              " 'The power of deeper networks for expressing natural functions\\nIt is well-known that neural networks are universal approximators, but that\\ndeeper networks tend to be much more efficient than shallow ones. We shed light\\non this by proving that the total number of neurons $m$ required to approximate\\nnatural classes of multivariate polynomials of $n$ variables grows only\\nlinearly with $n$ for deep neural networks, but grows exponentially when merely\\na single hidden layer is allowed. We also provide evidence that when the number\\nof hidden layers is increased from $1$ to $k$, the neuron requirement grows\\nexponentially not with $n$ but with $n^{1/k}$, suggesting that the minimum\\nnumber of layers required for computational tractability grows only\\nlogarithmically with $n$.',\n",
              " 'Gradient Descent for Spiking Neural Networks\\nMuch of studies on neural computation are based on network models of static\\nneurons that produce analog output, despite the fact that information\\nprocessing in the brain is predominantly carried out by dynamic neurons that\\nproduce discrete pulses called spikes. Research in spike-based computation has\\nbeen impeded by the lack of efficient supervised learning algorithm for spiking\\nnetworks. Here, we present a gradient descent method for optimizing spiking\\nnetwork models by introducing a differentiable formulation of spiking networks\\nand deriving the exact gradient calculation. For demonstration, we trained\\nrecurrent spiking networks on two dynamic tasks: one that requires optimizing\\nfast (~millisecond) spike-based interactions for efficient encoding of\\ninformation, and a delayed memory XOR task over extended duration (~second).\\nThe results show that our method indeed optimizes the spiking network dynamics\\non the time scale of individual spikes as well as behavioral time scales. In\\nconclusion, our result offers a general purpose supervised learning algorithm\\nfor spiking neural networks, thus advancing further investigations on\\nspike-based computation.',\n",
              " 'Unsure When to Stop? Ask Your Semantic Neighbors\\nIn iterative supervised learning algorithms it is common to reach a point in\\nthe search where no further induction seems to be possible with the available\\ndata. If the search is continued beyond this point, the risk of overfitting\\nincreases significantly. Following the recent developments in inductive\\nsemantic stochastic methods, this paper studies the feasibility of using\\ninformation gathered from the semantic neighborhood to decide when to stop the\\nsearch. Two semantic stopping criteria are proposed and experimentally assessed\\nin Geometric Semantic Genetic Programming (GSGP) and in the Semantic Learning\\nMachine (SLM) algorithm (the equivalent algorithm for neural networks). The\\nexperiments are performed on real-world high-dimensional regression datasets.\\nThe results show that the proposed semantic stopping criteria are able to\\ndetect stopping points that result in a competitive generalization for both\\nGSGP and SLM. This approach also yields computationally efficient algorithms as\\nit allows the evolution of neural networks in less than 3 seconds on average,\\nand of GP trees in at most 10 seconds. The usage of the proposed semantic\\nstopping criteria in conjunction with the computation of optimal\\nmutation/learning steps also results in small trees and neural networks.',\n",
              " 'Anomaly Detection on Graph Time Series\\nIn this paper, we use variational recurrent neural network to investigate the\\nanomaly detection problem on graph time series. The temporal correlation is\\nmodeled by the combination of recurrent neural network (RNN) and variational\\ninference (VI), while the spatial information is captured by the graph\\nconvolutional network. In order to incorporate external factors, we use feature\\nextractor to augment the transition of latent variables, which can learn the\\ninfluence of external factors. With the target function as accumulative ELBO,\\nit is easy to extend this model to on-line method. The experimental study on\\ntraffic flow data shows the detection capability of the proposed method.',\n",
              " 'A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and\\n  Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\\nGated Recurrent Unit (GRU) is a recently-developed variation of the long\\nshort-term memory (LSTM) unit, both of which are types of recurrent neural\\nnetwork (RNN). Through empirical evidence, both models have been proven to be\\neffective in a wide variety of machine learning tasks such as natural language\\nprocessing (Wen et al., 2015), speech recognition (Chorowski et al., 2015), and\\ntext classification (Yang et al., 2016). Conventionally, like most neural\\nnetworks, both of the aforementioned RNN variants employ the Softmax function\\nas its final output layer for its prediction, and the cross-entropy function\\nfor computing its loss. In this paper, we present an amendment to this norm by\\nintroducing linear support vector machine (SVM) as the replacement for Softmax\\nin the final output layer of a GRU model. Furthermore, the cross-entropy\\nfunction shall be replaced with a margin-based function. While there have been\\nsimilar studies (Alalshekmubarak & Smith, 2013; Tang, 2013), this proposal is\\nprimarily intended for binary classification on intrusion detection using the\\n2013 network traffic data from the honeypot systems of Kyoto University.\\nResults show that the GRU-SVM model performs relatively higher than the\\nconventional GRU-Softmax model. The proposed model reached a training accuracy\\nof ~81.54% and a testing accuracy of ~84.15%, while the latter was able to\\nreach a training accuracy of ~63.07% and a testing accuracy of ~70.75%. In\\naddition, the juxtaposition of these two final output layers indicate that the\\nSVM would outperform Softmax in prediction time - a theoretical implication\\nwhich was supported by the actual training and testing time in the study.',\n",
              " \"DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in\\n  Neural Networks\\nDeep neural networks have become widely used, obtaining remarkable results in\\ndomains such as computer vision, speech recognition, natural language\\nprocessing, audio recognition, social network filtering, machine translation,\\nand bio-informatics, where they have produced results comparable to human\\nexperts. However, these networks can be easily fooled by adversarial\\nperturbations: minimal changes to correctly-classified inputs, that cause the\\nnetwork to mis-classify them. This phenomenon represents a concern for both\\nsafety and security, but it is currently unclear how to measure a network's\\nrobustness against such perturbations. Existing techniques are limited to\\nchecking robustness around a few individual input points, providing only very\\nlimited guarantees. We propose a novel approach for automatically identifying\\nsafe regions of the input space, within which the network is robust against\\nadversarial perturbations. The approach is data-guided, relying on clustering\\nto identify well-defined geometric regions as candidate safe regions. We then\\nutilize verification techniques to confirm that these regions are safe or to\\nprovide counter-examples showing that they are not safe. We also introduce the\\nnotion of targeted robustness which, for a given target label and region,\\nensures that a NN does not map any input in the region to the target label. We\\nevaluated our technique on the MNIST dataset and on a neural network\\nimplementation of a controller for the next-generation Airborne Collision\\nAvoidance System for unmanned aircraft (ACAS Xu). For these networks, our\\napproach identified multiple regions which were completely safe as well as some\\nwhich were only safe for specific labels. It also discovered several\\nadversarial perturbations of interest.\",\n",
              " 'A Method of Generating Random Weights and Biases in Feedforward Neural\\n  Networks with Random Hidden Nodes\\nNeural networks with random hidden nodes have gained increasing interest from\\nresearchers and practical applications. This is due to their unique features\\nsuch as very fast training and universal approximation property. In these\\nnetworks the weights and biases of hidden nodes determining the nonlinear\\nfeature mapping are set randomly and are not learned. Appropriate selection of\\nthe intervals from which weights and biases are selected is extremely\\nimportant. This topic has not yet been sufficiently explored in the literature.\\nIn this work a method of generating random weights and biases is proposed. This\\nmethod generates the parameters of the hidden nodes in such a way that\\nnonlinear fragments of the activation functions are located in the input space\\nregions with data and can be used to construct the surface approximating a\\nnonlinear target function. The weights and biases are dependent on the input\\ndata range and activation function type. The proposed methods allows us to\\ncontrol the generalization degree of the model. These all lead to improvement\\nin approximation performance of the network. Several experiments show very\\npromising results.',\n",
              " \"Rotational Unit of Memory\\nThe concepts of unitary evolution matrices and associative memory have\\nboosted the field of Recurrent Neural Networks (RNN) to state-of-the-art\\nperformance in a variety of sequential tasks. However, RNN still have a limited\\ncapacity to manipulate long-term memory. To bypass this weakness the most\\nsuccessful applications of RNN use external techniques such as attention\\nmechanisms. In this paper we propose a novel RNN model that unifies the\\nstate-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM\\nis its rotational operation, which is, naturally, a unitary matrix, providing\\narchitectures with the power to learn long-term dependencies by overcoming the\\nvanishing and exploding gradients problem. Moreover, the rotational unit also\\nserves as associative memory. We evaluate our model on synthetic memorization,\\nquestion answering and language modeling tasks. RUM learns the Copying Memory\\ntask completely and improves the state-of-the-art result in the Recall task.\\nRUM's performance in the bAbI Question Answering task is comparable to that of\\nmodels with attention mechanism. We also improve the state-of-the-art result to\\n1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB)\\ntask, which is to signify the applications of RUM to real-world sequential\\ndata. The universality of our construction, at the core of RNN, establishes RUM\\nas a promising approach to language modeling, speech recognition and machine\\ntranslation.\",\n",
              " 'Progressive Growing of GANs for Improved Quality, Stability, and\\n  Variation\\nWe describe a new training methodology for generative adversarial networks.\\nThe key idea is to grow both the generator and discriminator progressively:\\nstarting from a low resolution, we add new layers that model increasingly fine\\ndetails as training progresses. This both speeds the training up and greatly\\nstabilizes it, allowing us to produce images of unprecedented quality, e.g.,\\nCelebA images at 1024^2. We also propose a simple way to increase the variation\\nin generated images, and achieve a record inception score of 8.80 in\\nunsupervised CIFAR10. Additionally, we describe several implementation details\\nthat are important for discouraging unhealthy competition between the generator\\nand discriminator. Finally, we suggest a new metric for evaluating GAN results,\\nboth in terms of image quality and variation. As an additional contribution, we\\nconstruct a higher-quality version of the CelebA dataset.',\n",
              " 'Generative Adversarial Source Separation\\nGenerative source separation methods such as non-negative matrix\\nfactorization (NMF) or auto-encoders, rely on the assumption of an output\\nprobability density. Generative Adversarial Networks (GANs) can learn data\\ndistributions without needing a parametric assumption on the output density. We\\nshow on a speech source separation experiment that, a multi-layer perceptron\\ntrained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders\\ntrained with maximum likelihood, and variational auto-encoders in terms of\\nsource to distortion ratio.',\n",
              " 'A Supervised STDP-based Training Algorithm for Living Neural Networks\\nNeural networks have shown great potential in many applications like speech\\nrecognition, drug discovery, image classification, and object detection. Neural\\nnetwork models are inspired by biological neural networks, but they are\\noptimized to perform machine learning tasks on digital computers. The proposed\\nwork explores the possibilities of using living neural networks in vitro as\\nbasic computational elements for machine learning applications. A new\\nsupervised STDP-based learning algorithm is proposed in this work, which\\nconsiders neuron engineering constrains. A 74.7% accuracy is achieved on the\\nMNIST benchmark for handwritten digit recognition.',\n",
              " 'Improving Factor-Based Quantitative Investing by Forecasting Company\\n  Fundamentals\\nOn a periodic basis, publicly traded companies are required to report\\nfundamentals: financial data such as revenue, operating income, debt, among\\nothers. These data points provide some insight into the financial health of a\\ncompany. Academic research has identified some factors, i.e. computed features\\nof the reported data, that are known through retrospective analysis to\\noutperform the market average. Two popular factors are the book value\\nnormalized by market capitalization (book-to-market) and the operating income\\nnormalized by the enterprise value (EBIT/EV). In this paper: we first show\\nthrough simulation that if we could (clairvoyantly) select stocks using factors\\ncalculated on future fundamentals (via oracle), then our portfolios would far\\noutperform a standard factor approach. Motivated by this analysis, we train\\ndeep neural networks to forecast future fundamentals based on a trailing\\n5-years window. Quantitative analysis demonstrates a significant improvement in\\nMSE over a naive strategy. Moreover, in retrospective analysis using an\\nindustry-grade stock portfolio simulator (backtester), we show an improvement\\nin compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor\\nmodel.',\n",
              " \"Genetic Algorithms for Mentor-Assisted Evaluation Function Optimization\\nIn this paper we demonstrate how genetic algorithms can be used to reverse\\nengineer an evaluation function's parameters for computer chess. Our results\\nshow that using an appropriate mentor, we can evolve a program that is on par\\nwith top tournament-playing chess programs, outperforming a two-time World\\nComputer Chess Champion. This performance gain is achieved by evolving a\\nprogram with a smaller number of parameters in its evaluation function to mimic\\nthe behavior of a superior mentor which uses a more extensive evaluation\\nfunction. In principle, our mentor-assisted approach could be used in a wide\\nrange of problems for which appropriate mentors are available.\",\n",
              " 'Block Neural Network Avoids Catastrophic Forgetting When Learning\\n  Multiple Task\\nIn the present work we propose a Deep Feed Forward network architecture which\\ncan be trained according to a sequential learning paradigm, where tasks of\\nincreasing difficulty are learned sequentially, yet avoiding catastrophic\\nforgetting. The proposed architecture can re-use the features learned on\\nprevious tasks in a new task when the old tasks and the new one are related.\\nThe architecture needs fewer computational resources (neurons and connections)\\nand less data for learning the new task than a network trained from scratch',\n",
              " 'A Scalable Deep Neural Network Architecture for Multi-Building and\\n  Multi-Floor Indoor Localization Based on Wi-Fi Fingerprinting\\nOne of the key technologies for future large-scale location-aware services\\ncovering a complex of multi-story buildings --- e.g., a big shopping mall and a\\nuniversity campus --- is a scalable indoor localization technique. In this\\npaper, we report the current status of our investigation on the use of deep\\nneural networks (DNNs) for scalable building/floor classification and\\nfloor-level position estimation based on Wi-Fi fingerprinting. Exploiting the\\nhierarchical nature of the building/floor estimation and floor-level\\ncoordinates estimation of a location, we propose a new DNN architecture\\nconsisting of a stacked autoencoder for the reduction of feature space\\ndimension and a feed-forward classifier for multi-label classification of\\nbuilding/floor/location, on which the multi-building and multi-floor indoor\\nlocalization system based on Wi-Fi fingerprinting is built. Experimental\\nresults for the performance of building/floor estimation and floor-level\\ncoordinates estimation of a given location demonstrate the feasibility of the\\nproposed DNN-based indoor localization system, which can provide near\\nstate-of-the-art performance using a single DNN, for the implementation with\\nlower complexity and energy consumption at mobile devices.',\n",
              " 'Dynamic Boltzmann Machines for Second Order Moments and Generalized\\n  Gaussian Distributions\\nDynamic Boltzmann Machine (DyBM) has been shown highly efficient to predict\\ntime-series data. Gaussian DyBM is a DyBM that assumes the predicted data is\\ngenerated by a Gaussian distribution whose first-order moment (mean)\\ndynamically changes over time but its second-order moment (variance) is fixed.\\nHowever, in many financial applications, the assumption is quite limiting in\\ntwo aspects. First, even when the data follows a Gaussian distribution, its\\nvariance may change over time. Such variance is also related to important\\ntemporal economic indicators such as the market volatility. Second, financial\\ntime-series data often requires learning datasets generated by the generalized\\nGaussian distribution with an additional shape parameter that is important to\\napproximate heavy-tailed distributions. Addressing those aspects, we show how\\nto extend DyBM that results in significant performance improvement in\\npredicting financial time-series data.',\n",
              " 'Multi-timescale memory dynamics in a reinforcement learning network with\\n  attention-gated memory\\nLearning and memory are intertwined in our brain and their relationship is at\\nthe core of several recent neural network models. In particular, the\\nAttention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning\\nnetwork with an emphasis on biological plausibility of memory dynamics and\\nlearning. We find that the AuGMEnT network does not solve some hierarchical\\ntasks, where higher-level stimuli have to be maintained over a long time, while\\nlower-level stimuli need to be remembered and forgotten over a shorter\\ntimescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky\\nor short-timescale and non-leaky or long-timescale units in memory, that allow\\nto exchange lower-level information while maintaining higher-level one, thus\\nsolving both hierarchical and distractor tasks.',\n",
              " 'Weighted Contrastive Divergence\\nLearning algorithms for energy based Boltzmann architectures that rely on\\ngradient descent are in general computationally prohibitive, typically due to\\nthe exponential number of terms involved in computing the partition function.\\nIn this way one has to resort to approximation schemes for the evaluation of\\nthe gradient. This is the case of Restricted Boltzmann Machines (RBM) and its\\nlearning algorithm Contrastive Divergence (CD). It is well-known that CD has a\\nnumber of shortcomings, and its approximation to the gradient has several\\ndrawbacks. Overcoming these defects has been the basis of much research and new\\nalgorithms have been devised, such as persistent CD. In this manuscript we\\npropose a new algorithm that we call Weighted CD (WCD), built from small\\nmodifications of the negative phase in standard CD. However small these\\nmodifications may be, experimental work reported in this paper suggest that WCD\\nprovides a significant improvement over standard CD and persistent CD at a\\nsmall additional computational cost.',\n",
              " 'Dynamic Optimization of Neural Network Structures Using Probabilistic\\n  Modeling\\nDeep neural networks (DNNs) are powerful machine learning models and have\\nsucceeded in various artificial intelligence tasks. Although various\\narchitectures and modules for the DNNs have been proposed, selecting and\\ndesigning the appropriate network structure for a target problem is a\\nchallenging task. In this paper, we propose a method to simultaneously optimize\\nthe network structure and weight parameters during neural network training. We\\nconsider a probability distribution that generates network structures, and\\noptimize the parameters of the distribution instead of directly optimizing the\\nnetwork structure. The proposed method can apply to the various network\\nstructure optimization problems under the same framework. We apply the proposed\\nmethod to several structure optimization problems such as selection of layers,\\nselection of unit types, and selection of connections using the MNIST,\\nCIFAR-10, and CIFAR-100 datasets. The experimental results show that the\\nproposed method can find the appropriate and competitive network structures.',\n",
              " 'Pruning Techniques for Mixed Ensembles of Genetic Programming Models\\nThe objective of this paper is to define an effective strategy for building\\nan ensemble of Genetic Programming (GP) models. Ensemble methods are widely\\nused in machine learning due to their features: they average out biases, they\\nreduce the variance and they usually generalize better than single models.\\nDespite these advantages, building ensemble of GP models is not a\\nwell-developed topic in the evolutionary computation community. To fill this\\ngap, we propose a strategy that blends individuals produced by standard\\nsyntax-based GP and individuals produced by geometric semantic genetic\\nprogramming, one of the newest semantics-based method developed in GP. In fact,\\nrecent literature showed that combining syntax and semantics could improve the\\ngeneralization ability of a GP model. Additionally, to improve the diversity of\\nthe GP models used to build up the ensemble, we propose different pruning\\ncriteria that are based on correlation and entropy, a commonly used measure in\\ninformation theory. Experimental results,obtained over different complex\\nproblems, suggest that the pruning criteria based on correlation and entropy\\ncould be effective in improving the generalization ability of the ensemble\\nmodel and in reducing the computational burden required to build it.',\n",
              " 'Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines\\nThis paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for\\ntraining Boltzmann Machines. Similar in spirit to the Hessian-Free method of\\nMartens [8], our algorithm belongs to the family of truncated Newton methods\\nand exploits an efficient matrix-vector product to avoid explicitely storing\\nthe natural gradient metric $L$. This metric is shown to be the expected second\\nderivative of the log-partition function (under the model distribution), or\\nequivalently, the variance of the vector of partial derivatives of the energy\\nfunction. We evaluate our method on the task of joint-training a 3-layer Deep\\nBoltzmann Machine and show that MFNG does indeed have faster per-epoch\\nconvergence compared to Stochastic Maximum Likelihood with centering, though\\nwall-clock performance is currently not competitive.',\n",
              " 'Stochastic Pooling for Regularization of Deep Convolutional Neural\\n  Networks\\nWe introduce a simple and effective method for regularizing large\\nconvolutional neural networks. We replace the conventional deterministic\\npooling operations with a stochastic procedure, randomly picking the activation\\nwithin each pooling region according to a multinomial distribution, given by\\nthe activities within the pooling region. The approach is hyper-parameter free\\nand can be combined with other regularization approaches, such as dropout and\\ndata augmentation. We achieve state-of-the-art performance on four image\\ndatasets, relative to other approaches that do not utilize data augmentation.',\n",
              " \"Training Neural Networks with Stochastic Hessian-Free Optimization\\nHessian-free (HF) optimization has been successfully used for training deep\\nautoencoders and recurrent networks. HF uses the conjugate gradient algorithm\\nto construct update directions through curvature-vector products that can be\\ncomputed on the same order of time as gradients. In this paper we exploit this\\nproperty and study stochastic HF with gradient and curvature mini-batches\\nindependent of the dataset size. We modify Martens' HF for these settings and\\nintegrate dropout, a method for preventing co-adaptation of feature detectors,\\nto guard against overfitting. Stochastic Hessian-free optimization gives an\\nintermediary between SGD and HF that achieves competitive performance on both\\nclassification and deep autoencoder experiments.\",\n",
              " 'Reversible Jump MCMC Simulated Annealing for Neural Networks\\nWe propose a novel reversible jump Markov chain Monte Carlo (MCMC) simulated\\nannealing algorithm to optimize radial basis function (RBF) networks. This\\nalgorithm enables us to maximize the joint posterior distribution of the\\nnetwork parameters and the number of basis functions. It performs a global\\nsearch in the joint space of the parameters and number of parameters, thereby\\nsurmounting the problem of local minima. We also show that by calibrating a\\nBayesian model, we can obtain the classical AIC, BIC and MDL model selection\\ncriteria within a penalized likelihood framework. Finally, we show\\ntheoretically and empirically that the algorithm converges to the modes of the\\nfull posterior distribution in an efficient way.',\n",
              " 'Predicting Parameters in Deep Learning\\nWe demonstrate that there is significant redundancy in the parameterization\\nof several deep learning models. Given only a few weight values for each\\nfeature it is possible to accurately predict the remaining values. Moreover, we\\nshow that not only can the parameter values be predicted, but many of them need\\nnot be learned at all. We train several different architectures by learning\\nonly a small number of weights and predicting the rest. In the best case we are\\nable to predict more than 95% of the weights of a network without any drop in\\naccuracy.',\n",
              " 'Disentangling Factors of Variation via Generative Entangling\\nHere we propose a novel model family with the objective of learning to\\ndisentangle the factors of variation in data. Our approach is based on the\\nspike-and-slab restricted Boltzmann machine which we generalize to include\\nhigher-order interactions among multiple latent variables. Seen from a\\ngenerative perspective, the multiplicative interactions emulates the entangling\\nof factors of variation. Inference in the model can be seen as disentangling\\nthese generative factors. Unlike previous attempts at disentangling latent\\nfactors, the proposed model is trained using no supervised information\\nregarding the latent factors. We apply our model to the task of facial\\nexpression classification.',\n",
              " \"Neural Networks for Complex Data\\nArtificial neural networks are simple and efficient machine learning tools.\\nDefined originally in the traditional setting of simple vector data, neural\\nnetwork models have evolved to address more and more difficulties of complex\\nreal world problems, ranging from time evolving data to sophisticated data\\nstructures such as graphs and functions. This paper summarizes advances on\\nthose themes from the last decade, with a focus on results obtained by members\\nof the SAMM team of Universit\\\\'e Paris 1\",\n",
              " \"Multi-task Neural Networks for QSAR Predictions\\nAlthough artificial neural networks have occasionally been used for\\nQuantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in\\nthe past, the literature has of late been dominated by other machine learning\\ntechniques such as random forests. However, a variety of new neural net\\ntechniques along with successful applications in other domains have renewed\\ninterest in network approaches. In this work, inspired by the winning team's\\nuse of neural networks in a recent QSAR competition, we used an artificial\\nneural network to learn a function that predicts activities of compounds for\\nmultiple assays at the same time. We conducted experiments leveraging recent\\nmethods for dealing with overfitting in neural networks as well as other tricks\\nfrom the neural networks literature. We compared our methods to alternative\\nmethods reported to perform well on these tasks and found that our neural net\\nmethods provided superior performance.\",\n",
              " 'A Hybrid Latent Variable Neural Network Model for Item Recommendation\\nCollaborative filtering is used to recommend items to a user without\\nrequiring a knowledge of the item itself and tends to outperform other\\ntechniques. However, collaborative filtering suffers from the cold-start\\nproblem, which occurs when an item has not yet been rated or a user has not\\nrated any items. Incorporating additional information, such as item or user\\ndescriptions, into collaborative filtering can address the cold-start problem.\\nIn this paper, we present a neural network model with latent input variables\\n(latent neural network or LNN) as a hybrid collaborative filtering technique\\nthat addresses the cold-start problem. LNN outperforms a broad selection of\\ncontent-based filters (which make recommendations based on item descriptions)\\nand other hybrid approaches while maintaining the accuracy of state-of-the-art\\ncollaborative filtering techniques.',\n",
              " 'Techniques for Learning Binary Stochastic Feedforward Neural Networks\\nStochastic binary hidden units in a multi-layer perceptron (MLP) network give\\nat least three potential benefits when compared to deterministic MLP networks.\\n(1) They allow to learn one-to-many type of mappings. (2) They can be used in\\nstructured prediction problems, where modeling the internal structure of the\\noutput is important. (3) Stochasticity has been shown to be an excellent\\nregularizer, which makes generalization performance potentially better in\\ngeneral. However, training stochastic networks is considerably more difficult.\\nWe study training using M samples of hidden activations per input. We show that\\nthe case M=1 leads to a fundamentally different behavior where the network\\ntries to avoid stochasticity. We propose two new estimators for the training\\ngradient and propose benchmark tests for comparing training algorithms. Our\\nexperiments confirm that training stochastic networks is difficult and show\\nthat the proposed two estimators perform favorably among all the five known\\nestimators.',\n",
              " 'Learning ELM network weights using linear discriminant analysis\\nWe present an alternative to the pseudo-inverse method for determining the\\nhidden to output weight values for Extreme Learning Machines performing\\nclassification tasks. The method is based on linear discriminant analysis and\\nprovides Bayes optimal single point estimates for the weight values.',\n",
              " 'Exponentially Increasing the Capacity-to-Computation Ratio for\\n  Conditional Computation in Deep Learning\\nMany state-of-the-art results obtained with deep networks are achieved with\\nthe largest models that could be trained, and if more computation power was\\navailable, we might be able to exploit much larger datasets in order to improve\\ngeneralization ability. Whereas in learning algorithms such as decision trees\\nthe ratio of capacity (e.g., the number of parameters) to computation is very\\nfavorable (up to exponentially more parameters than computation), the ratio is\\nessentially 1 for deep neural networks. Conditional computation has been\\nproposed as a way to increase the capacity of a deep neural network without\\nincreasing the amount of computation required, by activating some parameters\\nand computation \"on-demand\", on a per-example basis. In this note, we propose a\\nnovel parametrization of weight matrices in neural networks which has the\\npotential to increase up to exponentially the ratio of the number of parameters\\nto computation. The proposed approach is based on turning on some parameters\\n(weight matrices) when specific bit patterns of hidden unit activations are\\nobtained. In order to better control for the overfitting that might result, we\\npropose a parametrization that is tree-structured, where each node of the tree\\ncorresponds to a prefix of a sequence of sign bits, or gating units, associated\\nwith hidden units.',\n",
              " 'Soft-Deep Boltzmann Machines\\nWe present a layered Boltzmann machine (BM) that can better exploit the\\nadvantages of a distributed representation. It is widely believed that deep BMs\\n(DBMs) have far greater representational power than its shallow counterpart,\\nrestricted Boltzmann machines (RBMs). However, this expectation on the\\nsupremacy of DBMs over RBMs has not ever been validated in a theoretical\\nfashion. In this paper, we provide both theoretical and empirical evidences\\nthat the representational power of DBMs can be actually rather limited in\\ntaking advantages of distributed representations. We propose an approximate\\nmeasure for the representational power of a BM regarding to the efficiency of a\\ndistributed representation. With this measure, we show a surprising fact that\\nDBMs can make inefficient use of distributed representations. Based on these\\nobservations, we propose an alternative BM architecture, which we dub soft-deep\\nBMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed\\nrepresentations in terms of the measure. Experiments demonstrate that sDBMs\\noutperform several state-of-the-art models, including DBMs, in generative tasks\\non binarized MNIST and Caltech-101 silhouettes.',\n",
              " 'Domain-Adversarial Training of Neural Networks\\nWe introduce a new representation learning approach for domain adaptation, in\\nwhich data at training and test time come from similar but different\\ndistributions. Our approach is directly inspired by the theory on domain\\nadaptation suggesting that, for effective domain transfer to be achieved,\\npredictions must be made based on features that cannot discriminate between the\\ntraining (source) and test (target) domains. The approach implements this idea\\nin the context of neural network architectures that are trained on labeled data\\nfrom the source domain and unlabeled data from the target domain (no labeled\\ntarget-domain data is necessary). As the training progresses, the approach\\npromotes the emergence of features that are (i) discriminative for the main\\nlearning task on the source domain and (ii) indiscriminate with respect to the\\nshift between the domains. We show that this adaptation behaviour can be\\nachieved in almost any feed-forward model by augmenting it with few standard\\nlayers and a new gradient reversal layer. The resulting augmented architecture\\ncan be trained using standard backpropagation and stochastic gradient descent,\\nand can thus be implemented with little effort using any of the deep learning\\npackages. We demonstrate the success of our approach for two distinct\\nclassification problems (document sentiment analysis and image classification),\\nwhere state-of-the-art domain adaptation performance on standard benchmarks is\\nachieved. We also validate the approach for descriptor learning task in the\\ncontext of person re-identification application.',\n",
              " 'Deep Online Convex Optimization with Gated Games\\nMethods from convex optimization are widely used as building blocks for deep\\nlearning algorithms. However, the reasons for their empirical success are\\nunclear, since modern convolutional networks (convnets), incorporating\\nrectifier units and max-pooling, are neither smooth nor convex. Standard\\nguarantees therefore do not apply. This paper provides the first convergence\\nrates for gradient descent on rectifier convnets. The proof utilizes the\\nparticular structure of rectifier networks which consists in binary\\nactive/inactive gates applied on top of an underlying linear network. The\\napproach generalizes to max-pooling, dropout and maxout. In other words, to\\nprecisely the neural networks that perform best empirically. The key step is to\\nintroduce gated games, an extension of convex games with similar convergence\\nproperties that capture the gating function of rectifiers. The main result is\\nthat rectifier convnets converge to a critical point at a rate controlled by\\nthe gated-regret of the units in the network. Corollaries of the main result\\ninclude: (i) a game-theoretic description of the representations learned by a\\nneural network; (ii) a logarithmic-regret algorithm for training neural nets;\\nand (iii) a formal setting for analyzing conditional computation in neural nets\\nthat can be applied to recently developed models of attention.',\n",
              " 'Churn analysis using deep convolutional neural networks and autoencoders\\nCustomer temporal behavioral data was represented as images in order to\\nperform churn prediction by leveraging deep learning architectures prominent in\\nimage classification. Supervised learning was performed on labeled data of over\\n6 million customers using deep convolutional neural networks, which achieved an\\nAUC of 0.743 on the test dataset using no more than 12 temporal features for\\neach customer. Unsupervised learning was conducted using autoencoders to better\\nunderstand the reasons for customer churn. Images that maximally activate the\\nhidden units of an autoencoder trained with churned customers reveal ample\\nopportunities for action to be taken to prevent churn among strong data, no\\nvoice users.',\n",
              " 'Developing an ICU scoring system with interaction terms using a genetic\\n  algorithm\\nICU mortality scoring systems attempt to predict patient mortality using\\npredictive models with various clinical predictors. Examples of such systems\\nare APACHE, SAPS and MPM. However, most such scoring systems do not actively\\nlook for and include interaction terms, despite physicians intuitively taking\\nsuch interactions into account when making a diagnosis. One barrier to\\nincluding such terms in predictive models is the difficulty of using most\\nvariable selection methods in high-dimensional datasets. A genetic algorithm\\nframework for variable selection with logistic regression models is used to\\nsearch for two-way interaction terms in a clinical dataset of adult ICU\\npatients, with separate models being built for each category of diagnosis upon\\nadmittance to the ICU. The models had good discrimination across all\\ncategories, with a weighted average AUC of 0.84 (>0.90 for several categories)\\nand the genetic algorithm was able to find several significant interaction\\nterms, which may be able to provide greater insight into mortality prediction\\nfor health practitioners. The GA selected models had improved performance\\nagainst stepwise selection and random forest models, and provides greater\\nflexibility in terms of variable selection by being able to optimize over any\\nmodeler-defined model performance metric instead of a specific variable\\nimportance metric.',\n",
              " 'Scale Normalization\\nOne of the difficulties of training deep neural networks is caused by\\nimproper scaling between layers. Scaling issues introduce exploding / gradient\\nproblems, and have typically been addressed by careful scale-preserving\\ninitialization. We investigate the value of preserving scale, or isometry,\\nbeyond the initial weights. We propose two methods of maintaing isometry, one\\nexact and one stochastic. Preliminary experiments show that for both\\ndeterminant and scale-normalization effectively speeds up learning. Results\\nsuggest that isometry is important in the beginning of learning, and\\nmaintaining it leads to faster learning.',\n",
              " 'Layer-wise learning of deep generative models\\nWhen using deep, multi-layered architectures to build generative models of\\ndata, it is difficult to train all layers at once. We propose a layer-wise\\ntraining procedure admitting a performance guarantee compared to the global\\noptimum. It is based on an optimistic proxy of future performance, the best\\nlatent marginal. We interpret auto-encoders in this setting as generative\\nmodels, by showing that they train a lower bound of this criterion. We test the\\nnew learning procedure against a state of the art method (stacked RBMs), and\\nfind it to improve performance. Both theory and experiments highlight the\\nimportance, when training deep architectures, of using an inference model (from\\ndata to hidden variables) richer than the generative model (from hidden\\nvariables to data).',\n",
              " 'Distributed optimization of deeply nested systems\\nIn science and engineering, intelligent processing of complex signals such as\\nimages, sound or language is often performed by a parameterized hierarchy of\\nnonlinear processing layers, sometimes biologically inspired. Hierarchical\\nsystems (or, more generally, nested systems) offer a way to generate complex\\nmappings using simple stages. Each layer performs a different operation and\\nachieves an ever more sophisticated representation of the input, as, for\\nexample, in an deep artificial neural network, an object recognition cascade in\\ncomputer vision or a speech front-end processing. Joint estimation of the\\nparameters of all the layers and selection of an optimal architecture is widely\\nconsidered to be a difficult numerical nonconvex optimization problem,\\ndifficult to parallelize for execution in a distributed computation\\nenvironment, and requiring significant human expert effort, which leads to\\nsuboptimal systems in practice. We describe a general mathematical strategy to\\nlearn the parameters and, to some extent, the architecture of nested systems,\\ncalled the method of auxiliary coordinates (MAC). This replaces the original\\nproblem involving a deeply nested function with a constrained problem involving\\na different function in an augmented space without nesting. The constrained\\nproblem may be solved with penalty-based methods using alternating optimization\\nover the parameters and the auxiliary coordinates. MAC has provable\\nconvergence, is easy to implement reusing existing algorithms for single\\nlayers, can be parallelized trivially and massively, applies even when\\nparameter derivatives are not available or not desirable, and is competitive\\nwith state-of-the-art nonlinear optimizers even in the serial computation\\nsetting, often providing reasonable models within a few iterations.',\n",
              " \"Understanding Boltzmann Machine and Deep Learning via A Confident\\n  Information First Principle\\nTypical dimensionality reduction methods focus on directly reducing the\\nnumber of random variables while retaining maximal variations in the data. In\\nthis paper, we consider the dimensionality reduction in parameter spaces of\\nbinary multivariate distributions. We propose a general\\nConfident-Information-First (CIF) principle to maximally preserve parameters\\nwith confident estimates and rule out unreliable or noisy parameters. Formally,\\nthe confidence of a parameter can be assessed by its Fisher information, which\\nestablishes a connection with the inverse variance of any unbiased estimate for\\nthe parameter via the Cram\\\\'{e}r-Rao bound. We then revisit Boltzmann machines\\n(BM) and theoretically show that both single-layer BM without hidden units\\n(SBM) and restricted BM (RBM) can be solidly derived using the CIF principle.\\nThis can not only help us uncover and formalize the essential parts of the\\ntarget density that SBM and RBM capture, but also suggest that the deep neural\\nnetwork consisting of several layers of RBM can be seen as the layer-wise\\napplication of CIF. Guided by the theoretical analysis, we develop a\\nsample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and\\na CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are\\nstudied in a series of density estimation experiments.\",\n",
              " 'Canonical dual solutions to nonconvex radial basis neural network\\n  optimization problem\\nRadial Basis Functions Neural Networks (RBFNNs) are tools widely used in\\nregression problems. One of their principal drawbacks is that the formulation\\ncorresponding to the training with the supervision of both the centers and the\\nweights is a highly non-convex optimization problem, which leads to some\\nfundamentally difficulties for traditional optimization theory and methods.\\n  This paper presents a generalized canonical duality theory for solving this\\nchallenging problem. We demonstrate that by sequential canonical dual\\ntransformations, the nonconvex optimization problem of the RBFNN can be\\nreformulated as a canonical dual problem (without duality gap). Both global\\noptimal solution and local extrema can be classified. Several applications to\\none of the most used Radial Basis Functions, the Gaussian function, are\\nillustrated. Our results show that even for one-dimensional case, the global\\nminimizer of the nonconvex problem may not be the best solution to the RBFNNs,\\nand the canonical dual theory is a promising tool for solving general neural\\nnetworks training problems.',\n",
              " \"On the Number of Linear Regions of Deep Neural Networks\\nWe study the complexity of functions computable by deep feedforward neural\\nnetworks with piecewise linear activations in terms of the symmetries and the\\nnumber of linear regions that they have. Deep networks are able to sequentially\\nmap portions of each layer's input-space to the same output. In this way, deep\\nmodels compute functions that react equally to complicated patterns of\\ndifferent inputs. The compositional structure of these functions enables them\\nto re-use pieces of computation exponentially often in terms of the network's\\ndepth. This paper investigates the complexity of such compositional maps and\\ncontributes new theoretical results regarding the advantage of depth for neural\\nnetworks with piecewise linear activation functions. In particular, our\\nanalysis is not specific to a single family of models, and as an example, we\\nemploy it for rectifier and maxout networks. We improve complexity bounds from\\npre-existing work and investigate the behavior of units in higher layers.\",\n",
              " 'Geometry and Expressive Power of Conditional Restricted Boltzmann\\n  Machines\\nConditional restricted Boltzmann machines are undirected stochastic neural\\nnetworks with a layer of input and output units connected bipartitely to a\\nlayer of hidden units. These networks define models of conditional probability\\ndistributions on the states of the output units given the states of the input\\nunits, parametrized by interaction weights and biases. We address the\\nrepresentational power of these models, proving results their ability to\\nrepresent conditional Markov random fields and conditional distributions with\\nrestricted supports, the minimal size of universal approximators, the maximal\\nmodel approximation errors, and on the dimension of the set of representable\\nconditional distributions. We contribute new tools for investigating\\nconditional probability models, which allow us to improve the results that can\\nbe derived from existing work on restricted Boltzmann machine probability\\nmodels.',\n",
              " 'Is Joint Training Better for Deep Auto-Encoders?\\nTraditionally, when generative models of data are developed via deep\\narchitectures, greedy layer-wise pre-training is employed. In a well-trained\\nmodel, the lower layer of the architecture models the data distribution\\nconditional upon the hidden variables, while the higher layers model the hidden\\ndistribution prior. But due to the greedy scheme of the layerwise training\\ntechnique, the parameters of lower layers are fixed when training higher\\nlayers. This makes it extremely challenging for the model to learn the hidden\\ndistribution prior, which in turn leads to a suboptimal model for the data\\ndistribution. We therefore investigate joint training of deep autoencoders,\\nwhere the architecture is viewed as one stack of two or more single-layer\\nautoencoders. A single global reconstruction objective is jointly optimized,\\nsuch that the objective for the single autoencoders at each layer acts as a\\nlocal, layer-level regularizer. We empirically evaluate the performance of this\\njoint training scheme and observe that it not only learns a better data model,\\nbut also learns better higher layer representations, which highlights its\\npotential for unsupervised feature learning. In addition, we find that the\\nusage of regularizations in the joint training scheme is crucial in achieving\\ngood performance. In the supervised setting, joint training also shows superior\\nperformance when training deeper models. The joint training framework can thus\\nprovide a platform for investigating more efficient usage of different types of\\nregularizers, especially in light of the growing volumes of available unlabeled\\ndata.',\n",
              " 'Massively Multitask Networks for Drug Discovery\\nMassively multitask neural architectures provide a learning framework for\\ndrug discovery that synthesizes information from many distinct biological\\nsources. To train these architectures at scale, we gather large amounts of data\\nfrom public sources to create a dataset of nearly 40 million measurements\\nacross more than 200 biological targets. We investigate several aspects of the\\nmultitask framework by performing a series of empirical studies and obtain some\\ninteresting results: (1) massively multitask networks obtain predictive\\naccuracies significantly better than single-task methods, (2) the predictive\\npower of multitask networks improves as additional tasks and data are added,\\n(3) the total amount of data and the total number of tasks both contribute\\nsignificantly to multitask improvement, and (4) multitask networks afford\\nlimited transferability to tasks not in the training set. Our results\\nunderscore the need for greater data sharing and further algorithmic innovation\\nto accelerate the drug discovery process.',\n",
              " 'Gated Feedback Recurrent Neural Networks\\nIn this work, we propose a novel recurrent neural network (RNN) architecture.\\nThe proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of\\nstacking multiple recurrent layers by allowing and controlling signals flowing\\nfrom upper recurrent layers to lower layers using a global gating unit for each\\npair of layers. The recurrent signals exchanged between layers are gated\\nadaptively based on the previous hidden states and the current input. We\\nevaluated the proposed GF-RNN with different types of recurrent units, such as\\ntanh, long short-term memory and gated recurrent units, on the tasks of\\ncharacter-level language modeling and Python program evaluation. Our empirical\\nevaluation of different RNN units, revealed that in both tasks, the GF-RNN\\noutperforms the conventional approaches to build deep stacked RNNs. We suggest\\nthat the improvement arises because the GF-RNN can adaptively assign different\\nlayers to different timescales and layer-to-layer interactions (including the\\ntop-down ones which are not usually present in a stacked RNN) by learning to\\ngate these interactions.',\n",
              " \"Deep Learning with Limited Numerical Precision\\nTraining of large-scale deep neural networks is often constrained by the\\navailable computational resources. We study the effect of limited precision\\ndata representation and computation on neural network training. Within the\\ncontext of low-precision fixed-point computations, we observe the rounding\\nscheme to play a crucial role in determining the network's behavior during\\ntraining. Our results show that deep networks can be trained using only 16-bit\\nwide fixed-point number representation when using stochastic rounding, and\\nincur little to no degradation in the classification accuracy. We also\\ndemonstrate an energy-efficient hardware accelerator that implements\\nlow-precision fixed-point arithmetic with stochastic rounding.\",\n",
              " \"MADE: Masked Autoencoder for Distribution Estimation\\nThere has been a lot of recent interest in designing neural network models to\\nestimate a distribution from a set of examples. We introduce a simple\\nmodification for autoencoder neural networks that yields powerful generative\\nmodels. Our method masks the autoencoder's parameters to respect autoregressive\\nconstraints: each input is reconstructed only from previous inputs in a given\\nordering. Constrained this way, the autoencoder outputs can be interpreted as a\\nset of conditional probabilities, and their product, the full joint\\nprobability. We can also train a single network that can decompose the joint\\nprobability in multiple different orderings. Our simple framework can be\\napplied to multiple architectures, including deep ones. Vectorized\\nimplementations, such as on GPUs, are simple and fast. Experiments demonstrate\\nthat this approach is competitive with state-of-the-art tractable distribution\\nestimators. At test time, the method is significantly faster and scales better\\nthan other autoregressive estimators.\",\n",
              " 'Simple, Efficient, and Neural Algorithms for Sparse Coding\\nSparse coding is a basic task in many fields including signal processing,\\nneuroscience and machine learning where the goal is to learn a basis that\\nenables a sparse representation of a given set of data, if one exists. Its\\nstandard formulation is as a non-convex optimization problem which is solved in\\npractice by heuristics based on alternating minimization. Re- cent work has\\nresulted in several algorithms for sparse coding with provable guarantees, but\\nsomewhat surprisingly these are outperformed by the simple alternating\\nminimization heuristics. Here we give a general framework for understanding\\nalternating minimization which we leverage to analyze existing heuristics and\\nto design new ones also with provable guarantees. Some of these algorithms seem\\nimplementable on simple neural architectures, which was the original motivation\\nof Olshausen and Field (1997a) in introducing sparse coding. We also give the\\nfirst efficient algorithm for sparse coding that works almost up to the\\ninformation theoretic limit for sparse recovery on incoherent dictionaries. All\\nprevious algorithms that approached or surpassed this limit run in time\\nexponential in some natural parameter. Finally, our algorithms improve upon the\\nsample complexity of existing approaches. We believe that our analysis\\nframework will have applications in other settings where simple iterative\\nalgorithms are used.',\n",
              " 'Toxicity Prediction using Deep Learning\\nEveryday we are exposed to various chemicals via food additives, cleaning and\\ncosmetic products and medicines -- and some of them might be toxic. However\\ntesting the toxicity of all existing compounds by biological experiments is\\nneither financially nor logistically feasible. Therefore the government\\nagencies NIH, EPA and FDA launched the Tox21 Data Challenge within the\\n\"Toxicology in the 21st Century\" (Tox21) initiative. The goal of this challenge\\nwas to assess the performance of computational methods in predicting the\\ntoxicity of chemical compounds. State of the art toxicity prediction methods\\nbuild upon specifically-designed chemical descriptors developed over decades.\\nThough Deep Learning is new to the field and was never applied to toxicity\\nprediction before, it clearly outperformed all other participating methods. In\\nthis application paper we show that deep nets automatically learn features\\nresembling well-established toxicophores. In total, our Deep Learning approach\\nwon both of the panel-challenges (nuclear receptors and stress response) as\\nwell as the overall Grand Challenge, and thereby sets a new standard in tox\\nprediction.',\n",
              " 'To Drop or Not to Drop: Robustness, Consistency and Differential Privacy\\n  Properties of Dropout\\nTraining deep belief networks (DBNs) requires optimizing a non-convex\\nfunction with an extremely large number of parameters. Naturally, existing\\ngradient descent (GD) based methods are prone to arbitrarily poor local minima.\\nIn this paper, we rigorously show that such local minima can be avoided (upto\\nan approximation error) by using the dropout technique, a widely used heuristic\\nin this domain. In particular, we show that by randomly dropping a few nodes of\\na one-hidden layer neural network, the training objective function, up to a\\ncertain approximation error, decreases by a multiplicative factor.\\n  On the flip side, we show that for training convex empirical risk minimizers\\n(ERM), dropout in fact acts as a \"stabilizer\" or regularizer. That is, a simple\\ndropout based GD method for convex ERMs is stable in the face of arbitrary\\nchanges to any one of the training points. Using the above assertion, we show\\nthat dropout provides fast rates for generalization error in learning (convex)\\ngeneralized linear models (GLM). Moreover, using the above mentioned stability\\nproperties of dropout, we design dropout based differentially private\\nalgorithms for solving ERMs. The learned GLM thus, preserves privacy of each of\\nthe individual training points while providing accurate predictions for new\\ntest points. Finally, we empirically validate our stability assertions for\\ndropout in the context of convex ERMs and show that surprisingly, dropout\\nsignificantly outperforms (in terms of prediction accuracy) the L2\\nregularization based methods for several benchmark datasets.',\n",
              " 'Distilling the Knowledge in a Neural Network\\nA very simple way to improve the performance of almost any machine learning\\nalgorithm is to train many different models on the same data and then to\\naverage their predictions. Unfortunately, making predictions using a whole\\nensemble of models is cumbersome and may be too computationally expensive to\\nallow deployment to a large number of users, especially if the individual\\nmodels are large neural nets. Caruana and his collaborators have shown that it\\nis possible to compress the knowledge in an ensemble into a single model which\\nis much easier to deploy and we develop this approach further using a different\\ncompression technique. We achieve some surprising results on MNIST and we show\\nthat we can significantly improve the acoustic model of a heavily used\\ncommercial system by distilling the knowledge in an ensemble of models into a\\nsingle model. We also introduce a new type of ensemble composed of one or more\\nfull models and many specialist models which learn to distinguish fine-grained\\nclasses that the full models confuse. Unlike a mixture of experts, these\\nspecialist models can be trained rapidly and in parallel.',\n",
              " 'A mathematical motivation for complex-valued convolutional networks\\nA complex-valued convolutional network (convnet) implements the repeated\\napplication of the following composition of three operations, recursively\\napplying the composition to an input vector of nonnegative real numbers: (1)\\nconvolution with complex-valued vectors followed by (2) taking the absolute\\nvalue of every entry of the resulting vectors followed by (3) local averaging.\\nFor processing real-valued random vectors, complex-valued convnets can be\\nviewed as \"data-driven multiscale windowed power spectra,\" \"data-driven\\nmultiscale windowed absolute spectra,\" \"data-driven multiwavelet absolute\\nvalues,\" or (in their most general configuration) \"data-driven nonlinear\\nmultiwavelet packets.\" Indeed, complex-valued convnets can calculate multiscale\\nwindowed spectra when the convnet filters are windowed complex-valued\\nexponentials. Standard real-valued convnets, using rectified linear units\\n(ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max.\\npooling, etc., do not obviously exhibit the same exact correspondence with\\ndata-driven wavelets (whereas for complex-valued convnets, the correspondence\\nis much more than just a vague analogy). Courtesy of the exact correspondence,\\nthe remarkably rich and rigorous body of mathematical analysis for wavelets\\napplies directly to (complex-valued) convnets.',\n",
              " \"Optimizing Neural Networks with Kronecker-factored Approximate Curvature\\nWe propose an efficient method for approximating natural gradient descent in\\nneural networks which we call Kronecker-Factored Approximate Curvature (K-FAC).\\nK-FAC is based on an efficiently invertible approximation of a neural network's\\nFisher information matrix which is neither diagonal nor low-rank, and in some\\ncases is completely non-sparse. It is derived by approximating various large\\nblocks of the Fisher (corresponding to entire layers) as being the Kronecker\\nproduct of two much smaller matrices. While only several times more expensive\\nto compute than the plain stochastic gradient, the updates produced by K-FAC\\nmake much more progress optimizing the objective, which results in an algorithm\\nthat can be much faster than stochastic gradient descent with momentum in\\npractice. And unlike some previously proposed approximate\\nnatural-gradient/Newton methods which use high-quality non-diagonal curvature\\nmatrices (such as Hessian-free optimization), K-FAC works very well in highly\\nstochastic optimization regimes. This is because the cost of storing and\\ninverting K-FAC's approximation to the curvature matrix does not depend on the\\namount of data used to estimate it, which is a feature typically associated\\nonly with diagonal or low-rank approximations to the curvature matrix.\",\n",
              " 'Unsupervised model compression for multilayer bootstrap networks\\nRecently, multilayer bootstrap network (MBN) has demonstrated promising\\nperformance in unsupervised dimensionality reduction. It can learn compact\\nrepresentations in standard data sets, i.e. MNIST and RCV1. However, as a\\nbootstrap method, the prediction complexity of MBN is high. In this paper, we\\npropose an unsupervised model compression framework for this general problem of\\nunsupervised bootstrap methods. The framework compresses a large unsupervised\\nbootstrap model into a small model by taking the bootstrap model and its\\napplication together as a black box and learning a mapping function from the\\ninput of the bootstrap model to the output of the application by a supervised\\nlearner. To specialize the framework, we propose a new technique, named\\ncompressive MBN. It takes MBN as the unsupervised bootstrap model and deep\\nneural network (DNN) as the supervised learner. Our initial result on MNIST\\nshowed that compressive MBN not only maintains the high prediction accuracy of\\nMBN but also is over thousands of times faster than MBN at the prediction\\nstage. Our result suggests that the new technique integrates the effectiveness\\nof MBN on unsupervised learning and the effectiveness and efficiency of DNN on\\nsupervised learning together for the effectiveness and efficiency of\\ncompressive MBN on unsupervised learning.',\n",
              " 'Positive blood culture detection in time series data using a BiLSTM\\n  network\\nThe presence of bacteria or fungi in the bloodstream of patients is abnormal\\nand can lead to life-threatening conditions. A computational model based on a\\nbidirectional long short-term memory artificial neural network, is explored to\\nassist doctors in the intensive care unit to predict whether examination of\\nblood cultures of patients will return positive. As input it uses nine\\nmonitored clinical parameters, presented as time series data, collected from\\n2177 ICU admissions at the Ghent University Hospital. Our main goal is to\\ndetermine if general machine learning methods and more specific, temporal\\nmodels, can be used to create an early detection system. This preliminary\\nresearch obtains an area of 71.95% under the precision recall curve, proving\\nthe potential of temporal neural networks in this context.',\n",
              " 'Known Unknowns: Uncertainty Quality in Bayesian Neural Networks\\nWe evaluate the uncertainty quality in neural networks using anomaly\\ndetection. We extract uncertainty measures (e.g. entropy) from the predictions\\nof candidate models, use those measures as features for an anomaly detector,\\nand gauge how well the detector differentiates known from unknown classes. We\\nassign higher uncertainty quality to candidate models that lead to better\\ndetectors. We also propose a novel method for sampling a variational\\napproximation of a Bayesian neural network, called One-Sample Bayesian\\nApproximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We\\ncompare the following candidate neural network models: Maximum Likelihood,\\nBayesian Dropout, OSBA, and --- for MNIST --- the standard variational\\napproximation. We show that Bayesian Dropout and OSBA provide better\\nuncertainty information than Maximum Likelihood, and are essentially equivalent\\nto the standard variational approximation, but much faster.',\n",
              " 'Semi-Supervised Learning with the Deep Rendering Mixture Model\\nSemi-supervised learning algorithms reduce the high cost of acquiring labeled\\ntraining data by using both labeled and unlabeled data during learning. Deep\\nConvolutional Networks (DCNs) have achieved great success in supervised tasks\\nand as such have been widely employed in the semi-supervised learning. In this\\npaper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a\\nprobabilistic generative model that models latent nuisance variation, and whose\\ninference algorithm yields DCNs. We develop an EM algorithm for the DRMM to\\nlearn from both labeled and unlabeled data. Guided by the theory of the DRMM,\\nwe introduce a novel non-negativity constraint and a variational inference\\nterm. We report state-of-the-art performance on MNIST and SVHN and competitive\\nresults on CIFAR10. We also probe deeper into how a DRMM trained in a\\nsemi-supervised setting represents latent nuisance variation using\\nsynthetically rendered images. Taken together, our work provides a unified\\nframework for supervised, unsupervised, and semi-supervised learning.',\n",
              " 'Self-calibrating Neural Networks for Dimensionality Reduction\\nRecently, a novel family of biologically plausible online algorithms for\\nreducing the dimensionality of streaming data has been derived from the\\nsimilarity matching principle. In these algorithms, the number of output\\ndimensions can be determined adaptively by thresholding the singular values of\\nthe input data matrix. However, setting such threshold requires knowing the\\nmagnitude of the desired singular values in advance. Here we propose online\\nalgorithms where the threshold is self-calibrating based on the singular values\\ncomputed from the existing observations. To derive these algorithms from the\\nsimilarity matching cost function we propose novel regularizers. As before,\\nthese online algorithms can be implemented by Hebbian/anti-Hebbian neural\\nnetworks in which the learning rule depends on the chosen regularizer. We\\ndemonstrate both mathematically and via simulation the effectiveness of these\\nonline algorithms in various settings.',\n",
              " 'Tunable Efficient Unitary Neural Networks (EUNN) and their application\\n  to RNNs\\nUsing unitary (instead of general) matrices in artificial neural networks\\n(ANNs) is a promising way to solve the gradient explosion/vanishing problem, as\\nwell as to enable ANNs to learn long-term correlations in the data. This\\napproach appears particularly promising for Recurrent Neural Networks (RNNs).\\nIn this work, we present a new architecture for implementing an Efficient\\nUnitary Neural Network (EUNNs); its main advantages can be summarized as\\nfollows. Firstly, the representation capacity of the unitary space in an EUNN\\nis fully tunable, ranging from a subspace of SU(N) to the entire unitary space.\\nSecondly, the computational complexity for training an EUNN is merely\\n$\\\\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on\\nthe standard copying task, the pixel-permuted MNIST digit recognition benchmark\\nas well as the Speech Prediction Test (TIMIT). We find that our architecture\\nsignificantly outperforms both other state-of-the-art unitary RNNs and the LSTM\\narchitecture, in terms of the final performance and/or the wall-clock training\\nspeed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide\\nvariety of applications.',\n",
              " 'Sequence Transduction with Recurrent Neural Networks\\nMany machine learning tasks can be expressed as the transformation---or\\n\\\\emph{transduction}---of input sequences into output sequences: speech\\nrecognition, machine translation, protein secondary structure prediction and\\ntext-to-speech to name but a few. One of the key challenges in sequence\\ntransduction is learning to represent both the input and output sequences in a\\nway that is invariant to sequential distortions such as shrinking, stretching\\nand translating. Recurrent neural networks (RNNs) are a powerful sequence\\nlearning architecture that has proven capable of learning such representations.\\nHowever RNNs traditionally require a pre-defined alignment between the input\\nand output sequences to perform transduction. This is a severe limitation since\\n\\\\emph{finding} the alignment is the most difficult aspect of many sequence\\ntransduction problems. Indeed, even determining the length of the output\\nsequence is often challenging. This paper introduces an end-to-end,\\nprobabilistic sequence transduction system, based entirely on RNNs, that is in\\nprinciple able to transform any input sequence into any finite, discrete output\\nsequence. Experimental results for phoneme recognition are provided on the\\nTIMIT speech corpus.',\n",
              " 'On Fast Dropout and its Applicability to Recurrent Networks\\nRecurrent Neural Networks (RNNs) are rich models for the processing of\\nsequential data. Recent work on advancing the state of the art has been focused\\non the optimization or modelling of RNNs, mostly motivated by adressing the\\nproblems of the vanishing and exploding gradients. The control of overfitting\\nhas seen considerably less attention. This paper contributes to that by\\nanalyzing fast dropout, a recent regularization method for generalized linear\\nmodels and neural networks from a back-propagation inspired perspective. We\\nshow that fast dropout implements a quadratic form of an adaptive,\\nper-parameter regularizer, which rewards large weights in the light of\\nunderfitting, penalizes them for overconfident predictions and vanishes at\\nminima of an unregularized training loss. The derivatives of that regularizer\\nare exclusively based on the training error signal. One consequence of this is\\nthe absense of a global weight attractor, which is particularly appealing for\\nRNNs, since the dynamics are not biased towards a certain regime. We positively\\ntest the hypothesis that this improves the performance of RNNs on four musical\\ndata sets.',\n",
              " 'Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks\\nIn this paper we propose and investigate a novel nonlinear unit, called $L_p$\\nunit, for deep neural networks. The proposed $L_p$ unit receives signals from\\nseveral projections of a subset of units in the layer below and computes a\\nnormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$\\nunit. First, the proposed unit can be understood as a generalization of a\\nnumber of conventional pooling operators such as average, root-mean-square and\\nmax pooling widely used in, for instance, convolutional neural networks (CNN),\\nHMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain\\ndegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)\\nwhich achieved the state-of-the-art object recognition results on a number of\\nbenchmark datasets. Secondly, we provide a geometrical interpretation of the\\nactivation function based on which we argue that the $L_p$ unit is more\\nefficient at representing complex, nonlinear separating boundaries. Each $L_p$\\nunit defines a superelliptic boundary, with its exact shape defined by the\\norder $p$. We claim that this makes it possible to model arbitrarily shaped,\\ncurved boundaries more efficiently by combining a few $L_p$ units of different\\norders. This insight justifies the need for learning different orders for each\\nunit in the model. We empirically evaluate the proposed $L_p$ units on a number\\nof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$\\nunits achieve the state-of-the-art results on a number of benchmark datasets.\\nFurthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep\\nrecurrent neural networks (RNN).',\n",
              " 'Missing Value Imputation With Unsupervised Backpropagation\\nMany data mining and data analysis techniques operate on dense matrices or\\ncomplete tables of data. Real-world data sets, however, often contain unknown\\nvalues. Even many classification algorithms that are designed to operate with\\nmissing values still exhibit deteriorated accuracy. One approach to handling\\nmissing values is to fill in (impute) the missing values. In this paper, we\\npresent a technique for unsupervised learning called Unsupervised\\nBackpropagation (UBP), which trains a multi-layer perceptron to fit to the\\nmanifold sampled by a set of observed point-vectors. We evaluate UBP with the\\ntask of imputing missing values in datasets, and show that UBP is able to\\npredict missing values with significantly lower sum-squared error than other\\ncollaborative filtering and imputation techniques. We also demonstrate with 24\\ndatasets and 9 supervised learning algorithms that classification accuracy is\\nusually higher when randomly-withheld values are imputed using UBP, rather than\\nwith other methods.',\n",
              " 'Stochastic Gradient Estimate Variance in Contrastive Divergence and\\n  Persistent Contrastive Divergence\\nContrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are\\npopular methods for training the weights of Restricted Boltzmann Machines.\\nHowever, both methods use an approximate method for sampling from the model\\ndistribution. As a side effect, these approximations yield significantly\\ndifferent biases and variances for stochastic gradient estimates of individual\\ndata points. It is well known that CD yields a biased gradient estimate. In\\nthis paper we however show empirically that CD has a lower stochastic gradient\\nestimate variance than exact sampling, while the mean of subsequent PCD\\nestimates has a higher variance than exact sampling. The results give one\\nexplanation to the finding that CD can be used with smaller minibatches or\\nhigher learning rates than PCD.',\n",
              " 'How to Construct Deep Recurrent Neural Networks\\nIn this paper, we explore different ways to extend a recurrent neural network\\n(RNN) to a \\\\textit{deep} RNN. We start by arguing that the concept of depth in\\nan RNN is not as clear as it is in feedforward neural networks. By carefully\\nanalyzing and understanding the architecture of an RNN, however, we find three\\npoints of an RNN which may be made deeper; (1) input-to-hidden function, (2)\\nhidden-to-hidden transition and (3) hidden-to-output function. Based on this\\nobservation, we propose two novel architectures of a deep RNN which are\\northogonal to an earlier attempt of stacking multiple recurrent layers to build\\na deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an\\nalternative interpretation of these deep RNNs using a novel framework based on\\nneural operators. The proposed deep RNNs are empirically evaluated on the tasks\\nof polyphonic music prediction and language modeling. The experimental result\\nsupports our claim that the proposed deep RNNs benefit from the depth and\\noutperform the conventional, shallow RNNs.',\n",
              " 'Neuronal Synchrony in Complex-Valued Deep Networks\\nDeep learning has recently led to great successes in tasks such as image\\nrecognition (e.g Krizhevsky et al., 2012). However, deep networks are still\\noutmatched by the power and versatility of the brain, perhaps in part due to\\nthe richer neuronal computations available to cortical circuits. The challenge\\nis to identify which neuronal mechanisms are relevant, and to find suitable\\nabstractions to model them. Here, we show how aspects of spike timing, long\\nhypothesized to play a crucial role in cortical information processing, could\\nbe incorporated into deep networks to build richer, versatile representations.\\n  We introduce a neural network formulation based on complex-valued neuronal\\nunits that is not only biologically meaningful but also amenable to a variety\\nof deep learning frameworks. Here, units are attributed both a firing rate and\\na phase, the latter indicating properties of spike timing. We show how this\\nformulation qualitatively captures several aspects thought to be related to\\nneuronal synchrony, including gating of information processing and dynamic\\nbinding of distributed object representations. Focusing on the latter, we\\ndemonstrate the potential of the approach in several simple experiments. Thus,\\nneuronal synchrony could be a flexible mechanism that fulfills multiple\\nfunctional roles in deep networks.',\n",
              " 'An empirical analysis of dropout in piecewise linear networks\\nThe recently introduced dropout training criterion for neural networks has\\nbeen the subject of much attention due to its simplicity and remarkable\\neffectiveness as a regularizer, as well as its interpretation as a training\\nprocedure for an exponentially large ensemble of networks that share\\nparameters. In this work we empirically investigate several questions related\\nto the efficacy of dropout, specifically as it concerns networks employing the\\npopular rectified linear activation function. We investigate the quality of the\\ntest time weight-scaling inference procedure by evaluating the geometric\\naverage exactly in small models, as well as compare the performance of the\\ngeometric mean to the arithmetic mean more commonly employed by ensemble\\ntechniques. We explore the effect of tied weights on the ensemble\\ninterpretation by training ensembles of masked networks without tied weights.\\nFinally, we investigate an alternative criterion based on a biased estimator of\\nthe maximum likelihood ensemble gradient.',\n",
              " 'An Empirical Investigation of Catastrophic Forgetting in Gradient-Based\\n  Neural Networks\\nCatastrophic forgetting is a problem faced by many machine learning models\\nand algorithms. When trained on one task, then trained on a second task, many\\nmachine learning models \"forget\" how to perform the first task. This is widely\\nbelieved to be a serious problem for neural networks. Here, we investigate the\\nextent to which the catastrophic forgetting problem occurs for modern neural\\nnetworks, comparing both established and recent gradient-based training\\nalgorithms and activation functions. We also examine the effect of the\\nrelationship between the first task and the second task on catastrophic\\nforgetting. We find that it is always best to train using the dropout\\nalgorithm--the dropout algorithm is consistently best at adapting to the new\\ntask, remembering the old task, and has the best tradeoff curve between these\\ntwo extremes. We find that different tasks and relationships between tasks\\nresult in very different rankings of activation function performance. This\\nsuggests the choice of activation function should always be cross-validated.',\n",
              " 'Unsupervised Domain Adaptation by Backpropagation\\nTop-performing deep architectures are trained on massive amounts of labeled\\ndata. In the absence of labeled data for a certain task, domain adaptation\\noften provides an attractive option given that labeled data of similar nature\\nbut from a different domain (e.g. synthetic images) are available. Here, we\\npropose a new approach to domain adaptation in deep architectures that can be\\ntrained on large amount of labeled data from the source domain and large amount\\nof unlabeled data from the target domain (no labeled target-domain data is\\nnecessary).\\n  As the training progresses, the approach promotes the emergence of \"deep\"\\nfeatures that are (i) discriminative for the main learning task on the source\\ndomain and (ii) invariant with respect to the shift between the domains. We\\nshow that this adaptation behaviour can be achieved in almost any feed-forward\\nmodel by augmenting it with few standard layers and a simple new gradient\\nreversal layer. The resulting augmented architecture can be trained using\\nstandard backpropagation.\\n  Overall, the approach can be implemented with little effort using any of the\\ndeep-learning packages. The method performs very well in a series of image\\nclassification experiments, achieving adaptation effect in the presence of big\\ndomain shifts and outperforming previous state-of-the-art on Office datasets.',\n",
              " 'Deep Directed Generative Autoencoders\\nFor discrete data, the likelihood $P(x)$ can be rewritten exactly and\\nparametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$\\nhas enough capacity to put no probability mass on any $x\\'$ for which $f(x\\')\\\\neq\\nf(x)$, where $f(\\\\cdot)$ is a deterministic discrete function. The log of the\\nfirst factor gives rise to the log-likelihood reconstruction error of an\\nautoencoder with $f(\\\\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic)\\ndecoder. The log of the second term can be seen as a regularizer on the encoded\\nactivations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decoder\\ncan be represented by a deep neural network and trained to maximize the average\\nof the optimal log-likelihood $\\\\log p(x)$. The objective is to learn an encoder\\n$f(\\\\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than\\n$X$ itself, estimated by $P(H)$. This \"flattens the manifold\" or concentrates\\nprobability mass in a smaller number of (relevant) dimensions over which the\\ndistribution factorizes. Generating samples from the model is straightforward\\nusing ancestral sampling. One challenge is that regular back-propagation cannot\\nbe used to obtain the gradient on the parameters of the encoder, but we find\\nthat using the straight-through estimator works well here. We also find that\\nalthough optimizing a single level of such architecture may be difficult, much\\nbetter results can be obtained by pre-training and stacking them, gradually\\ntransforming the data distribution into one that is more easily captured by a\\nsimple parametric model.',\n",
              " 'An exact mapping between the Variational Renormalization Group and Deep\\n  Learning\\nDeep learning is a broad set of techniques that uses multiple layers of\\nrepresentation to automatically learn relevant features directly from\\nstructured data. Recently, such techniques have yielded record-breaking results\\non a diverse set of difficult machine learning tasks in computer vision, speech\\nrecognition, and natural language processing. Despite the enormous success of\\ndeep learning, relatively little is understood theoretically about why these\\ntechniques are so successful at feature learning and compression. Here, we show\\nthat deep learning is intimately related to one of the most important and\\nsuccessful techniques in theoretical physics, the renormalization group (RG).\\nRG is an iterative coarse-graining scheme that allows for the extraction of\\nrelevant features (i.e. operators) as a physical system is examined at\\ndifferent length scales. We construct an exact mapping from the variational\\nrenormalization group, first introduced by Kadanoff, and deep learning\\narchitectures based on Restricted Boltzmann Machines (RBMs). We illustrate\\nthese ideas using the nearest-neighbor Ising Model in one and two-dimensions.\\nOur results suggests that deep learning algorithms may be employing a\\ngeneralized RG-like scheme to learn relevant features from data.',\n",
              " 'Non-parametric Bayesian Learning with Deep Learning Structure and Its\\n  Applications in Wireless Networks\\nIn this paper, we present an infinite hierarchical non-parametric Bayesian\\nmodel to extract the hidden factors over observed data, where the number of\\nhidden factors for each layer is unknown and can be potentially infinite.\\nMoreover, the number of layers can also be infinite. We construct the model\\nstructure that allows continuous values for the hidden factors and weights,\\nwhich makes the model suitable for various applications. We use the\\nMetropolis-Hastings method to infer the model structure. Then the performance\\nof the algorithm is evaluated by the experiments. Simulation results show that\\nthe model fits the underlying structure of simulated data.',\n",
              " 'Parallel training of DNNs with Natural Gradient and Parameter Averaging\\nWe describe the neural-network training framework used in the Kaldi speech\\nrecognition toolkit, which is geared towards training DNNs with large amounts\\nof training data using multiple GPU-equipped or multi-core machines. In order\\nto be as hardware-agnostic as possible, we needed a way to use multiple\\nmachines without generating excessive network traffic. Our method is to average\\nthe neural network parameters periodically (typically every minute or two), and\\nredistribute the averaged parameters to the machines for further training. Each\\nmachine sees different data. By itself, this method does not work very well.\\nHowever, we have another method, an approximate and efficient implementation of\\nNatural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow\\nour periodic-averaging method to work well, as well as substantially improving\\nthe convergence of SGD on a single machine.',\n",
              " 'End-to-end Continuous Speech Recognition using Attention-based Recurrent\\n  NN: First Results\\nWe replace the Hidden Markov Model (HMM) which is traditionally used in in\\ncontinuous speech recognition with a bi-directional recurrent neural network\\nencoder coupled to a recurrent neural network decoder that directly emits a\\nstream of phonemes. The alignment between the input and output sequences is\\nestablished using an attention mechanism: the decoder emits each symbol based\\non a context created with a subset of input symbols elected by the attention\\nmechanism. We report initial results demonstrating that this new approach\\nachieves phoneme error rates that are comparable to the state-of-the-art\\nHMM-based decoders, on the TIMIT dataset.',\n",
              " 'Provable Methods for Training Neural Networks with Sparse Connectivity\\nWe provide novel guaranteed approaches for training feedforward neural\\nnetworks with sparse connectivity. We leverage on the techniques developed\\npreviously for learning linear networks and show that they can also be\\neffectively adopted to learn non-linear networks. We operate on the moments\\ninvolving label and the score function of the input, and show that their\\nfactorization provably yields the weight matrix of the first layer of a deep\\nnetwork under mild conditions. In practice, the output of our method can be\\nemployed as effective initializers for gradient descent.',\n",
              " 'Domain-Adversarial Neural Networks\\nWe introduce a new representation learning algorithm suited to the context of\\ndomain adaptation, in which data at training and test time come from similar\\nbut different distributions. Our algorithm is directly inspired by theory on\\ndomain adaptation suggesting that, for effective domain transfer to be\\nachieved, predictions must be made based on a data representation that cannot\\ndiscriminate between the training (source) and test (target) domains. We\\npropose a training objective that implements this idea in the context of a\\nneural network, whose hidden layer is trained to be predictive of the\\nclassification task, but uninformative as to the domain of the input. Our\\nexperiments on a sentiment analysis classification benchmark, where the target\\ndomain data available at training time is unlabeled, show that our neural\\nnetwork for domain adaption algorithm has better performance than either a\\nstandard neural network or an SVM, even if trained on input features extracted\\nwith the state-of-the-art marginalized stacked denoising autoencoders of Chen\\net al. (2012).',\n",
              " 'Learning with Pseudo-Ensembles\\nWe formalize the notion of a pseudo-ensemble, a (possibly infinite)\\ncollection of child models spawned from a parent model by perturbing it\\naccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep\\nneural network trains a pseudo-ensemble of child subnetworks generated by\\nrandomly masking nodes in the parent network. We present a novel regularizer\\nbased on making the behavior of a pseudo-ensemble robust with respect to the\\nnoise process generating it. In the fully-supervised setting, our regularizer\\nmatches the performance of dropout. But, unlike dropout, our regularizer\\nnaturally extends to the semi-supervised setting, where it produces\\nstate-of-the-art results. We provide a case study in which we transform the\\nRecursive Neural Tensor Network of (Socher et. al, 2013) into a\\npseudo-ensemble, which significantly improves its performance on a real-world\\nsentiment analysis benchmark.',\n",
              " 'Random Walk Initialization for Training Very Deep Feedforward Networks\\nTraining very deep networks is an important open problem in machine learning.\\nOne of many difficulties is that the norm of the back-propagated error gradient\\ncan grow or decay exponentially. Here we show that training very deep\\nfeed-forward networks (FFNs) is not as difficult as previously thought. Unlike\\nwhen back-propagation is applied to a recurrent network, application to an FFN\\namounts to multiplying the error gradient by a different random matrix at each\\nlayer. We show that the successive application of correctly scaled random\\nmatrices to an initial vector results in a random walk of the log of the norm\\nof the resulting vectors, and we compute the scaling that makes this walk\\nunbiased. The variance of the random walk grows only linearly with network\\ndepth and is inversely proportional to the size of each layer. Practically,\\nthis implies a gradient whose log-norm scales with the square root of the\\nnetwork depth and shows that the vanishing gradient problem can be mitigated by\\nincreasing the width of the layers. Mathematical analyses and experimental\\nresults using stochastic gradient descent to optimize tasks related to the\\nMNIST and TIMIT datasets are provided to support these claims. Equations for\\nthe optimal matrix scaling are provided for the linear and ReLU cases.',\n",
              " 'Variational Recurrent Auto-Encoders\\nIn this paper we propose a model that combines the strengths of RNNs and\\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\\nfor efficient, large scale unsupervised learning on time series data, mapping\\nthe time series data to a latent vector representation. The model is\\ngenerative, such that data can be generated from samples of the latent space.\\nAn important contribution of this work is that the model can make use of\\nunlabeled data in order to facilitate supervised training of RNNs by\\ninitialising the weights and network state.',\n",
              " 'Neural Network Regularization via Robust Weight Factorization\\nRegularization is essential when training large neural networks. As deep\\nneural networks can be mathematically interpreted as universal function\\napproximators, they are effective at memorizing sampling noise in the training\\ndata. This results in poor generalization to unseen data. Therefore, it is no\\nsurprise that a new regularization technique, Dropout, was partially\\nresponsible for the now-ubiquitous winning entry to ImageNet 2012 by the\\nUniversity of Toronto. Currently, Dropout (and related methods such as\\nDropConnect) are the most effective means of regularizing large neural\\nnetworks. These amount to efficiently visiting a large number of related models\\nat training time, while aggregating them to a single predictor at test time.\\nThe proposed FaMe model aims to apply a similar strategy, yet learns a\\nfactorization of each weight matrix such that the factors are robust to noise.',\n",
              " 'A Bayesian encourages dropout\\nDropout is one of the key techniques to prevent the learning from\\noverfitting. It is explained that dropout works as a kind of modified L2\\nregularization. Here, we shed light on the dropout from Bayesian standpoint.\\nBayesian interpretation enables us to optimize the dropout rate, which is\\nbeneficial for learning of weight parameters and prediction after learning. The\\nexperiment result also encourages the optimization of the dropout.',\n",
              " 'Deep Fried Convnets\\nThe fully connected layers of a deep convolutional neural network typically\\ncontain over 90% of the network parameters, and consume the majority of the\\nmemory required to store the network parameters. Reducing the number of\\nparameters while preserving essentially the same predictive performance is\\ncritically important for operating deep neural networks in memory constrained\\nenvironments such as GPUs or embedded devices.\\n  In this paper we show how kernel methods, in particular a single Fastfood\\nlayer, can be used to replace all fully connected layers in a deep\\nconvolutional neural network. This novel Fastfood layer is also end-to-end\\ntrainable in conjunction with convolutional layers, allowing us to combine them\\ninto a new architecture, named deep fried convolutional networks, which\\nsubstantially reduces the memory footprint of convolutional networks trained on\\nMNIST and ImageNet with no drop in predictive performance.',\n",
              " 'Lateral Connections in Denoising Autoencoders Support Supervised\\n  Learning\\nWe show how a deep denoising autoencoder with lateral connections can be used\\nas an auxiliary unsupervised learning task to support supervised learning. The\\nproposed model is trained to minimize simultaneously the sum of supervised and\\nunsupervised cost functions by back-propagation, avoiding the need for\\nlayer-wise pretraining. It improves the state of the art significantly in the\\npermutation-invariant MNIST classification task.',\n",
              " 'Deep Neural Networks with Random Gaussian Weights: A Universal\\n  Classification Strategy?\\nThree important properties of a classification machinery are: (i) the system\\npreserves the core information of the input data; (ii) the training examples\\nconvey information about unseen data; and (iii) the system is able to treat\\ndifferently points from different classes. In this work we show that these\\nfundamental properties are satisfied by the architecture of deep neural\\nnetworks. We formally prove that these networks with random Gaussian weights\\nperform a distance-preserving embedding of the data, with a special treatment\\nfor in-class and out-of-class data. Similar points at the input of the network\\nare likely to have a similar output. The theoretical analysis of deep networks\\nhere presented exploits tools used in the compressed sensing and dictionary\\nlearning literature, thereby making a formal connection between these important\\ntopics. The derived results allow drawing conclusions on the metric learning\\nproperties of the network and their relation to its structure, as well as\\nproviding bounds on the required size of the training set such that the\\ntraining examples would represent faithfully the unseen data. The results are\\nvalidated with state-of-the-art trained networks.',\n",
              " 'Imaging Time-Series to Improve Classification and Imputation\\nInspired by recent successes of deep learning in computer vision, we propose\\na novel framework for encoding time series as different types of images,\\nnamely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov\\nTransition Fields (MTF). This enables the use of techniques from computer\\nvision for time series classification and imputation. We used Tiled\\nConvolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn\\nhigh-level features from the individual and compound GASF-GADF-MTF images. Our\\napproaches achieve highly competitive results when compared to nine of the\\ncurrent best time series classification approaches. Inspired by the bijection\\nproperty of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on\\nthe GASF images of four standard and one synthesized compound dataset. The\\nimputation MSE on test data is reduced by 12.18%-48.02% when compared to using\\nthe raw data. An analysis of the features and weights learned via tiled CNNs\\nand DAs explains why the approaches work.',\n",
              " \"Blocks and Fuel: Frameworks for deep learning\\nWe introduce two Python frameworks to train neural networks on large\\ndatasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler\\nwith CUDA-support. It facilitates the training of complex neural network models\\nby providing parametrized Theano operations, attaching metadata to Theano's\\nsymbolic computational graph, and providing an extensive set of utilities to\\nassist training the networks, e.g. training algorithms, logging, monitoring,\\nvisualization, and serialization. Fuel provides a standard format for machine\\nlearning datasets. It allows the user to easily iterate over large datasets,\\nperforming many types of pre-processing on the fly.\",\n",
              " 'Adaptive Normalized Risk-Averting Training For Deep Neural Networks\\nThis paper proposes a set of new error criteria and learning approaches,\\nAdaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex\\noptimization problem in training deep neural networks (DNNs). Theoretically, we\\ndemonstrate its effectiveness on global and local convexity lower-bounded by\\nthe standard $L_p$-norm error. By analyzing the gradient on the convexity index\\n$\\\\lambda$, we explain the reason why to learn $\\\\lambda$ adaptively using\\ngradient descent works. In practice, we show how this method improves training\\nof deep neural networks to solve visual recognition tasks on the MNIST and\\nCIFAR-10 datasets. Without using pretraining or other tricks, we obtain results\\ncomparable or superior to those reported in recent literature on the same tasks\\nusing standard ConvNets + MSE/cross entropy. Performance on deep/shallow\\nmultilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can\\nbe combined with other quasi-Newton training methods, innovative network\\nvariants, regularization techniques and other specific tricks in DNNs. Other\\nthan unsupervised pretraining, it provides a new perspective to address the\\nnon-convex optimization problem in DNNs.',\n",
              " 'Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer\\n  Free Energy\\nRestricted Boltzmann machines are undirected neural networks which have been\\nshown to be effective in many applications, including serving as\\ninitializations for training deep multi-layer neural networks. One of the main\\nreasons for their success is the existence of efficient and practical\\nstochastic algorithms, such as contrastive divergence, for unsupervised\\ntraining. We propose an alternative deterministic iterative procedure based on\\nan improved mean field method from statistical physics known as the\\nThouless-Anderson-Palmer approach. We demonstrate that our algorithm provides\\nperformance equal to, and sometimes superior to, persistent contrastive\\ndivergence, while also providing a clear and easy to evaluate objective\\nfunction. We believe that this strategy can be easily generalized to other\\nmodels as well as to more accurate higher-order approximations, paving the way\\nfor systematic improvements in training Boltzmann machines with hidden units.',\n",
              " 'Pointer Networks\\nWe introduce a new neural architecture to learn the conditional probability\\nof an output sequence with elements that are discrete tokens corresponding to\\npositions in an input sequence. Such problems cannot be trivially addressed by\\nexistent approaches such as sequence-to-sequence and Neural Turing Machines,\\nbecause the number of target classes in each step of the output depends on the\\nlength of the input, which is variable. Problems such as sorting variable sized\\nsequences, and various combinatorial optimization problems belong to this\\nclass. Our model solves the problem of variable size output dictionaries using\\na recently proposed mechanism of neural attention. It differs from the previous\\nattention attempts in that, instead of using attention to blend hidden units of\\nan encoder to a context vector at each decoder step, it uses attention as a\\npointer to select a member of the input sequence as the output. We call this\\narchitecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn\\napproximate solutions to three challenging geometric problems -- finding planar\\nconvex hulls, computing Delaunay triangulations, and the planar Travelling\\nSalesman Problem -- using training examples alone. Ptr-Nets not only improve\\nover sequence-to-sequence with input attention, but also allow us to generalize\\nto variable size output dictionaries. We show that the learnt models generalize\\nbeyond the maximum lengths they were trained on. We hope our results on these\\ntasks will encourage a broader exploration of neural learning for discrete\\nproblems.',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "data['text_feature'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "K8u8nZIUPxT1",
        "outputId": "563ee063-66e9-4d61-e224-ab22c4059373"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([9.1700e+02, 7.7200e+03, 1.5965e+04, 1.1266e+04, 4.1430e+03,\n",
              "        9.6100e+02, 1.9000e+01, 4.0000e+00, 2.0000e+00, 3.0000e+00]),\n",
              " array([  66. ,  430.4,  794.8, 1159.2, 1523.6, 1888. , 2252.4, 2616.8,\n",
              "        2981.2, 3345.6, 3710. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist([len(feature) for feature in data['text_feature'].to_list()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NExR1eHVPxT1"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "ak9ycViMPxT1",
        "outputId": "a8f679ed-c949-40d7-bc0d-b43a573173f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  category  category_index                                             author  \\\n",
              "0    cs.AI              15  [{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...   \n",
              "1    cs.CL              20  [{'name': 'Ji Young Lee'}, {'name': 'Franck De...   \n",
              "2    cs.CL              20  [{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...   \n",
              "3  stat.ML             124  [{'name': 'Sebastian Ruder'}, {'name': 'Joachi...   \n",
              "4    cs.CL              20  [{'name': 'Iulian V. Serban'}, {'name': 'Chinn...   \n",
              "\n",
              "   day            id                                               link  \\\n",
              "0    1  1802.00209v1  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "1   12  1603.03827v1  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "2    2  1606.00776v2  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "3   23  1705.08142v2  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "4    7  1709.02349v2  [{'rel': 'alternate', 'href': 'http://arxiv.or...   \n",
              "\n",
              "   month                                            summary  \\\n",
              "0      2  We propose an architecture for VQA which utili...   \n",
              "1      3  Recent approaches based on artificial neural n...   \n",
              "2      6  We introduce the multiresolution recurrent neu...   \n",
              "3      5  Multi-task learning is motivated by the observ...   \n",
              "4      9  We present MILABOT: a deep reinforcement learn...   \n",
              "\n",
              "                                                 tag  \\\n",
              "0  [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n",
              "1  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "2  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "3  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "4  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "\n",
              "                                               title  year  category_id  \\\n",
              "0  Dual Recurrent Attention Units for Visual Ques...  2018           15   \n",
              "1  Sequential Short-Text Classification with Recu...  2016           20   \n",
              "2  Multiresolution Recurrent Neural Networks: An ...  2016           20   \n",
              "3  Learning what to share between loosely related...  2017          124   \n",
              "4              A Deep Reinforcement Learning Chatbot  2017           20   \n",
              "\n",
              "                                        text_feature  \n",
              "0  Dual Recurrent Attention Units for Visual Ques...  \n",
              "1  Sequential Short-Text Classification with Recu...  \n",
              "2  Multiresolution Recurrent Neural Networks: An ...  \n",
              "3  Learning what to share between loosely related...  \n",
              "4  A Deep Reinforcement Learning Chatbot\\nWe pres...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-14403b95-642a-43aa-b65d-6422d2185163\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>category_index</th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "      <th>category_id</th>\n",
              "      <th>text_feature</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cs.AI</td>\n",
              "      <td>15</td>\n",
              "      <td>[{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...</td>\n",
              "      <td>1</td>\n",
              "      <td>1802.00209v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>We propose an architecture for VQA which utili...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Dual Recurrent Attention Units for Visual Ques...</td>\n",
              "      <td>2018</td>\n",
              "      <td>15</td>\n",
              "      <td>Dual Recurrent Attention Units for Visual Ques...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20</td>\n",
              "      <td>[{'name': 'Ji Young Lee'}, {'name': 'Franck De...</td>\n",
              "      <td>12</td>\n",
              "      <td>1603.03827v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>3</td>\n",
              "      <td>Recent approaches based on artificial neural n...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Sequential Short-Text Classification with Recu...</td>\n",
              "      <td>2016</td>\n",
              "      <td>20</td>\n",
              "      <td>Sequential Short-Text Classification with Recu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20</td>\n",
              "      <td>[{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...</td>\n",
              "      <td>2</td>\n",
              "      <td>1606.00776v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Multiresolution Recurrent Neural Networks: An ...</td>\n",
              "      <td>2016</td>\n",
              "      <td>20</td>\n",
              "      <td>Multiresolution Recurrent Neural Networks: An ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stat.ML</td>\n",
              "      <td>124</td>\n",
              "      <td>[{'name': 'Sebastian Ruder'}, {'name': 'Joachi...</td>\n",
              "      <td>23</td>\n",
              "      <td>1705.08142v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>5</td>\n",
              "      <td>Multi-task learning is motivated by the observ...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Learning what to share between loosely related...</td>\n",
              "      <td>2017</td>\n",
              "      <td>124</td>\n",
              "      <td>Learning what to share between loosely related...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cs.CL</td>\n",
              "      <td>20</td>\n",
              "      <td>[{'name': 'Iulian V. Serban'}, {'name': 'Chinn...</td>\n",
              "      <td>7</td>\n",
              "      <td>1709.02349v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>9</td>\n",
              "      <td>We present MILABOT: a deep reinforcement learn...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A Deep Reinforcement Learning Chatbot</td>\n",
              "      <td>2017</td>\n",
              "      <td>20</td>\n",
              "      <td>A Deep Reinforcement Learning Chatbot\\nWe pres...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14403b95-642a-43aa-b65d-6422d2185163')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-14403b95-642a-43aa-b65d-6422d2185163 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-14403b95-642a-43aa-b65d-6422d2185163');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6b9e6eef-0e35-4594-8241-0da2870cbca2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b9e6eef-0e35-4594-8241-0da2870cbca2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6b9e6eef-0e35-4594-8241-0da2870cbca2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 41000,\n  \"fields\": [\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 126,\n        \"samples\": [\n          \"cs.PL\",\n          \"math.OC\",\n          \"cs.OS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 34,\n        \"min\": 0,\n        \"max\": 125,\n        \"num_unique_values\": 126,\n        \"samples\": [\n          48,\n          82,\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 36157,\n        \"samples\": [\n          \"[{'name': 'Cosmin Stamate'}, {'name': 'George D. Magoulas'}, {'name': 'Michael S. C. Thomas'}]\",\n          \"[{'name': 'E. R. Vimina'}, {'name': 'K. Poulose Jacob'}]\",\n          \"[{'name': 'Manu Goyal'}, {'name': 'Moi Hoon Yap'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1,\n        \"max\": 31,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          13,\n          31,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41000,\n        \"samples\": [\n          \"0809.0490v2\",\n          \"1503.00036v2\",\n          \"1711.09522v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41000,\n        \"samples\": [\n          \"[{'rel': 'related', 'href': 'http://dx.doi.org/10.4018/978-1-60566-766-9', 'type': 'text/html', 'title': 'doi'}, {'rel': 'alternate', 'href': 'http://arxiv.org/abs/0809.0490v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/0809.0490v2', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1503.00036v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1503.00036v2', 'type': 'application/pdf', 'title': 'pdf'}]\",\n          \"[{'rel': 'alternate', 'href': 'http://arxiv.org/abs/1711.09522v2', 'type': 'text/html'}, {'rel': 'related', 'href': 'http://arxiv.org/pdf/1711.09522v2', 'type': 'application/pdf', 'title': 'pdf'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 12,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          7,\n          11,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40967,\n        \"samples\": [\n          \"Many state-of-the-art computer vision algorithms use large scale\\nconvolutional neural networks (CNNs) as basic building blocks. These CNNs are\\nknown for their huge number of parameters, high redundancy in weights, and\\ntremendous computing resource consumptions. This paper presents a learning\\nalgorithm to simplify and speed up these CNNs. Specifically, we introduce a\\n\\\"try-and-learn\\\" algorithm to train pruning agents that remove unnecessary CNN\\nfilters in a data-driven way. With the help of a novel reward function, our\\nagents removes a significant number of filters in CNNs while maintaining\\nperformance at a desired level. Moreover, this method provides an easy control\\nof the tradeoff between network performance and its scale. Per- formance of our\\nalgorithm is validated with comprehensive pruning experiments on several\\npopular CNNs for visual recognition and semantic segmentation tasks.\",\n          \"In this study, we present Swift Linked Data Miner, an interruptible algorithm\\nthat can directly mine an online Linked Data source (e.g., a SPARQL endpoint)\\nfor OWL 2 EL class expressions to extend an ontology with new SubClassOf:\\naxioms. The algorithm works by downloading only a small part of the Linked Data\\nsource at a time, building a smart index in the memory and swiftly iterating\\nover the index to mine axioms. We propose a transformation function from mined\\naxioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment,\\nthat most of the axioms mined by Swift Linked Data Miner are correct and can be\\nadded to an ontology. We provide a ready to use Prot\\\\'eg\\\\'e plugin implementing\\nthe algorithm, to support ontology engineers in their daily modeling work.\",\n          \"We investigate the capacity, convexity and characterization of a general\\nfamily of norm-constrained feed-forward networks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5874,\n        \"samples\": [\n          \"[{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.DS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'F.2.2; I.5; J.3', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'physics.soc-ph', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'math.CO', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '05C85, 68R10, 90B15, 90C35', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'G.2.2', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\",\n          \"[{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'q-bio.NC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40955,\n        \"samples\": [\n          \"Hypothesis Testing for Automated Community Detection in Networks\",\n          \"Probabilistic Models for Computerized Adaptive Testing\",\n          \"Belief Revision and Rational Inference\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1993,\n        \"max\": 2018,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          2011,\n          2002,\n          2018\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 34,\n        \"min\": 0,\n        \"max\": 125,\n        \"num_unique_values\": 126,\n        \"samples\": [\n          48,\n          82,\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_feature\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40986,\n        \"samples\": [\n          \"Norm-Based Capacity Control in Neural Networks\\nWe investigate the capacity, convexity and characterization of a general\\nfamily of norm-constrained feed-forward networks.\",\n          \"Pixel-wise Segmentation of Street with Neural Networks\\nPixel-wise street segmentation of photographs taken from a drivers\\nperspective is important for self-driving cars and can also support other\\nobject recognition tasks. A framework called SST was developed to examine the\\naccuracy and execution time of different neural networks. The best neural\\nnetwork achieved an $F_1$-score of 89.5% with a simple feedforward neural\\nnetwork which trained to solve a regression task.\",\n          \"Projected Model Counting\\nModel counting is the task of computing the number of assignments to\\nvariables V that satisfy a given propositional theory F. Model counting is an\\nessential tool in probabilistic reasoning. In this paper, we introduce the\\nproblem of model counting projected on a subset P of original variables that we\\ncall 'priority' variables. The task is to compute the number of assignments to\\nP such that there exists an extension to 'non-priority' variables V\\\\P that\\nsatisfies F. Projected model counting arises when some parts of the model are\\nirrelevant to the counts, in particular when we require additional variables to\\nmodel the problem we are counting in SAT. We discuss three different approaches\\nto projected model counting (two of which are novel), and compare their\\nperformance on different benchmark problems.\\n  To appear in 18th International Conference on Theory and Applications of\\nSatisfiability Testing, September 24-27, 2015, Austin, Texas, USA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRkCF65TPxT2"
      },
      "source": [
        "Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TIkBbbfPxT2",
        "outputId": "b7291137-567c-4787-8e7f-118cebd3f9a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41000"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YjGulT9PxT2"
      },
      "outputs": [],
      "source": [
        "data = data.loc[:, ['text_feature', 'category']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2zq3V4rMPxT2",
        "outputId": "0cc1cad3-6409-4197-e93d-6cc591809e8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            text_feature category\n",
              "0      Dual Recurrent Attention Units for Visual Ques...    cs.AI\n",
              "1      Sequential Short-Text Classification with Recu...    cs.CL\n",
              "2      Multiresolution Recurrent Neural Networks: An ...    cs.CL\n",
              "3      Learning what to share between loosely related...  stat.ML\n",
              "4      A Deep Reinforcement Learning Chatbot\\nWe pres...    cs.CL\n",
              "...                                                  ...      ...\n",
              "40995  Nearly Tight Bounds on $\\ell_1$ Approximation ...    cs.LG\n",
              "40996  Concurrent bandits and cognitive radio network...    cs.LG\n",
              "40997  A Comparison of Clustering and Missing Data Me...  math.NA\n",
              "40998  Applying machine learning to the problem of ch...    cs.SC\n",
              "40999  A Multi Level Data Fusion Approach for Speaker...    cs.SD\n",
              "\n",
              "[41000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-daf6db10-cf2e-49d3-9d95-8153c00b7253\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_feature</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dual Recurrent Attention Units for Visual Ques...</td>\n",
              "      <td>cs.AI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sequential Short-Text Classification with Recu...</td>\n",
              "      <td>cs.CL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Multiresolution Recurrent Neural Networks: An ...</td>\n",
              "      <td>cs.CL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Learning what to share between loosely related...</td>\n",
              "      <td>stat.ML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A Deep Reinforcement Learning Chatbot\\nWe pres...</td>\n",
              "      <td>cs.CL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40995</th>\n",
              "      <td>Nearly Tight Bounds on $\\ell_1$ Approximation ...</td>\n",
              "      <td>cs.LG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40996</th>\n",
              "      <td>Concurrent bandits and cognitive radio network...</td>\n",
              "      <td>cs.LG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40997</th>\n",
              "      <td>A Comparison of Clustering and Missing Data Me...</td>\n",
              "      <td>math.NA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40998</th>\n",
              "      <td>Applying machine learning to the problem of ch...</td>\n",
              "      <td>cs.SC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40999</th>\n",
              "      <td>A Multi Level Data Fusion Approach for Speaker...</td>\n",
              "      <td>cs.SD</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-daf6db10-cf2e-49d3-9d95-8153c00b7253')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-daf6db10-cf2e-49d3-9d95-8153c00b7253 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-daf6db10-cf2e-49d3-9d95-8153c00b7253');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9bba8068-6a38-493f-a6fb-f8cc3051f7b2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bba8068-6a38-493f-a6fb-f8cc3051f7b2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9bba8068-6a38-493f-a6fb-f8cc3051f7b2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 41000,\n  \"fields\": [\n    {\n      \"column\": \"text_feature\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40986,\n        \"samples\": [\n          \"Norm-Based Capacity Control in Neural Networks\\nWe investigate the capacity, convexity and characterization of a general\\nfamily of norm-constrained feed-forward networks.\",\n          \"Pixel-wise Segmentation of Street with Neural Networks\\nPixel-wise street segmentation of photographs taken from a drivers\\nperspective is important for self-driving cars and can also support other\\nobject recognition tasks. A framework called SST was developed to examine the\\naccuracy and execution time of different neural networks. The best neural\\nnetwork achieved an $F_1$-score of 89.5% with a simple feedforward neural\\nnetwork which trained to solve a regression task.\",\n          \"Projected Model Counting\\nModel counting is the task of computing the number of assignments to\\nvariables V that satisfy a given propositional theory F. Model counting is an\\nessential tool in probabilistic reasoning. In this paper, we introduce the\\nproblem of model counting projected on a subset P of original variables that we\\ncall 'priority' variables. The task is to compute the number of assignments to\\nP such that there exists an extension to 'non-priority' variables V\\\\P that\\nsatisfies F. Projected model counting arises when some parts of the model are\\nirrelevant to the counts, in particular when we require additional variables to\\nmodel the problem we are counting in SAT. We discuss three different approaches\\nto projected model counting (two of which are novel), and compare their\\nperformance on different benchmark problems.\\n  To appear in 18th International Conference on Theory and Applications of\\nSatisfiability Testing, September 24-27, 2015, Austin, Texas, USA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 126,\n        \"samples\": [\n          \"cs.PL\",\n          \"math.OC\",\n          \"cs.OS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe9h_9jPPxT2"
      },
      "outputs": [],
      "source": [
        "indices = list(range(len(data)))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_indices, val_indices = indices[:32000], indices[32000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXczF4xZPxT2"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "train_ds = Dataset.from_pandas(data.iloc[train_indices], split='train')\n",
        "val_ds = Dataset.from_pandas(data.iloc[val_indices], split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efE56o6DPxT3"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_workers = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZdQ7PyDPxT3"
      },
      "source": [
        "HF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58TFGydJPxT4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5MWAnI9PxT5"
      },
      "outputs": [],
      "source": [
        "#tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "checkpoint_model = \"dimsafin/arxiv_distilbert_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_model)\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text_feature\"], max_length=512, truncation=True, padding='max_length')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8d71afc8ddc347a69b9f397685536dc6",
            "4a336ca0f7ca46048c69121143ab36c3",
            "4efd0557a7b34e51aba9ec2c53d5f0b6",
            "645e3e34db6442a2b995fa4f1e2600cd",
            "540dc1c4cd6c49088bd29909012c9fde",
            "cdfe110bec554a55b6b0b2223ddf14de",
            "950e4ae7d6fc40e1b6869ba60f85967b",
            "ef6ffd63fbbc44539b1d5701a6e797e3",
            "03c64029ab3549d2ac16e793e4a8242e",
            "5f63e4aa881744e2bd227278c4cf144d",
            "f8c9ab1df7a048d7b2f6c684e49dc1e6",
            "fc535c205091491e84fc5476c204a038",
            "6cc9fe443fcf44c0b78180f8d614ed26",
            "ac2884d3af364f89a58e9f20fef8f9d0",
            "55669dc3b88c44ecb41b299b1d1a6242",
            "f7970200effc4d56b8612e5ee410c6e8",
            "145ae0a3c02943d9b0d569e972d059f0",
            "2541b4b66a47445a9c6a7f4ec17f1bee",
            "440902eac41b40da98586077ce165f98",
            "e8ff60a379124e388185287935cbb7cf",
            "c67587a030b54f5c84bcaaea20cab19d",
            "973b583e451a43f9b966d8389bfba4a6"
          ]
        },
        "id": "PG9iBJWMPxT5",
        "outputId": "d8a26e31-5d81-4319-b8fe-8bc470adcd59"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/32000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d71afc8ddc347a69b9f397685536dc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc535c205091491e84fc5476c204a038"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_ds = train_ds.map(preprocess_function, batched=True)\n",
        "val_ds = val_ds.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = ClassLabel(num_classes=num_categories, names=list(unique_categories))\n",
        "\n",
        "def transform_labels(row):\n",
        "    # default name for a label (label or label_ids)\n",
        "    return {\"label\": labels_map.str2int(row[\"category\"])}\n",
        "\n",
        "train_ds = train_ds.map(transform_labels, batched=True)\n",
        "val_ds = val_ds.map(transform_labels, batched=True)\n",
        "\n",
        "train_ds = train_ds.cast_column('label', labels_map)\n",
        "val_ds = val_ds.cast_column('label', labels_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "b07362ee68604e0893b4cbde3f4f5ea8",
            "0fb9defb31fe44bb8189407fb87697d3",
            "7480f2a1e1d04a818a1d4f5f0854380d",
            "2700c68173dd44d2bd7dc0a51f6af8f6",
            "7fed4c301693476b8789664173168d53",
            "f590d62414504658b7aac1735eb70bcd",
            "7c9a47dc492d46d2b0689bed3f3d26c9",
            "f18d8eb3673f49359aa2f700ea363452",
            "f6fd9f2b8eb448b298bf91bc03d1bf5c",
            "6bb09c7f5ecc435f82a49ce131ad4aa7",
            "9a19b55321ef4b5691a963af6f1fef2f",
            "df7631c3aa0f4d67ae86f37956f23b37",
            "75717ea4b5494f74bd6a2d5d3c6b70bc",
            "1a0c44a9e6a946fe9ab4d5272d2f2101",
            "ceec9b67014447458aacd1a2c7c85112",
            "480511d9ab7d413f96f6680139e9c16f",
            "98cc0ebfc82d4288b58ac9ba20d0c8f5",
            "eaf8d2cb297d44a1987fe1cf5fe398e1",
            "2c7a8faa3f2a40409a66a190d28be336",
            "66213cb739994523a6065acb4ec7f067",
            "e8def574d6e74b45b349a39c5197d4ce",
            "8fb53798192b404e9c110ebbb1d8a6ec",
            "8d8adfc3ba1b4a68b69c224e08589a8c",
            "6541330b9242496bb179c561f1e18db7",
            "49e983e9613d4c588c6d2507ac64b32a",
            "640373c9bed740a3a38d46c8c9a48c82",
            "d549551e72a1400883b4eefad4ee3465",
            "475c78e15ba9423384a6828027268375",
            "2dd04aeb786d46c6a6ee32c077e3da3c",
            "9c78af92f8c144f7879958f2ca56209e",
            "21c66e5ad4fd493f980e0ba192daf726",
            "c2e8032bef4a4510829789379087d769",
            "834f4e2619b0493cb846359c64052bf6",
            "c1bdeca03dc24c7d90e6c09471a0fa1f",
            "84b5a1f33cbb4cfea6cd5b7e09105d1b",
            "fb81cb235dba47c8adb265d2f0e3f994",
            "27354949cdcd486fb4a80b0a42411ace",
            "b00460343a654d2fbf179e943bec14cf",
            "59ff816d0720490b9059efabbdb88d04",
            "73c3295da6bc40b2a621ab5ef7396a10",
            "f7556e978d9f45acbe4ad86cc521284f",
            "3f78ea8047014624a3a69b18150a642f",
            "36d136526c4e4a07933cd078d832974a",
            "4f487cfe75604671b54dd7edb42f5393"
          ]
        },
        "id": "fApfQNVwSH5h",
        "outputId": "88f3d1ac-8071-4ed1-e92e-9713f0ef3444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/32000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b07362ee68604e0893b4cbde3f4f5ea8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df7631c3aa0f4d67ae86f37956f23b37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/32000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d8adfc3ba1b4a68b69c224e08589a8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/9000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1bdeca03dc24c7d90e6c09471a0fa1f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qDOszYtPxT5"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66tQ1QJCPxT5",
        "outputId": "57cca50e-eb9f-4b17-e6b1-b2534366d36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#    \"distilbert/distilbert-base-uncased\",\n",
        "    checkpoint_model,\n",
        "    num_labels=num_categories,\n",
        "    id2label={i:cat for i, cat in enumerate(unique_categories)},\n",
        "    label2id=category_to_ind).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHq1ik4UPxT6",
        "outputId": "545cc5bc-8d25-467e-fbb4-83e41c3200cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=126, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7VwOL3jPxT6"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "Vv7TTNaIUAw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owo-T-E0PxT7",
        "outputId": "b2f8a640-bb75-4cab-8419-a1ca5f951a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"arxiv_distilbert_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"arxiv_distilbert_model\",\n",
        "    hub_token=token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip4oZwEzPxT7"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "b9JGZ5R2PxT7",
        "outputId": "74d35ca9-b304-4324-a328-67888b1e5ea5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5919' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5919/20000 2:33:48 < 6:06:01, 0.64 it/s, Epoch 5.92/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.469300</td>\n",
              "      <td>1.361324</td>\n",
              "      <td>0.631778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.177800</td>\n",
              "      <td>1.215741</td>\n",
              "      <td>0.658556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.000800</td>\n",
              "      <td>1.190308</td>\n",
              "      <td>0.658000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.869300</td>\n",
              "      <td>1.202424</td>\n",
              "      <td>0.660222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.703900</td>\n",
              "      <td>1.257401</td>\n",
              "      <td>0.655222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2234\u001b[0m                 \u001b[0;31m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m                 return inner_training_loop(\n\u001b[0m\u001b[1;32m   2237\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m                     \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2559\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2561\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2562\u001b[0m                     ):\n\u001b[1;32m   2563\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()  # it's not the last model version but anyway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llVJd5iDPxT7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaxBdwgZPxT8"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"arxiv_distilbert_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G9WG3wrPxT8"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"arxiv_distilbert_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXoKGRiePxT8"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d71afc8ddc347a69b9f397685536dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a336ca0f7ca46048c69121143ab36c3",
              "IPY_MODEL_4efd0557a7b34e51aba9ec2c53d5f0b6",
              "IPY_MODEL_645e3e34db6442a2b995fa4f1e2600cd"
            ],
            "layout": "IPY_MODEL_540dc1c4cd6c49088bd29909012c9fde"
          }
        },
        "4a336ca0f7ca46048c69121143ab36c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdfe110bec554a55b6b0b2223ddf14de",
            "placeholder": "​",
            "style": "IPY_MODEL_950e4ae7d6fc40e1b6869ba60f85967b",
            "value": "Map: 100%"
          }
        },
        "4efd0557a7b34e51aba9ec2c53d5f0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef6ffd63fbbc44539b1d5701a6e797e3",
            "max": 32000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03c64029ab3549d2ac16e793e4a8242e",
            "value": 32000
          }
        },
        "645e3e34db6442a2b995fa4f1e2600cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f63e4aa881744e2bd227278c4cf144d",
            "placeholder": "​",
            "style": "IPY_MODEL_f8c9ab1df7a048d7b2f6c684e49dc1e6",
            "value": " 32000/32000 [00:25&lt;00:00, 1383.67 examples/s]"
          }
        },
        "540dc1c4cd6c49088bd29909012c9fde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdfe110bec554a55b6b0b2223ddf14de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "950e4ae7d6fc40e1b6869ba60f85967b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef6ffd63fbbc44539b1d5701a6e797e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c64029ab3549d2ac16e793e4a8242e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f63e4aa881744e2bd227278c4cf144d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8c9ab1df7a048d7b2f6c684e49dc1e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc535c205091491e84fc5476c204a038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cc9fe443fcf44c0b78180f8d614ed26",
              "IPY_MODEL_ac2884d3af364f89a58e9f20fef8f9d0",
              "IPY_MODEL_55669dc3b88c44ecb41b299b1d1a6242"
            ],
            "layout": "IPY_MODEL_f7970200effc4d56b8612e5ee410c6e8"
          }
        },
        "6cc9fe443fcf44c0b78180f8d614ed26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_145ae0a3c02943d9b0d569e972d059f0",
            "placeholder": "​",
            "style": "IPY_MODEL_2541b4b66a47445a9c6a7f4ec17f1bee",
            "value": "Map: 100%"
          }
        },
        "ac2884d3af364f89a58e9f20fef8f9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_440902eac41b40da98586077ce165f98",
            "max": 9000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8ff60a379124e388185287935cbb7cf",
            "value": 9000
          }
        },
        "55669dc3b88c44ecb41b299b1d1a6242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c67587a030b54f5c84bcaaea20cab19d",
            "placeholder": "​",
            "style": "IPY_MODEL_973b583e451a43f9b966d8389bfba4a6",
            "value": " 9000/9000 [00:07&lt;00:00, 1039.92 examples/s]"
          }
        },
        "f7970200effc4d56b8612e5ee410c6e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "145ae0a3c02943d9b0d569e972d059f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2541b4b66a47445a9c6a7f4ec17f1bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "440902eac41b40da98586077ce165f98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ff60a379124e388185287935cbb7cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c67587a030b54f5c84bcaaea20cab19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "973b583e451a43f9b966d8389bfba4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b07362ee68604e0893b4cbde3f4f5ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fb9defb31fe44bb8189407fb87697d3",
              "IPY_MODEL_7480f2a1e1d04a818a1d4f5f0854380d",
              "IPY_MODEL_2700c68173dd44d2bd7dc0a51f6af8f6"
            ],
            "layout": "IPY_MODEL_7fed4c301693476b8789664173168d53"
          }
        },
        "0fb9defb31fe44bb8189407fb87697d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f590d62414504658b7aac1735eb70bcd",
            "placeholder": "​",
            "style": "IPY_MODEL_7c9a47dc492d46d2b0689bed3f3d26c9",
            "value": "Map: 100%"
          }
        },
        "7480f2a1e1d04a818a1d4f5f0854380d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f18d8eb3673f49359aa2f700ea363452",
            "max": 32000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6fd9f2b8eb448b298bf91bc03d1bf5c",
            "value": 32000
          }
        },
        "2700c68173dd44d2bd7dc0a51f6af8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bb09c7f5ecc435f82a49ce131ad4aa7",
            "placeholder": "​",
            "style": "IPY_MODEL_9a19b55321ef4b5691a963af6f1fef2f",
            "value": " 32000/32000 [00:00&lt;00:00, 67920.16 examples/s]"
          }
        },
        "7fed4c301693476b8789664173168d53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f590d62414504658b7aac1735eb70bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c9a47dc492d46d2b0689bed3f3d26c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f18d8eb3673f49359aa2f700ea363452": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6fd9f2b8eb448b298bf91bc03d1bf5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bb09c7f5ecc435f82a49ce131ad4aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a19b55321ef4b5691a963af6f1fef2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df7631c3aa0f4d67ae86f37956f23b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75717ea4b5494f74bd6a2d5d3c6b70bc",
              "IPY_MODEL_1a0c44a9e6a946fe9ab4d5272d2f2101",
              "IPY_MODEL_ceec9b67014447458aacd1a2c7c85112"
            ],
            "layout": "IPY_MODEL_480511d9ab7d413f96f6680139e9c16f"
          }
        },
        "75717ea4b5494f74bd6a2d5d3c6b70bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98cc0ebfc82d4288b58ac9ba20d0c8f5",
            "placeholder": "​",
            "style": "IPY_MODEL_eaf8d2cb297d44a1987fe1cf5fe398e1",
            "value": "Map: 100%"
          }
        },
        "1a0c44a9e6a946fe9ab4d5272d2f2101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c7a8faa3f2a40409a66a190d28be336",
            "max": 9000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66213cb739994523a6065acb4ec7f067",
            "value": 9000
          }
        },
        "ceec9b67014447458aacd1a2c7c85112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8def574d6e74b45b349a39c5197d4ce",
            "placeholder": "​",
            "style": "IPY_MODEL_8fb53798192b404e9c110ebbb1d8a6ec",
            "value": " 9000/9000 [00:00&lt;00:00, 94899.72 examples/s]"
          }
        },
        "480511d9ab7d413f96f6680139e9c16f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98cc0ebfc82d4288b58ac9ba20d0c8f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaf8d2cb297d44a1987fe1cf5fe398e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c7a8faa3f2a40409a66a190d28be336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66213cb739994523a6065acb4ec7f067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8def574d6e74b45b349a39c5197d4ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fb53798192b404e9c110ebbb1d8a6ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d8adfc3ba1b4a68b69c224e08589a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6541330b9242496bb179c561f1e18db7",
              "IPY_MODEL_49e983e9613d4c588c6d2507ac64b32a",
              "IPY_MODEL_640373c9bed740a3a38d46c8c9a48c82"
            ],
            "layout": "IPY_MODEL_d549551e72a1400883b4eefad4ee3465"
          }
        },
        "6541330b9242496bb179c561f1e18db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_475c78e15ba9423384a6828027268375",
            "placeholder": "​",
            "style": "IPY_MODEL_2dd04aeb786d46c6a6ee32c077e3da3c",
            "value": "Casting the dataset: 100%"
          }
        },
        "49e983e9613d4c588c6d2507ac64b32a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c78af92f8c144f7879958f2ca56209e",
            "max": 32000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21c66e5ad4fd493f980e0ba192daf726",
            "value": 32000
          }
        },
        "640373c9bed740a3a38d46c8c9a48c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2e8032bef4a4510829789379087d769",
            "placeholder": "​",
            "style": "IPY_MODEL_834f4e2619b0493cb846359c64052bf6",
            "value": " 32000/32000 [00:00&lt;00:00, 85073.15 examples/s]"
          }
        },
        "d549551e72a1400883b4eefad4ee3465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475c78e15ba9423384a6828027268375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dd04aeb786d46c6a6ee32c077e3da3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c78af92f8c144f7879958f2ca56209e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21c66e5ad4fd493f980e0ba192daf726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2e8032bef4a4510829789379087d769": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "834f4e2619b0493cb846359c64052bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1bdeca03dc24c7d90e6c09471a0fa1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84b5a1f33cbb4cfea6cd5b7e09105d1b",
              "IPY_MODEL_fb81cb235dba47c8adb265d2f0e3f994",
              "IPY_MODEL_27354949cdcd486fb4a80b0a42411ace"
            ],
            "layout": "IPY_MODEL_b00460343a654d2fbf179e943bec14cf"
          }
        },
        "84b5a1f33cbb4cfea6cd5b7e09105d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59ff816d0720490b9059efabbdb88d04",
            "placeholder": "​",
            "style": "IPY_MODEL_73c3295da6bc40b2a621ab5ef7396a10",
            "value": "Casting the dataset: 100%"
          }
        },
        "fb81cb235dba47c8adb265d2f0e3f994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7556e978d9f45acbe4ad86cc521284f",
            "max": 9000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f78ea8047014624a3a69b18150a642f",
            "value": 9000
          }
        },
        "27354949cdcd486fb4a80b0a42411ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d136526c4e4a07933cd078d832974a",
            "placeholder": "​",
            "style": "IPY_MODEL_4f487cfe75604671b54dd7edb42f5393",
            "value": " 9000/9000 [00:00&lt;00:00, 159616.81 examples/s]"
          }
        },
        "b00460343a654d2fbf179e943bec14cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59ff816d0720490b9059efabbdb88d04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73c3295da6bc40b2a621ab5ef7396a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7556e978d9f45acbe4ad86cc521284f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f78ea8047014624a3a69b18150a642f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36d136526c4e4a07933cd078d832974a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f487cfe75604671b54dd7edb42f5393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}